{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Bayes Rule](#bayes_rule)\n",
    "* [AUC](#auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Rule in Probability Theory <a id=\"bayes_rule\"></a>\n",
    "\n",
    "**Bayes’ Rule** (or **Bayes’ Theorem**) is a fundamental concept in probability theory and statistics that describes how to update the probability of a hypothesis based on new evidence. It provides a way to calculate a conditional probability by using prior knowledge and new data. This theorem is the backbone of **Bayesian inference** and allows for the continuous updating of beliefs in the presence of new information.\n",
    "\n",
    "### Formula of Bayes' Rule\n",
    "\n",
    "The mathematical form of **Bayes' Rule** is as follows:\n",
    "\n",
    "$$\n",
    "P(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ P(H|E) $ is the **posterior probability**: the probability of the hypothesis $ H $ given the evidence $ E $ (what we're trying to calculate).\n",
    "- $ P(E|H) $ is the **likelihood**: the probability of observing the evidence $ E $ given that the hypothesis $ H $ is true.\n",
    "- $ P(H) $ is the **prior probability**: the probability of the hypothesis $ H $ before seeing the evidence (our initial belief about $ H $).\n",
    "- $ P(E) $ is the **marginal likelihood** (also called the **evidence**): the total probability of the evidence $ E $ under all possible hypotheses.\n",
    "\n",
    "### Explanation of the Terms\n",
    "\n",
    "1. **Prior Probability $ P(H) $**:\n",
    "   - The **prior** is the initial belief or probability about the hypothesis $ H $, before observing any data or evidence.\n",
    "   - For example, if you think a coin is fair before flipping it, you may assign a prior probability of 0.5 that it will land heads.\n",
    "\n",
    "2. **Likelihood $ P(E|H) $**:\n",
    "   - The **likelihood** represents how probable the evidence $ E $ is, assuming the hypothesis $ H $ is true.\n",
    "   - In the context of the coin flip, the likelihood might represent how likely it is to observe 7 heads out of 10 flips if you assume the coin is fair.\n",
    "\n",
    "3. **Posterior Probability $ P(H|E) $**:\n",
    "   - The **posterior** is the updated probability of the hypothesis $ H $, after taking into account the evidence $ E $.\n",
    "   - This is what Bayes' Rule helps to compute: it gives us a new probability estimate for the hypothesis after factoring in the new evidence.\n",
    "\n",
    "4. **Marginal Likelihood $ P(E) $**:\n",
    "   - The **marginal likelihood** (or **evidence**) is the total probability of observing the evidence $ E $, considering all possible hypotheses.\n",
    "   - This term normalizes the posterior probability, ensuring that the probabilities add up to 1. It can be computed as:\n",
    "     \n",
    "     $$\n",
    "     P(E) = \\sum_i P(E|H_i) \\cdot P(H_i)\n",
    "     $$\n",
    "   - In other words, it’s the sum of the probabilities of the evidence occurring under each possible hypothesis.\n",
    "\n",
    "### Bayes' Rule in Words\n",
    "\n",
    "Bayes' Rule states that:\n",
    "\n",
    "- The **posterior probability** of a hypothesis $ H $ given the evidence $ E $ is proportional to the **likelihood** of the evidence given the hypothesis and the **prior** probability of the hypothesis.\n",
    "- You **update** your belief (prior) about a hypothesis based on how well it explains the observed data (likelihood).\n",
    "- The posterior is normalized by the **marginal likelihood**, which accounts for the overall probability of the evidence.\n",
    "\n",
    "### Example: Coin Flip\n",
    "\n",
    "Imagine you're testing whether a coin is biased or not (hypothesis $ H $). Initially, you believe the coin is fair with probability $ P(H) = 0.5 $ (this is your prior). You flip the coin 10 times and observe 7 heads. The question is: how should this evidence update your belief about the fairness of the coin?\n",
    "\n",
    "- **Prior** $ P(H) = 0.5 $: You initially think the coin is fair.\n",
    "- **Likelihood** $ P(E|H) $: You compute the probability of observing 7 heads out of 10 flips, assuming the coin is fair.\n",
    "- **Marginal likelihood** $ P(E) $: You consider all possible hypotheses (fair or biased coin) and calculate the total probability of observing 7 heads.\n",
    "- **Posterior** $ P(H|E) $: After observing 7 heads, you update your belief about whether the coin is fair or biased.\n",
    "\n",
    "By applying Bayes' Rule, you update your belief based on the observed evidence.\n",
    "\n",
    "### Intuitive Explanation of Bayes' Rule\n",
    "\n",
    "Bayes’ Rule helps you **revise your prior beliefs** when new data or evidence becomes available. Here’s a simple breakdown:\n",
    "\n",
    "- **Start with your prior belief** (what you think before seeing any evidence).\n",
    "- **Check how well the evidence fits** with your hypothesis (this is the likelihood).\n",
    "- **Adjust your belief** based on how the evidence supports or contradicts your hypothesis.\n",
    "  \n",
    "If the evidence strongly supports your hypothesis, the posterior probability will increase; if the evidence contradicts your hypothesis, the posterior probability will decrease.\n",
    "\n",
    "### Use Cases of Bayes' Rule\n",
    "\n",
    "Bayes’ Rule is widely used in various fields:\n",
    "- **Machine Learning**: In Bayesian inference models, like **Naive Bayes classifiers** and **Bayesian Neural Networks**.\n",
    "- **Medical Diagnosis**: Doctors use Bayes’ Rule to update the probability of a disease based on symptoms and test results.\n",
    "- **Spam Filtering**: Email systems use Bayesian techniques to determine the likelihood that an email is spam based on certain keywords or characteristics.\n",
    "- **Decision Making**: Bayes' Rule is used to make informed decisions based on incomplete information.\n",
    "\n",
    "In summary, Bayes' Rule is a powerful tool in probability theory that provides a formal way to update beliefs or probabilities when new evidence is encountered. It helps refine our understanding of an uncertain world by combining prior knowledge with new data.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
