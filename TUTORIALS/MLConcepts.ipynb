{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Kullback-Leibler Divergence and Cross Entropy\n",
    "\n",
    "#### Kullback-Leibler Divergence\n",
    "\n",
    "Kullback-Leibler (KL) divergence is a measure of how one probability distribution diverges from a second, expected probability distribution. It is often used in information theory, statistics, and machine learning to quantify the difference between two probability distributions.\n",
    "\n",
    "**Mathematical Definition**\n",
    "\n",
    "Given two probability distributions $P$ (true distribution) and $Q$ (approximate distribution), the KL divergence from $Q$ to $P$ is defined as:\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\parallel Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(x)}\n",
    "$$\n",
    "\n",
    "For continuous distributions, the sum is replaced by an integral:\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\parallel Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} \\, dx\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $P(x)$ and $Q(x)$ are the probability mass functions (for discrete distributions) or probability density functions (for continuous distributions).\n",
    "- $\\mathcal{X}$ is the set of all possible events.\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "- $D_{KL}(P \\parallel Q)$ measures the amount of information lost when $Q$ is used to approximate $P$.\n",
    "- It is not symmetric: $D_{KL}(P \\parallel Q) \\neq D_{KL}(Q \\parallel P)$.\n",
    "\n",
    "#### Cross Entropy\n",
    "\n",
    "Cross entropy is a measure of the difference between two probability distributions for a given set of events. It quantifies the average number of bits needed to identify an event from a set of possibilities if a coding scheme is used based on a given probability distribution $Q$, rather than the true distribution $P$.\n",
    "\n",
    "**Mathematical Definition**\n",
    "\n",
    "For discrete distributions $P$ and $Q$, the cross entropy is defined as:\n",
    "\n",
    "$$\n",
    "H(P, Q) = -\\sum_{x \\in \\mathcal{X}} P(x) \\log Q(x)\n",
    "$$\n",
    "\n",
    "For continuous distributions:\n",
    "\n",
    "$$\n",
    "H(P, Q) = -\\int_{-\\infty}^{\\infty} p(x) \\log q(x) \\, dx\n",
    "$$\n",
    "\n",
    "### Relationship Between KL Divergence and Cross Entropy\n",
    "\n",
    "The KL divergence can be expressed in terms of entropy and cross entropy. The entropy $H(P)$ of a distribution $P$ is given by:\n",
    "\n",
    "$$\n",
    "H(P) = -\\sum_{x \\in \\mathcal{X}} P(x) \\log P(x)\n",
    "$$\n",
    "\n",
    "For continuous distributions:\n",
    "\n",
    "$$\n",
    "H(P) = -\\int_{-\\infty}^{\\infty} p(x) \\log p(x) \\, dx\n",
    "$$\n",
    "\n",
    "The cross entropy $H(P, Q)$ can be decomposed as:\n",
    "\n",
    "$$\n",
    "H(P, Q) = H(P) + D_{KL}(P \\parallel Q)\n",
    "$$\n",
    "\n",
    "Thus, the KL divergence can be interpreted as the difference between the cross entropy and the entropy of $P$:\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\parallel Q) = H(P, Q) - H(P)\n",
    "$$\n",
    "\n",
    "This equation highlights that KL divergence measures the extra amount of information (in bits) required to encode the distribution $P$ using the distribution $Q$ compared to using the optimal code based on $P$.\n",
    "\n",
    "### Example in Context\n",
    "\n",
    "Suppose you have a true probability distribution $P$ and you are using an approximate distribution $Q$. If you calculate the cross entropy $H(P, Q)$, it will tell you how many bits on average you need to encode samples from $P$ using the distribution $Q$.\n",
    "\n",
    "The entropy $H(P)$ tells you how many bits are needed if you use the true distribution $P$.\n",
    "\n",
    "The KL divergence $D_{KL}(P \\parallel Q)$ then gives you the inefficiency introduced by using $Q$ instead of $P$. It essentially quantifies how much more difficult it is to represent the true distribution $P$ using the approximate distribution $Q$.\n",
    "\n",
    "### Visual Representation\n",
    "\n",
    "To illustrate, consider the following:\n",
    "\n",
    "- $P$ is the true distribution of data.\n",
    "- $Q$ is a model's predicted distribution.\n",
    "\n",
    "When you use $Q$ to approximate $P$, the KL divergence provides a way to measure how \"wrong\" $Q$ is. The lower the KL divergence, the closer $Q$ is to $P$.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **KL Divergence** measures the difference between two probability distributions.\n",
    "- **Cross Entropy** measures the average number of bits needed to encode data from a true distribution using an approximate distribution.\n",
    "- **Relation**: KL divergence is the difference between the cross entropy and the entropy of the true distribution. It provides a measure of inefficiency when using an approximate distribution instead of the true distribution.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
