{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders (VAEs)\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Variational Autoencoders (VAEs)** are a type of generative model that combine the ideas from autoencoders and variational inference. They learn a **probabilistic latent space** that can be sampled to generate new data similar to the input data. VAEs are commonly used for generating images, learning latent representations, and unsupervised tasks.\n",
    "\n",
    "VAEs provide a principled way of performing both dimensionality reduction and data generation by learning a distribution of the latent variables, unlike standard autoencoders, which directly map inputs to latent codes.\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture of VAEs\n",
    "\n",
    "The basic architecture of a VAE consists of:\n",
    "\n",
    "1. **Encoder**: Maps input data $ x $ to a probability distribution over the latent space $ z $. This distribution is typically Gaussian.\n",
    "2. **Latent Space**: Encodes the data in a low-dimensional, continuous latent variable $ z $, sampled from the distribution learned by the encoder.\n",
    "3. **Decoder**: Maps latent variable $ z $ back to a distribution over the original data space $ x $.\n",
    "\n",
    "In contrast to standard autoencoders, VAEs encode the input as a distribution, rather than a single point. They aim to minimize the reconstruction error **and** regularize the latent space using a term based on **Kullback-Leibler (KL) divergence**.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Foundations of VAEs\n",
    "\n",
    "### 1. The Encoder Network\n",
    "\n",
    "The encoder maps the input $ x $ to a distribution over the latent variable $ z $. The distribution is often assumed to be Gaussian, so the encoder outputs the mean $ \\mu(x) $ and the standard deviation $ \\sigma(x) $ of the Gaussian distribution for each input:\n",
    "\n",
    "$$\n",
    "q(z|x) = \\mathcal{N}(z; \\mu(x), \\sigma(x)^2)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $ \\mu(x) $ is the mean of the Gaussian.\n",
    "- $ \\sigma(x) $ is the standard deviation (often represented as $ \\log(\\sigma(x)) $ to avoid negative values).\n",
    "\n",
    "### 2. Sampling Latent Variable $ z $\n",
    "\n",
    "To make backpropagation work, we use the **reparameterization trick**. Instead of directly sampling $ z $ from the Gaussian, we reparameterize the sampling process as:\n",
    "\n",
    "$$\n",
    "z = \\mu(x) + \\sigma(x) \\odot \\epsilon\n",
    "$$\n",
    "\n",
    "where $ \\epsilon $ is sampled from a standard normal distribution $ \\mathcal{N}(0, 1) $. This allows the model to learn $ \\mu(x) $ and $ \\sigma(x) $ through gradient descent.\n",
    "\n",
    "### 3. The Decoder Network\n",
    "\n",
    "The decoder reconstructs the data from the latent variable $ z $. It aims to maximize the likelihood of the data $ p(x|z) $, which can also be modeled as a Gaussian distribution:\n",
    "\n",
    "$$\n",
    "p(x|z) = \\mathcal{N}(x; \\hat{x}(z), \\sigma^2)\n",
    "$$\n",
    "\n",
    "where $ \\hat{x}(z) $ is the reconstructed output from the decoder network.\n",
    "\n",
    "### 4. The Loss Function\n",
    "\n",
    "The loss function for a VAE consists of two terms:\n",
    "\n",
    "1. **Reconstruction Loss**: Measures how well the decoder reconstructs the input data $ x $. This can be computed using binary cross-entropy or mean squared error depending on the data type.\n",
    "   \n",
    "   $$\n",
    "   \\mathcal{L}_{\\text{reconstruction}} = - \\mathbb{E}_{q(z|x)}[\\log p(x|z)]\n",
    "   $$\n",
    "\n",
    "2. **KL Divergence (Regularization Term)**: Encourages the distribution $ q(z|x) $ to be close to the prior distribution $ p(z) $, which is typically a standard Gaussian $ \\mathcal{N}(0, 1) $. This regularizes the latent space to ensure smoothness and allows for meaningful sampling from it.\n",
    "\n",
    "   $$\n",
    "   \\mathcal{L}_{\\text{KL}} = D_{\\text{KL}}(q(z|x) \\parallel p(z))\n",
    "   $$\n",
    "\n",
    "   The KL divergence between the approximate posterior $ q(z|x) = \\mathcal{N}(z; \\mu(x), \\sigma(x)^2) $ and the prior $ p(z) = \\mathcal{N}(0, 1) $ is given by:\n",
    "\n",
    "   $$\n",
    "   D_{\\text{KL}}(q(z|x) \\parallel p(z)) = \\frac{1}{2} \\sum_{i=1}^{d} \\left( \\mu_i^2 + \\sigma_i^2 - \\log(\\sigma_i^2) - 1 \\right)\n",
    "   $$\n",
    "\n",
    "Thus, the total loss for the VAE is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{VAE}} = \\mathcal{L}_{\\text{reconstruction}} + \\mathcal{L}_{\\text{KL}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## How VAEs Work in Practice\n",
    "\n",
    "1. **Training**: The VAE is trained to optimize the total loss function that balances the reconstruction accuracy and the regularization (KL divergence). The encoder learns to map the input data to a Gaussian distribution in the latent space, while the decoder learns to generate realistic outputs from sampled latent variables.\n",
    "\n",
    "2. **Generation**: After training, we can generate new data by sampling from the prior distribution $ p(z) = \\mathcal{N}(0, 1) $ in the latent space and feeding these samples into the decoder.\n",
    "\n",
    "---\n",
    "\n",
    "## Common Use Cases of VAEs\n",
    "\n",
    "1. **Data Generation**: VAEs can generate new data that resembles the training data by sampling from the latent space. For example, they are used for generating images, speech, and other forms of data.\n",
    "\n",
    "2. **Dimensionality Reduction**: VAEs can be used for unsupervised learning tasks where the goal is to learn a low-dimensional representation of the data, like PCA but with a probabilistic interpretation.\n",
    "\n",
    "3. **Anomaly Detection**: Since VAEs model the distribution of the data, they can detect anomalies by observing how well the model reconstructs new data points. Poor reconstruction suggests that the data point is unusual or anomalous.\n",
    "\n",
    "4. **Semi-supervised Learning**: VAEs can be used in semi-supervised learning settings, where only a small portion of the data is labeled. The VAE can help by learning meaningful latent representations from the unlabeled data.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Variational Autoencoders (VAEs) are a powerful class of generative models that learn to map data into a probabilistic latent space. They are trained by optimizing a loss function that balances the reconstruction error and the KL divergence between the learned latent distribution and a prior. VAEs are used for tasks like data generation, dimensionality reduction, and anomaly detection.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
