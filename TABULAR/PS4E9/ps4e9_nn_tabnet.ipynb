{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import statistics\n",
    "import re\n",
    "import torch\n",
    "import wandb\n",
    "import gc\n",
    "import pytorch_lightning as pl\n",
    "import torch.multiprocessing as mp\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, TargetEncoder\n",
    "import category_encoders as ce\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectFromModel, SequentialFeatureSelector\n",
    "from functools import partial\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"/home/bk_anupam/code/ML/ML_UTILS/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_tabular_utils as tt\n",
    "import cv_split_utils\n",
    "import enums\n",
    "from enums import ModelName\n",
    "import data_utils\n",
    "import param_tuning_utils as ptu\n",
    "import dl_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    RUNTIME = \"LOCAL\"\n",
    "    RANDOM_SEED = 42\n",
    "    NUM_FOLDS = 5\n",
    "    TARGET_COL_NAME = \"price\"            \n",
    "    METRIC = enums.Metrics.RMSE\n",
    "    # These values are more dynamic   \n",
    "    MODEL_TO_USE = ModelName.TabNetRegressor    \n",
    "    TRAIN_SINGLE_FOLD = False    \n",
    "    PERSIST_MODEL = False    \n",
    "    USE_MANUAL_FEATURES = True\n",
    "    USE_ORIGINAL_DATA = False        \n",
    "\n",
    "COLS_TO_LEAVE = [\"id\", \"price\", \"kfold\", \"transmission_speed\", \"target_grp\"]\n",
    "CPU_COUNT = os.cpu_count()\n",
    "\n",
    "DATA_READPATH = \"./data/\"\n",
    "DATA_WRITEPATH = \"./output/\"\n",
    "SUBMISSION_FILEPATH = DATA_READPATH\n",
    "if Config.RUNTIME == \"KAGGLE\":    \n",
    "    DATA_READPATH = \"/kaggle/input/playground-series-s4e9/\"\n",
    "    if Config.USE_MANUAL_FEATURES:\n",
    "        DATA_READPATH = \"/kaggle/input/ps4e9-fe/\"\n",
    "    SUBMISSION_FILEPATH = \"/kaggle/input/playground-series-s4e9/\"\n",
    "    DATA_WRITEPATH = \"/kaggle/working/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for tabnet\n",
    "class TabNetConfig:\n",
    "    PATIENCE = 10\n",
    "    WEIGHT_DECAY = 1e-6    \n",
    "    PRECISION = \"16-mixed\"\n",
    "    BATCH_SIZE = 4096*8\n",
    "    NUM_WORKERS = mp.cpu_count()\n",
    "    NUM_EPOCHS = 2    \n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
    "\n",
    "class SchedulerConfig:\n",
    "    # for ReduceLROnPlateau (number of epochs with no improvement after which the learning rate will be reduced)\n",
    "    SCHEDULER_PATIENCE = 5  \n",
    "    # for ReduceLROnPlateau (factor by which the learning rate will be reduced)\n",
    "    FACTOR = 0.9 \n",
    "    SCHEDULER = \"ReduceLROnPlateau\"\n",
    "    T_0 = 10 # for CosineAnnealingWarmRestarts (Number of epochs before the first restart)\n",
    "    MIN_LR = 5e-7 # for CosineAnnealingWarmRestarts (Minimum learning rate)\n",
    "    T_mult = 1 # for CosineAnnealingWarmRestarts (Factor by which Ti(number of epochs between two restarts) increases)\n",
    "    MAX_LR = 1e-2 # for CosineAnnealing (Initial learning rate)\n",
    "    STEPS_PER_EPOCH = 13 # for OneCycleLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbConfig:\n",
    "    WANDB_KEY = \"c5e2877bf080e6b62fcc57231c91e3a1455f97d0\"\n",
    "    WANDB_RUN_NAME = \"tabnet_cv_5folds\"\n",
    "    WANDB_PROJECT = \"ps4e9_nn\"\n",
    "    USE_WANDB = False        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_to_dict(cfg):\n",
    "    # dir is an inbuilt python function that returns the list of attributes and methods of any object\n",
    "    return dict((name, getattr(cfg, name)) for name in dir(cfg) if not name.startswith('__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = config_to_dict(Config)\n",
    "tabnet_config_dict = config_to_dict(TabNetConfig)\n",
    "schd_config_dict = config_to_dict(SchedulerConfig)\n",
    "wandb_config_dict = config_to_dict(WandbConfig)\n",
    "merged_config_dict = {**config_dict, **tabnet_config_dict, **schd_config_dict, **wandb_config_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.USE_MANUAL_FEATURES and not Config.USE_ORIGINAL_DATA:\n",
    "    df_train = pd.read_csv(DATA_READPATH + \"train_preprocessed.csv\")\n",
    "    df_test = pd.read_csv(DATA_READPATH + \"test_preprocessed.csv\")        \n",
    "    # remove rows where price > 2000000\n",
    "    # df_train = df_train[df_train['price'] <= 2000000]\n",
    "elif Config.USE_MANUAL_FEATURES and Config.USE_ORIGINAL_DATA:\n",
    "    df_train = pd.read_csv(DATA_READPATH + \"train_withorig_preprocessed.csv\")\n",
    "    df_test = pd.read_csv(DATA_READPATH + \"test_withorig_preprocessed.csv\")\n",
    "else:\n",
    "    df_train = pd.read_csv(DATA_READPATH + \"train.csv\")\n",
    "    df_test = pd.read_csv(DATA_READPATH + \"test.csv\")\n",
    "# keep a copy of original train and test data for later use\n",
    "df_train_orig = df_train.copy()\n",
    "df_test_orig = df_test.copy()\n",
    "# drop id column\n",
    "df_train = df_train.drop(\"id\", axis=1)\n",
    "df_test = df_test.drop(\"id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = cv_split_utils.strat_kfold_dataframe(df_train, \n",
    "                                                random_state=Config.RANDOM_SEED, \n",
    "                                                num_folds=Config.NUM_FOLDS,\n",
    "                                                target_col_name=Config.TARGET_COL_NAME, \n",
    "                                                n_bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_cols = ['brand', 'model', 'model_year', 'milage', 'fuel_type', 'engine', 'transmission', 'ext_col', 'int_col', 'accident', 'clean_title', 'horsepower', 'capacity', 'cylinders', 'fuel', 'turbo', 'hybrid', 'transmission_type', 'age']\n",
      "cat_cols = ['brand', 'model', 'fuel_type', 'engine', 'transmission', 'ext_col', 'int_col', 'accident', 'clean_title', 'fuel', 'transmission_type']\n",
      "cat_idxs = [0, 1, 4, 5, 6, 7, 8, 9, 10, 14, 17]\n",
      "int_cols = ['model_year', 'milage', 'age']\n",
      "float_cols = ['horsepower', 'capacity', 'cylinders', 'transmission_speed']\n"
     ]
    }
   ],
   "source": [
    "# do not include 'id' column in the list of int columns\n",
    "int_cols = [col for col in df_train.columns if df_train[col].dtypes == 'int64' and col not in COLS_TO_LEAVE]\n",
    "float_cols = [col for col in df_train.columns if df_train[col].dtypes == 'float64']\n",
    "bool_cols = [col for col in df_train.columns if df_train[col].dtypes == 'bool']\n",
    "cat_cols = [col for col in df_train.columns if df_train[col].dtypes == 'object' and col not in COLS_TO_LEAVE]\n",
    "feature_cols = [x for x in df_train.columns if x not in COLS_TO_LEAVE]\n",
    "cat_idxs = [ i for i, f in enumerate(feature_cols) if f in cat_cols]\n",
    "print(f\"feature_cols = {feature_cols}\")\n",
    "print(f\"cat_cols = {cat_cols}\")\n",
    "print(f\"cat_idxs = {cat_idxs}\")\n",
    "print(f\"int_cols = {int_cols}\")\n",
    "print(f\"float_cols = {float_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation_config = {\n",
    "        'horsepower': SimpleImputer(strategy=\"median\"),\n",
    "        'capacity': SimpleImputer(strategy=\"median\"),\n",
    "        'cylinders': SimpleImputer(strategy=\"median\"),\n",
    "        'transmission_speed': SimpleImputer(strategy=\"median\"),\n",
    "    }\n",
    "for column, imputer in imputation_config.items():\n",
    "    imputer.fit(df_train[[column]])\n",
    "    df_train[column] = imputer.transform(df_train[[column]])\n",
    "    if column != 'horsepower':\n",
    "        # convert column datatype to int\n",
    "        df_train[column] = df_train[column].round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle categorical columns\n",
    "cat_cols = df_train.select_dtypes(include=['object']).columns\n",
    "cat_cols_dims = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_train[col] = le.fit_transform(df_train[col].astype(str))\n",
    "    cat_cols_dims[col] = len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "num_cols = int_cols + float_cols\n",
    "df_train[num_cols] = scaler.fit_transform(df_train[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"turbo\"] = df_train[\"turbo\"].astype(int)\n",
    "df_train[\"hybrid\"] = df_train[\"hybrid\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_dataloaders(fold, df, feature_cols, target_col_name):\n",
    "    df_train = df[df[\"kfold\"] != fold].reset_index(drop=True)\n",
    "    df_val = df[df[\"kfold\"] == fold].reset_index(drop=True)\n",
    "    train_X = df_train.loc[:, feature_cols].values\n",
    "    train_y = df_train.loc[:, target_col_name].values\n",
    "    val_X = df_val.loc[:, feature_cols].values\n",
    "    val_y = df_val.loc[:, target_col_name].values\n",
    "    ds_train = TensorDataset(torch.Tensor(train_X), torch.Tensor(train_y))\n",
    "    ds_val = TensorDataset(torch.Tensor(val_X), torch.Tensor(val_y))    \n",
    "    dl_train = DataLoader(ds_train, batch_size=TabNetConfig.BATCH_SIZE, shuffle=True, num_workers=TabNetConfig.NUM_WORKERS)\n",
    "    dl_val = DataLoader(ds_val, batch_size=TabNetConfig.BATCH_SIZE, shuffle=False, num_workers=TabNetConfig.NUM_WORKERS)    \n",
    "    return dl_train, dl_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PyTorch Lightning Module\n",
    "class TabNetLitModel(pl.LightningModule):\n",
    "    def __init__(self, cat_idxs, cat_dims, device):\n",
    "        super(TabNetLitModel, self).__init__()\n",
    "        self.model = TabNetRegressor(\n",
    "            n_d=64, n_a=64, n_steps=5,\n",
    "            gamma=1.5, n_independent=2, n_shared=2,\n",
    "            cat_idxs=cat_idxs, cat_dims=cat_dims,\n",
    "            lambda_sparse=1e-3, momentum=0.3, clip_value=2.,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params=dict(lr=5e-2),\n",
    "            scheduler_params=dict(mode=\"min\",\n",
    "                                patience=5,\n",
    "                                min_lr=1e-5,\n",
    "                                factor=0.9),\n",
    "            scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "            mask_type='sparsemax',\n",
    "            device_name=device        \n",
    "        )\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self.model.predict(X)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        rmse = torch.sqrt(loss)        \n",
    "        # Log loss and RMSE to Weights & Biases\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log(\"train_rmse\", rmse, on_step=True, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self.model.predict(X)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        rmse = torch.sqrt(loss)\n",
    "        current_lr = self.trainer.optimizers[0].param_groups[0]['lr']        \n",
    "        # Log validation metrics\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"val_rmse\", rmse, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"cur_lr\", current_lr, prog_bar=True, on_step=True, on_epoch=True, logger=True)\n",
    "        return {\"val_loss\": loss, \"val_rmse\": rmse}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "         # We extract the model's parameters to provide them to the optimizer\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=5e-2)\n",
    "        #optimizer = torch.optim.Adam(self.parameters(), lr=5e-2)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=5, factor=0.9, min_lr=1e-5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, dl_train, dl_val, find_lr=True):\n",
    "    try:\n",
    "        fold_str = f\"fold{fold}\"\n",
    "        print(f\"Running training for {fold_str}\")\n",
    "        logger = None\n",
    "        if WandbConfig.USE_WANDB:                \n",
    "            logger = dl_utils.get_wandb_logger(fold, merged_config_dict)\n",
    "        print(\"Instantiated wandb logger\")    \n",
    "        chkpt_file_name = \"best_model_{epoch}_{val_loss:.4f}\"                \n",
    "        early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=TabNetConfig.PATIENCE, mode=\"min\", verbose=True)\n",
    "        if fold is not None:       \n",
    "            chkpt_file_name = fold_str + \"_\" + chkpt_file_name\n",
    "        tabnet_model = TabNetLitModel(\n",
    "            cat_idxs=cat_idxs, \n",
    "            cat_dims=list(cat_cols_dims.values()), \n",
    "            device=TabNetConfig.DEVICE\n",
    "        )    \n",
    "        loss_chkpt_callback = ModelCheckpoint(dirpath=\"./model\", verbose=True, monitor=\"val_loss\", mode=\"min\", filename=chkpt_file_name)\n",
    "        rmse_chkpt_callback = dl_utils.MetricsAggCallback(metric_to_monitor=\"val_rmse\", mode=\"min\")\n",
    "        trainer = pl.Trainer(\n",
    "            devices=\"auto\",\n",
    "            accelerator=\"gpu\",\n",
    "            # For results reproducibility \n",
    "            deterministic=True,\n",
    "            strategy=\"auto\",\n",
    "            log_every_n_steps=TabNetConfig.LOG_EVERY_N_STEPS,\n",
    "            max_epochs=TabNetConfig.NUM_EPOCHS,        \n",
    "            precision=TabNetConfig.PRECISION,   \n",
    "            enable_model_summary=True,\n",
    "            enable_progress_bar=True,                        \n",
    "            logger=logger,\n",
    "            fast_dev_run=False,            \n",
    "            callbacks=[loss_chkpt_callback, rmse_chkpt_callback, early_stopping_callback]\n",
    "        )\n",
    "        tuner = Tuner(trainer)\n",
    "        \n",
    "        if find_lr:\n",
    "            lr_finder = tuner.lr_find(model=tabnet_model, train_dataloaders=dl_train)\n",
    "            # Results can be found in\n",
    "            print(lr_finder.results)\n",
    "            # Results can be plotted to identify the optimal learning rate\n",
    "            fig = lr_finder.plot(suggest=True)\n",
    "            fig.show()\n",
    "            # Pick the suggested learning rate\n",
    "            new_lr = lr_finder.suggestion()\n",
    "            print(f\"new_lr = {new_lr}\")\n",
    "\n",
    "        trainer.fit(tabnet_model, train_dataloaders=dl_train, val_dataloaders=dl_val)                \n",
    "        loss = loss_chkpt_callback.best_model_score.detach().cpu().item()\n",
    "        rmse = rmse_chkpt_callback.best_metric\n",
    "        print(f\"Loss for {fold_str} = {loss}, rmse = {rmse}\")\n",
    "        del trainer, tuner, tabnet_model, early_stopping_callback, rmse_chkpt_callback, loss_chkpt_callback\n",
    "        return loss, rmse\n",
    "    except KeyboardInterrupt as e:\n",
    "        wandb.finish(exit_code=-1, quiet=True)\n",
    "        print(\"Marked the wandb run as failed\")\n",
    "    finally:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_loss = []\n",
    "# fold_rmse = []\n",
    "\n",
    "# for fold in range(Config.NUM_FOLDS):\n",
    "#     dl_train, dl_val = get_fold_dataloaders(fold, df_train, feature_cols, Config.TARGET_COL_NAME)\n",
    "#     loss, rmse = run_training(fold, dl_train, dl_val, find_lr=False)\n",
    "#     fold_loss.append(loss)\n",
    "#     fold_rmse.append(rmse)\n",
    "#     break     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize wandb\n",
    "# wandb.init(project=WandbConfig.WANDB_PROJECT, name=WandbConfig.WANDB_RUN_NAME)\n",
    "# wandb.login(key=WandbConfig.WANDB_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'WandbCallback' object has no attribute 'on_train_begin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 48\u001b[0m\n\u001b[1;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m TabNetRegressor(\n\u001b[1;32m     32\u001b[0m     n_d\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, n_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     33\u001b[0m     gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m, n_independent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, n_shared\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     device_name\u001b[38;5;241m=\u001b[39mdevice        \n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvirtual_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrmse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mWandbCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m     62\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:250\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations, compute_importance)\u001b[0m\n\u001b[1;32m    248\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading weights from unsupervised pretraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Call method on_train_begin for all callbacks\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Training loop over epochs\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs):\n\u001b[1;32m    254\u001b[0m \n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:87\u001b[0m, in \u001b[0;36mCallbackContainer.on_train_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m     85\u001b[0m logs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m---> 87\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_begin\u001b[49m(logs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WandbCallback' object has no attribute 'on_train_begin'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 2. Cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "X = df_train[feature_cols].values\n",
    "y = df_train[Config.TARGET_COL_NAME].values.reshape(-1, 1)\n",
    "\n",
    "# 3. Model training and evaluation\n",
    "rmse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "    print(f\"Fold {fold}\")\n",
    "    \n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # class WandbCallback:\n",
    "    #     def set_trainer(self, trainer):\n",
    "    #         pass  # Added to avoid the AttributeError\n",
    "        \n",
    "    #     def __call__(self, step, loss, y_true, y_pred, metrics):\n",
    "    #         wandb.log({\n",
    "    #             f\"fold_{fold}/train_loss\": loss,\n",
    "    #             f\"fold_{fold}/val_rmse\": metrics[\"rmse\"],\n",
    "    #             \"step\": step\n",
    "    #         })\n",
    "\n",
    "    # By default, PyTorch TabNet uses:cat_emb_dim = min(50, (cat_dim + 1) // 2) if cat_emb_dim is not specified.    \n",
    "    model = TabNetRegressor(\n",
    "        n_d=64, n_a=64, n_steps=5,\n",
    "        gamma=1.5, n_independent=2, n_shared=2,\n",
    "        cat_idxs=cat_idxs, cat_dims=list(cat_cols_dims.values()),\n",
    "        lambda_sparse=1e-3, momentum=0.3, clip_value=2.,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=SchedulerConfig.MAX_LR, weight_decay=TabNetConfig.WEIGHT_DECAY),\n",
    "        # scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        # scheduler_params=dict(mode=\"min\",\n",
    "        #                       patience=SchedulerConfig.SCHEDULER_PATIENCE,\n",
    "        #                       min_lr=SchedulerConfig.MIN_LR,\n",
    "        #                       factor=SchedulerConfig.FACTOR),\n",
    "        scheduler_fn=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "        scheduler_params=dict(T_0=SchedulerConfig.T_0,\n",
    "                              T_mult=SchedulerConfig.T_mult,\n",
    "                              eta_min=SchedulerConfig.MIN_LR),\n",
    "        mask_type='sparsemax',\n",
    "        device_name=TabNetConfig.DEVICE        \n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        max_epochs=TabNetConfig.NUM_EPOCHS,\n",
    "        patience=TabNetConfig.PATIENCE,\n",
    "        batch_size=TabNetConfig.BATCH_SIZE,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=TabNetConfig.NUM_WORKERS,\n",
    "        drop_last=False,\n",
    "        eval_metric=[\"rmse\"],\n",
    "        #callbacks=[WandbCallback()]\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))        \n",
    "    rmse_scores.append(rmse)        \n",
    "    print(f\"RMSE: {rmse:.4f}\")    \n",
    "    print()\n",
    "    # # Log final fold results to wandb\n",
    "    # wandb.log({\n",
    "    #     f\"fold_{fold}/final_rmse\": rmse\n",
    "    # })\n",
    "\n",
    "# Print and log average scores\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "std_rmse = np.std(rmse_scores)\n",
    "print(f\"Average RMSE: {avg_rmse:.4f} (+/- {std_rmse:.4f})\")\n",
    "\n",
    "# wandb.log({\n",
    "#     \"average_rmse\": avg_rmse,\n",
    "#     \"std_rmse\": std_rmse\n",
    "# })\n",
    "\n",
    "# # Finish the wandb run\n",
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
