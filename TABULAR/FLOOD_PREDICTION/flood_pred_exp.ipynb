{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q openfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "import statistics\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, SplineTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import clone\n",
    "from functools import partial\n",
    "from joblib import dump\n",
    "from scipy.stats import skew, kurtosis\n",
    "from openfe import OpenFE, transform\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"/home/bk_anupam/code/ML/ML_UTILS/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_tabular_utils as tt\n",
    "import cv_split_utils\n",
    "import enums\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    RUN_MODE = \"LOCAL\"\n",
    "    RANDOM_SEED = 1\n",
    "    NUM_FOLDS = 5\n",
    "    TARGET_COL_NAME = \"FloodProbability\"        \n",
    "    SCALER = enums.Scaler.StandardScaler\n",
    "    METRIC = enums.Metrics.R2\n",
    "    # These values are more dynamic   \n",
    "    MODEL_TYPE = enums.ModelName.Ridge    \n",
    "    NUM_TUNING_TRIALS = 25\n",
    "    TUNE_ON_SINGLE_FOLD = True\n",
    "    TRAIN_SINGLE_FOLD = False\n",
    "    GENERATE_AUTO_FEATURES = False\n",
    "    PERSIST_MODEL = False\n",
    "    TRANSFORM_TARGET = False\n",
    "\n",
    "COLS_TO_LEAVE = [\"FloodProbability\", \"kfold\"]\n",
    "CPU_COUNT = os.cpu_count()\n",
    "\n",
    "DATA_READPATH = \"./data/\"\n",
    "DATA_WRITEPATH = \"./output/\"\n",
    "SUBMISSION_FILEPATH = DATA_READPATH\n",
    "if Config.RUN_MODE == \"KAGGLE\":\n",
    "    # If we are not generating features, we are using already generated features\n",
    "    if Config.GENERATE_AUTO_FEATURES:\n",
    "        DATA_READPATH = \"/kaggle/input/playground-series-s4e5/\"\n",
    "        SUBMISSION_FILEPATH = DATA_READPATH\n",
    "    else:\n",
    "        DATA_READPATH = \"/kaggle/input/playground-series-s4e5/\"\n",
    "        SUBMISSION_FILEPATH = \"/kaggle/input/playground-series-s4e5/\"\n",
    "    DATA_WRITEPATH = \"/kaggle/working/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_static_params = {\n",
    "    enums.ModelName.XGBoost: {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"seed\": Config.RANDOM_SEED,\n",
    "        \"verbosity\": 0,\n",
    "    },\n",
    "    enums.ModelName.LGBM: {\n",
    "        \"objective\": \"root_mean_squared_error\",\n",
    "        \"metric\": 'rmse',\n",
    "        \"verbosity\": -1,    # <0: fatal, =0: error (warn), =1: info, >1: debug\n",
    "        \"boosting_type\": \"gbdt\"\n",
    "    },\n",
    "    enums.ModelName.CatBoost: {\n",
    "        \"objective\": \"RMSE\",\n",
    "        \"verbose\": 0,\n",
    "        \"random_seed\": Config.RANDOM_SEED,\n",
    "        \"eval_metric\": \"RMSE\",\n",
    "        'grow_policy':  'Lossguide'\n",
    "    },\n",
    "    enums.ModelName.RandomForest: {\n",
    "        \"random_state\": Config.RANDOM_SEED,\n",
    "        \"n_jobs\": -1\n",
    "    },\n",
    "    enums.ModelName.Ridge: {\n",
    "        \"random_state\": Config.RANDOM_SEED\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train dataset locally from data folder\n",
    "df_train = pd.read_csv(DATA_READPATH + \"train.csv\", index_col='id')\n",
    "# import test dataset locally from data folder\n",
    "df_test = pd.read_csv(DATA_READPATH + \"test.csv\", index_col='id')\n",
    "# keep a copy of original train and test data for later use\n",
    "df_train_orig = df_train.copy()\n",
    "df_test_orig = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_for_fe = df_test.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_features(df_train, df_test, feature_cols, NUM_NEW_FEATURES=10):\n",
    "    train_X = df_train[feature_cols] \n",
    "    test_X = df_test[feature_cols]   \n",
    "    train_y = df_train[Config.TARGET_COL_NAME]\n",
    "    ofe = OpenFE()\n",
    "    features = ofe.fit(data=train_X, label=train_y, n_jobs=CPU_COUNT, verbose=False)  # generate new features\n",
    "    # OpenFE recommends a list of new features. We include the top 10\n",
    "    # generated features to see how they influence the model performance\n",
    "    train_X, test_X = transform(train_X, test_X, ofe.new_features_list[:NUM_NEW_FEATURES], n_jobs=CPU_COUNT)\n",
    "    return train_X, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.GENERATE_AUTO_FEATURES:\n",
    "    df_train, df_test = generate_new_features(df_train, df_test, feature_cols_for_fe)    \n",
    "    df_train_labels = df_train_orig[[Config.TARGET_COL_NAME]]\n",
    "    # Add the label data to the dataframe\n",
    "    df_train = pd.concat([df_train, df_train_labels], axis=1)\n",
    "    # save the new train and test data with openfe features to csv files for later use\n",
    "    df_train.to_csv(DATA_WRITEPATH + \"train_openfe.csv\", index=False)\n",
    "    df_test.to_csv(DATA_WRITEPATH + \"test_openfe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute skewness and kurtosis for each row\n",
    "def compute_skew_kurtosis(matrix):\n",
    "    skewness = skew(matrix, axis=1)\n",
    "    kurt = kurtosis(matrix, axis=1)\n",
    "    return skewness, kurt\n",
    "\n",
    "def create_features(df, feature_cols):\n",
    "    # Create a new feature by summing all features\n",
    "    df[\"f_sum\"] = df[feature_cols].sum(axis=1)\n",
    "    # Create a new feature by taking mean of all features\n",
    "    df[\"f_mean\"] = df[feature_cols].mean(axis=1)\n",
    "    # standard deviation\n",
    "    df['f_std'] = df[feature_cols].std(axis=1)\n",
    "    # min and max\n",
    "    df['f_min'] = df[feature_cols].min(axis=1)\n",
    "    df['f_max'] = df[feature_cols].max(axis=1)\n",
    "    # Compute skewness and kurtosis\n",
    "    skewness, kurt = compute_skew_kurtosis(df[feature_cols].values)\n",
    "    df['f_skew'] = skewness\n",
    "    df['f_kurtosis'] = kurt    \n",
    "    # Quantiles\n",
    "    quantiles = [0.25, 0.5, 0.75]\n",
    "    for q in quantiles:\n",
    "        df[f'f_quantile_{int(q*100)}'] = df[feature_cols].quantile(q=q, axis=1)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = create_features(df_train, feature_cols_for_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_data(preprocessor, df_train, df_test):\n",
    "#     preprocessor.fit(df_train)\n",
    "#     col_names = preprocessor.get_feature_names_out()\n",
    "#     X_train = preprocessor.transform(df_train)\n",
    "#     #X_test = preprocessor.transform(df_test)\n",
    "#     df_train_fold_target = df_train[COLS_TO_LEAVE]\n",
    "#     df_train_processed = pd.concat([df_train_fold_target, pd.DataFrame(X_train, columns=col_names)], axis=1) \n",
    "#     #df_test_processed = pd.DataFrame(X_test, columns=col_names)\n",
    "#     return df_train_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_processed = preprocess_data(preprocessor=preprocessor, df_train=df_train, df_test=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tuning_params(trial, model_name):\n",
    "    if model_name == enums.ModelName.Ridge:\n",
    "        return {\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-4, 1e4, log=True)\n",
    "        }\n",
    "    if model_name == enums.ModelName.Lasso:\n",
    "        return {\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-4, 1e4, log=True)\n",
    "        }\n",
    "    if model_name == enums.ModelName.RandomForest:\n",
    "        return {        \n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 3000, step=100),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 10, 30),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 2, 16),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 16),\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"log2\", \"sqrt\", None])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparams_tuning_objective(trial, model_name, df_train,  \n",
    "                                 feature_cols, metric, target_col_name, single_fold=False,\n",
    "                                 num_folds=5, val_preds_col=\"val_preds\"):           \n",
    "    model_params = get_model_tuning_params(trial, model_name)    \n",
    "    fold_metrics_model, df_val_preds = tt.run_training(\n",
    "        model_name=model_name,\n",
    "        df_train=df_train,\n",
    "        target_col_name=target_col_name,\n",
    "        feature_col_names=feature_cols,\n",
    "        metric=metric,            \n",
    "        num_folds=num_folds,\n",
    "        model_params=model_params,\n",
    "        val_preds_col=val_preds_col,\n",
    "        single_fold=single_fold,\n",
    "        suppress_print=True,\n",
    "        transform_target=Config.TRANSFORM_TARGET\n",
    "    )       \n",
    "    fold_metrics = [x[0] for x in fold_metrics_model]\n",
    "    mean_metric = statistics.mean(fold_metrics)                \n",
    "    return mean_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model_params(study_name, study_direction, num_trials, model_name, \n",
    "                      df_train,  feature_cols, metric, target_col_name, \n",
    "                      single_fold=False, num_folds=5, val_preds_col=\"val_preds\"):\n",
    "    model_params_tuning_obj_partial = partial(\n",
    "        hyperparams_tuning_objective,\n",
    "        model_name=model_name,        \n",
    "        df_train=df_train,\n",
    "        feature_cols=feature_cols,\n",
    "        metric=metric,\n",
    "        target_col_name=target_col_name,\n",
    "        single_fold=single_fold,\n",
    "        num_folds=num_folds,\n",
    "        val_preds_col=val_preds_col\n",
    "    )\n",
    "    study = optuna.create_study(direction=study_direction, study_name=study_name)\n",
    "    study.optimize(model_params_tuning_obj_partial, n_trials=num_trials)\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"Best trial: number = {best_trial.number}, value = {best_trial.value}, params = {best_trial.params}\")\n",
    "    return best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if tuned_model_params is None:\n",
    "#     tuned_model_params = tune_model_params(\n",
    "#                             study_name=Config.MODEL_TYPE + \"_ModelTuning\", \n",
    "#                             study_direction=\"maximize\",\n",
    "#                             num_trials=Config.NUM_TUNING_TRIALS,\n",
    "#                             model_name=Config.MODEL_TYPE,\n",
    "#                             df_train=df_train,\n",
    "#                             feature_cols=feature_cols,\n",
    "#                             metric=Config.METRIC,\n",
    "#                             target_col_name=Config.TARGET_COL_NAME,\n",
    "#                             single_fold=Config.TUNE_ON_SINGLE_FOLD,\n",
    "#                             num_folds=Config.NUM_FOLDS\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = None\n",
    "params_static = model_static_params.get(Config.MODEL_TYPE)\n",
    "if params_static is not None and tuned_model_params is not None:\n",
    "    model_params = {**model_static_params[Config.MODEL_TYPE], **tuned_model_params}\n",
    "else:\n",
    "    model_params = tuned_model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a new feature by summing all features\n",
    "# df_train[\"f_sum\"] = df_train[feature_cols_for_fe].sum(axis=1)\n",
    "# df_train['special1'] = df_train['f_sum'].isin(np.arange(72, 76)) # for linear models\n",
    "# # Create a new feature by taking mean of all features\n",
    "# df_train[\"f_mean\"] = df_train[feature_cols_for_fe].mean(axis=1)\n",
    "# # standard deviation\n",
    "# df_train['f_std'] = df_train[feature_cols_for_fe].std(axis=1)\n",
    "# # min and max\n",
    "# df_train['f_min'] = df_train[feature_cols_for_fe].min(axis=1)\n",
    "# df_train['f_max'] = df_train[feature_cols_for_fe].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols= [x for x in df_train.columns.to_list() if x not in COLS_TO_LEAVE]\n",
    "print(f\"len(feature_cols)={len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_cols = feature_cols_for_fe + ['f_sum','special1', 'f_mean', 'f_std', 'f_min', 'f_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "polynomial_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "spline_transformer = SplineTransformer(n_knots=5, degree=3, include_bias=False)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[        \n",
    "        #(\"poly\", polynomial_features, feature_cols),\n",
    "        (\"scaler\", scaler, feature_cols),\n",
    "        (\"onehot\", onehot_encoder, ['f_sum']),        \n",
    "    ], remainder=\"passthrough\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tt.get_model(model_name=Config.MODEL_TYPE, params=model_params, metric=Config.METRIC)\n",
    "model_pipeline = make_pipeline(preprocessor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model_name, model, df, feature_cols, target_col_name, metric, n_repeat=1, single_fold=False, num_folds=5):    \n",
    "    df = cv_split_utils.kfold_dataframe(df, random_state=Config.RANDOM_SEED, num_folds=Config.NUM_FOLDS)\n",
    "    df_oof_preds = pd.DataFrame()\n",
    "    fold_metrics_model = []\n",
    "    for fold in range(num_folds):\n",
    "        fold_model = clone(model)\n",
    "        df_train_fold, df_val_fold = tt.get_fold_df(df, fold)\n",
    "        train_X, train_y, val_X, val_y = tt.get_train_val_nparray(df_train_fold, df_val_fold, feature_cols, target_col_name)\n",
    "        fold_model.fit(train_X, train_y)\n",
    "        val_y_pred = fold_model.predict(val_X)\n",
    "        fold_val_metric = tt.get_eval_metric(metric, val_y, val_y_pred)\n",
    "        print(f\"Fold {fold} - {model_name} - {metric} : {fold_val_metric}\")\n",
    "        df_fold_val_preds = df_val_fold[['kfold', target_col_name]]\n",
    "        df_fold_val_preds['oof_preds'] = val_y_pred\n",
    "        df_oof_preds = pd.concat([df_oof_preds, df_fold_val_preds], axis=0)\n",
    "        fold_metrics_model.append((fold_val_metric, fold_model))\n",
    "        if single_fold:\n",
    "            break\n",
    "    cv = tt.get_eval_metric(metric, df_oof_preds[target_col_name], df_oof_preds['oof_preds'] )\n",
    "    print(f\"{model_name} metric={metric} CV score = {cv}\")\n",
    "    metrics = [item[0] for item in fold_metrics_model]\n",
    "    mean_metric, std_metric = tt.get_metric_stats(metrics)    \n",
    "    print(f\"{model_name} Mean {metric} = {mean_metric}, std = {std_metric}\")        \n",
    "    return fold_metrics_model, df_oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist(model_name, fold_metrics_model, df_oof_preds, persist_model=False, output_path=\"\"):    \n",
    "    fold_models = [item[1] for item in fold_metrics_model]    \n",
    "    if persist_model:\n",
    "        for index, model in enumerate(fold_models):\n",
    "            fold_model_name = output_path + f\"{model_name}_{index}.joblib\"        \n",
    "            dump(model, fold_model_name)\n",
    "            print(f\"saved {fold_model_name}\")    \n",
    "    df_oof_preds.to_csv(output_path + f\"df_val_preds_{model_name}.csv\")\n",
    "    print(f\"Saved validation data predictions to df_val_preds_{model_name}.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 - Ridge - R2 : 0.8657955048767867\n",
      "Fold 1 - Ridge - R2 : 0.8660801615163919\n",
      "Fold 2 - Ridge - R2 : 0.865600674467472\n",
      "Fold 3 - Ridge - R2 : 0.8658030180796936\n",
      "Fold 4 - Ridge - R2 : 0.8661479357837484\n",
      "Ridge metric=R2 CV score = 0.8658857435280796\n",
      "Ridge Mean R2 = 0.8658854589448184, std = 0.00020139196184924065\n",
      "Saved validation data predictions to df_val_preds_Ridge.csv\n"
     ]
    }
   ],
   "source": [
    "fold_metrics_model, df_oof_preds = train_and_validate(\n",
    "                                        model_name=Config.MODEL_TYPE,\n",
    "                                        model=model_pipeline,\n",
    "                                        df=df_train,\n",
    "                                        feature_cols=feature_cols,\n",
    "                                        target_col_name=Config.TARGET_COL_NAME,\n",
    "                                        metric=Config.METRIC,\n",
    "                                        single_fold=Config.TRAIN_SINGLE_FOLD,\n",
    "                                        num_folds=Config.NUM_FOLDS\n",
    "                                    )\n",
    "\n",
    "persist(\n",
    "        model_name=Config.MODEL_TYPE, \n",
    "        fold_metrics_model=fold_metrics_model, \n",
    "        df_oof_preds=df_oof_preds, \n",
    "        persist_model=Config.PERSIST_MODEL, \n",
    "        output_path=DATA_WRITEPATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "from colorama import Fore, Style\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "SINGLE_FOLD = True\n",
    "\n",
    "def cross_validate(model, train, label, features=feature_cols, n_repeats=1):\n",
    "    \"\"\"Compute out-of-fold and test predictions for a given model.\n",
    "    \n",
    "    Out-of-fold and test predictions are stored in the global variables\n",
    "    oof and test_pred, respectively.\n",
    "    \n",
    "    If n_repeats > 1, the model is trained several times with different seeds.\n",
    "    \"\"\"\n",
    "    start_time = datetime.datetime.now()\n",
    "    scores = []\n",
    "    oof_preds = np.full_like(train.FloodProbability, np.nan, dtype=float)\n",
    "    for fold, (idx_tr, idx_va) in enumerate(kf.split(train)):\n",
    "        X_tr = train.iloc[idx_tr][features]\n",
    "        X_va = train.iloc[idx_va][features]\n",
    "        y_tr = train.iloc[idx_tr].FloodProbability\n",
    "        y_va = train.iloc[idx_va].FloodProbability\n",
    "        \n",
    "        y_pred = np.zeros_like(y_va, dtype=float)\n",
    "        for i in range(n_repeats):\n",
    "            m = clone(model)\n",
    "            if n_repeats > 1:\n",
    "                mm = m\n",
    "                if isinstance(mm, Pipeline):\n",
    "                    mm = mm[-1]\n",
    "                mm.set_params(random_state=i)\n",
    "            m.fit(X_tr, y_tr)\n",
    "            y_pred += m.predict(X_va)\n",
    "        y_pred /= n_repeats                \n",
    "        score = r2_score(y_va, y_pred)\n",
    "        print(f\"# Fold {fold}: R2={score:.5f}\")\n",
    "        scores.append(score)\n",
    "        oof_preds[idx_va] = y_pred\n",
    "        if Config.TRAIN_SINGLE_FOLD: break\n",
    "            \n",
    "    elapsed_time = datetime.datetime.now() - start_time\n",
    "    print(f\"{Fore.GREEN}# Overall: {np.array(scores).mean():.5f} {label}\"\n",
    "          f\"{' single fold' if SINGLE_FOLD else ''}\"\n",
    "          f\"   {int(np.round(elapsed_time.total_seconds() / 60))} min{Style.RESET_ALL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross_validate(model_pipeline, df_train, \"Ridge\", features=feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_metrics_model = tt.train_model(\n",
    "#                             df = df_train_processed,\n",
    "#                             model_name=Config.MODEL_TYPE,\n",
    "#                             model_params = model_params,\n",
    "#                             feature_col_names = feature_cols,\n",
    "#                             target_col_name = Config.TARGET_COL_NAME,\n",
    "#                             metric = Config.METRIC,\n",
    "#                             num_folds = Config.NUM_FOLDS,\n",
    "#                             single_fold = Config.TRAIN_SINGLE_FOLD,\n",
    "#                             persist_model = Config.PERSIST_MODEL,\n",
    "#                             output_path = DATA_WRITEPATH,\n",
    "#                             transform_target = Config.TRANSFORM_TARGET\n",
    "#                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "# r2_score(df_train.FloodProbability, (df_train[feature_cols_for_fe].sum(axis=1) * 0.0056) - 0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
