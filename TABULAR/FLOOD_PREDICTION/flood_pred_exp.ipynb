{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q openfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "import statistics\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, SplineTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from functools import partial\n",
    "from scipy.stats import skew, kurtosis\n",
    "from openfe import OpenFE, transform\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"/home/bk_anupam/code/ML/ML_UTILS/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_tabular_utils as tt\n",
    "import cv_split_utils\n",
    "import enums\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    RUN_MODE = \"LOCAL\"\n",
    "    RANDOM_SEED = 42\n",
    "    NUM_FOLDS = 5\n",
    "    TARGET_COL_NAME = \"FloodProbability\"        \n",
    "    SCALER = enums.Scaler.StandardScaler\n",
    "    METRIC = enums.Metrics.R2\n",
    "    # These values are more dynamic   \n",
    "    MODEL_TYPE = enums.ModelName.Ridge    \n",
    "    NUM_TUNING_TRIALS = 25\n",
    "    TUNE_ON_SINGLE_FOLD = True\n",
    "    TRAIN_SINGLE_FOLD = False\n",
    "    GENERATE_AUTO_FEATURES = False\n",
    "    PERSIST_MODEL = False\n",
    "    TRANSFORM_TARGET = False\n",
    "\n",
    "COLS_TO_LEAVE = [\"FloodProbability\", \"kfold\"]\n",
    "CPU_COUNT = os.cpu_count()\n",
    "\n",
    "DATA_READPATH = \"./data/\"\n",
    "DATA_WRITEPATH = \"./output/\"\n",
    "SUBMISSION_FILEPATH = DATA_READPATH\n",
    "if Config.RUN_MODE == \"KAGGLE\":\n",
    "    # If we are not generating features, we are using already generated features\n",
    "    if Config.GENERATE_AUTO_FEATURES:\n",
    "        DATA_READPATH = \"/kaggle/input/playground-series-s4e5/\"\n",
    "        SUBMISSION_FILEPATH = DATA_READPATH\n",
    "    else:\n",
    "        DATA_READPATH = \"/kaggle/input/playground-series-s4e5/\"\n",
    "        SUBMISSION_FILEPATH = \"/kaggle/input/playground-series-s4e5/\"\n",
    "    DATA_WRITEPATH = \"/kaggle/working/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train dataset locally from data folder\n",
    "df_train = pd.read_csv(DATA_READPATH + \"train.csv\", index_col='id')\n",
    "# import test dataset locally from data folder\n",
    "df_test = pd.read_csv(DATA_READPATH + \"test.csv\", index_col='id')\n",
    "# keep a copy of original train and test data for later use\n",
    "df_train_orig = df_train.copy()\n",
    "df_test_orig = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_for_fe = df_test.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_features(df_train, df_test, feature_cols, NUM_NEW_FEATURES=10):\n",
    "    train_X = df_train[feature_cols] \n",
    "    test_X = df_test[feature_cols]   \n",
    "    train_y = df_train[Config.TARGET_COL_NAME]\n",
    "    ofe = OpenFE()\n",
    "    features = ofe.fit(data=train_X, label=train_y, n_jobs=CPU_COUNT, verbose=False)  # generate new features\n",
    "    # OpenFE recommends a list of new features. We include the top 10\n",
    "    # generated features to see how they influence the model performance\n",
    "    train_X, test_X = transform(train_X, test_X, ofe.new_features_list[:NUM_NEW_FEATURES], n_jobs=CPU_COUNT)\n",
    "    return train_X, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.GENERATE_AUTO_FEATURES:\n",
    "    df_train, df_test = generate_new_features(df_train, df_test, feature_cols_for_fe)    \n",
    "    df_train_labels = df_train_orig[[Config.TARGET_COL_NAME]]\n",
    "    # Add the label data to the dataframe\n",
    "    df_train = pd.concat([df_train, df_train_labels], axis=1)\n",
    "    # save the new train and test data with openfe features to csv files for later use\n",
    "    df_train.to_csv(DATA_WRITEPATH + \"train_openfe.csv\", index=False)\n",
    "    df_test.to_csv(DATA_WRITEPATH + \"test_openfe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, feature_cols):\n",
    "    # Create a new feature by summing all features\n",
    "    df[\"f_sum\"] = df[feature_cols].sum(axis=1)\n",
    "    # Create a new feature by taking mean of all features\n",
    "    df[\"f_mean\"] = df[feature_cols].mean(axis=1)\n",
    "    # standard deviation\n",
    "    df['f_std'] = df[feature_cols].std(axis=1)\n",
    "    # min and max\n",
    "    df['f_min'] = df[feature_cols].min(axis=1)\n",
    "    df['f_max'] = df[feature_cols].max(axis=1)\n",
    "    # Skewness and kurtosis\n",
    "    df['f_skew'] = df[feature_cols].apply(lambda row: skew(row), axis=1)\n",
    "    df['f_kurtosis'] = df[feature_cols].apply(lambda row: kurtosis(row), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = create_features(df_train, feature_cols_for_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols= [x for x in df_train.columns.to_list() if x not in COLS_TO_LEAVE]\n",
    "print(f\"len(feature_cols)={len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = cv_split_utils.kfold_dataframe(df_train, random_state=Config.RANDOM_SEED, num_folds=Config.NUM_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "polynomial_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "spline_transformer = SplineTransformer(n_knots=5, degree=3, include_bias=False)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"scaler\", scaler, feature_cols),\n",
    "        (\"poly\", polynomial_features, feature_cols),\n",
    "        # (\"onehot\", onehot_encoder, feature_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(preprocessor, df_train, df_test):\n",
    "    preprocessor.fit(df_train)\n",
    "    col_names = preprocessor.get_feature_names_out()\n",
    "    X_train = preprocessor.transform(df_train)\n",
    "    #X_test = preprocessor.transform(df_test)\n",
    "    df_train_fold_target = df_train[COLS_TO_LEAVE]\n",
    "    df_train_processed = pd.concat([df_train_fold_target, pd.DataFrame(X_train, columns=col_names)], axis=1) \n",
    "    #df_test_processed = pd.DataFrame(X_test, columns=col_names)\n",
    "    return df_train_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_processed = preprocess_data(preprocessor=preprocessor, df_train=df_train, df_test=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_processed = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(feature_cols)=20\n"
     ]
    }
   ],
   "source": [
    "feature_cols= [x for x in df_train_processed.columns.to_list() if x not in COLS_TO_LEAVE]\n",
    "print(f\"len(feature_cols)={len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tuning_params(trial, model_name):\n",
    "    if model_name == enums.ModelName.Ridge:\n",
    "        return {\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-4, 1e4, log=True)\n",
    "        }\n",
    "    if model_name == enums.ModelName.Lasso:\n",
    "        return {\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-4, 1e4, log=True)\n",
    "        }\n",
    "    if model_name == enums.ModelName.RandomForest:\n",
    "        return {        \n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 3000, step=100),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 10, 30),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 2, 16),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 16),\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"log2\", \"sqrt\", None])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparams_tuning_objective(trial, model_name, df_train,  \n",
    "                                 feature_cols, metric, target_col_name, single_fold=False,\n",
    "                                 num_folds=5, val_preds_col=\"val_preds\"):           \n",
    "    model_params = get_model_tuning_params(trial, model_name)    \n",
    "    fold_metrics_model, df_val_preds = tt.run_training(\n",
    "        model_name=model_name,\n",
    "        df_train=df_train,\n",
    "        target_col_name=target_col_name,\n",
    "        feature_col_names=feature_cols,\n",
    "        metric=metric,            \n",
    "        num_folds=num_folds,\n",
    "        model_params=model_params,\n",
    "        val_preds_col=val_preds_col,\n",
    "        single_fold=single_fold,\n",
    "        suppress_print=True,\n",
    "        transform_target=Config.TRANSFORM_TARGET\n",
    "    )       \n",
    "    fold_metrics = [x[0] for x in fold_metrics_model]\n",
    "    mean_metric = statistics.mean(fold_metrics)                \n",
    "    return mean_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model_params(study_name, study_direction, num_trials, model_name, \n",
    "                      df_train,  feature_cols, metric, target_col_name, \n",
    "                      single_fold=False, num_folds=5, val_preds_col=\"val_preds\"):\n",
    "    model_params_tuning_obj_partial = partial(\n",
    "        hyperparams_tuning_objective,\n",
    "        model_name=model_name,        \n",
    "        df_train=df_train,\n",
    "        feature_cols=feature_cols,\n",
    "        metric=metric,\n",
    "        target_col_name=target_col_name,\n",
    "        single_fold=single_fold,\n",
    "        num_folds=num_folds,\n",
    "        val_preds_col=val_preds_col\n",
    "    )\n",
    "    study = optuna.create_study(direction=study_direction, study_name=study_name)\n",
    "    study.optimize(model_params_tuning_obj_partial, n_trials=num_trials)\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"Best trial: number = {best_trial.number}, value = {best_trial.value}, params = {best_trial.params}\")\n",
    "    return best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if tuned_model_params is None:\n",
    "#     tuned_model_params = tune_model_params(\n",
    "#                             study_name=Config.MODEL_TYPE + \"_ModelTuning\", \n",
    "#                             study_direction=\"maximize\",\n",
    "#                             num_trials=Config.NUM_TUNING_TRIALS,\n",
    "#                             model_name=Config.MODEL_TYPE,\n",
    "#                             df_train=df_train,\n",
    "#                             feature_cols=feature_cols,\n",
    "#                             metric=Config.METRIC,\n",
    "#                             target_col_name=Config.TARGET_COL_NAME,\n",
    "#                             single_fold=Config.TUNE_ON_SINGLE_FOLD,\n",
    "#                             num_folds=Config.NUM_FOLDS\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Ridge\n",
      "Fold 0 - Ridge - R2 : 0.8449679478019123\n",
      "Fold 1 - Ridge - R2 : 0.8454208689127656\n",
      "Fold 2 - Ridge - R2 : 0.845880290194893\n",
      "Fold 3 - Ridge - R2 : 0.8445347013946137\n",
      "Fold 4 - Ridge - R2 : 0.8439095425290722\n",
      "Ridge CV score = 0.8449444689492062\n",
      "Ridge Mean R2 = 0.8449426701666514, std = 0.0006844319617722557\n",
      "Saved validation data predictions to df_val_preds_Ridge.csv\n"
     ]
    }
   ],
   "source": [
    "fold_metrics_model = tt.train_model(\n",
    "                            df = df_train_processed,\n",
    "                            model_name=Config.MODEL_TYPE,\n",
    "                            model_params = model_params,\n",
    "                            feature_col_names = feature_cols,\n",
    "                            target_col_name = Config.TARGET_COL_NAME,\n",
    "                            metric = Config.METRIC,\n",
    "                            num_folds = Config.NUM_FOLDS,\n",
    "                            single_fold = Config.TRAIN_SINGLE_FOLD,\n",
    "                            persist_model = Config.PERSIST_MODEL,\n",
    "                            output_path = DATA_WRITEPATH,\n",
    "                            transform_target = Config.TRANSFORM_TARGET\n",
    "                        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
