{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder #, TargetEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "import optuna.integration.lightgbm as lgb\n",
    "from optuna.integration.lightgbm import LightGBMTunerCV, LightGBMTuner\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelType:\n",
    "    LGBM = \"LGBM\"\n",
    "    XGB = \"XGB\"\n",
    "    RF = \"RF\"\n",
    "    LR = \"LR\"\n",
    "    CATBOOST = \"CATBOOST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    RANDOM_SEED = 42\n",
    "    # Number of target classes\n",
    "    NUM_CLASSES = 3\n",
    "    NUM_FOLDS = 5\n",
    "    TARGET_COL_NAME = \"outcome\"    \n",
    "    EARLY_STOPPING = 500\n",
    "    RESULTS_FILE = \"model_execution_results.pkl\"\n",
    "    MODEL_TYPE = ModelType.RF\n",
    "\n",
    "DATA_PATH = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "df_test = pd.read_csv(DATA_PATH + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>surgery</th>\n",
       "      <th>age</th>\n",
       "      <th>hospital_number</th>\n",
       "      <th>rectal_temp</th>\n",
       "      <th>pulse</th>\n",
       "      <th>respiratory_rate</th>\n",
       "      <th>temp_of_extremities</th>\n",
       "      <th>peripheral_pulse</th>\n",
       "      <th>mucous_membrane</th>\n",
       "      <th>...</th>\n",
       "      <th>total_protein</th>\n",
       "      <th>abdomo_appearance</th>\n",
       "      <th>abdomo_protein</th>\n",
       "      <th>surgical_lesion</th>\n",
       "      <th>lesion_1</th>\n",
       "      <th>lesion_2</th>\n",
       "      <th>lesion_3</th>\n",
       "      <th>cp_data</th>\n",
       "      <th>outcome</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>753</td>\n",
       "      <td>no</td>\n",
       "      <td>adult</td>\n",
       "      <td>535381</td>\n",
       "      <td>39.4</td>\n",
       "      <td>86.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>pale_pink</td>\n",
       "      <td>...</td>\n",
       "      <td>75.0</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>2.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>3205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>euthanized</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>582</td>\n",
       "      <td>yes</td>\n",
       "      <td>adult</td>\n",
       "      <td>535029</td>\n",
       "      <td>37.5</td>\n",
       "      <td>112.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>cold</td>\n",
       "      <td>normal</td>\n",
       "      <td>bright_pink</td>\n",
       "      <td>...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>serosanguious</td>\n",
       "      <td>2.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>4205</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>euthanized</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>548</td>\n",
       "      <td>yes</td>\n",
       "      <td>adult</td>\n",
       "      <td>529461</td>\n",
       "      <td>38.5</td>\n",
       "      <td>72.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>cool</td>\n",
       "      <td>reduced</td>\n",
       "      <td>bright_red</td>\n",
       "      <td>...</td>\n",
       "      <td>8.6</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>4.3</td>\n",
       "      <td>yes</td>\n",
       "      <td>2112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>died</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113</td>\n",
       "      <td>yes</td>\n",
       "      <td>adult</td>\n",
       "      <td>534157</td>\n",
       "      <td>38.4</td>\n",
       "      <td>40.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>cool</td>\n",
       "      <td>reduced</td>\n",
       "      <td>pale_pink</td>\n",
       "      <td>...</td>\n",
       "      <td>77.0</td>\n",
       "      <td>serosanguious</td>\n",
       "      <td>2.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>2209</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>euthanized</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>174</td>\n",
       "      <td>yes</td>\n",
       "      <td>adult</td>\n",
       "      <td>529777</td>\n",
       "      <td>38.9</td>\n",
       "      <td>40.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>pale_pink</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>clear</td>\n",
       "      <td>5.4</td>\n",
       "      <td>yes</td>\n",
       "      <td>2206</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>lived</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id surgery    age  hospital_number  rectal_temp  pulse  respiratory_rate  \\\n",
       "0  753      no  adult           535381         39.4   86.0              21.0   \n",
       "1  582     yes  adult           535029         37.5  112.0              12.0   \n",
       "2  548     yes  adult           529461         38.5   72.0              44.0   \n",
       "3  113     yes  adult           534157         38.4   40.0              16.0   \n",
       "4  174     yes  adult           529777         38.9   40.0              24.0   \n",
       "\n",
       "  temp_of_extremities peripheral_pulse mucous_membrane  ... total_protein  \\\n",
       "0              normal           normal       pale_pink  ...          75.0   \n",
       "1                cold           normal     bright_pink  ...          57.0   \n",
       "2                cool          reduced      bright_red  ...           8.6   \n",
       "3                cool          reduced       pale_pink  ...          77.0   \n",
       "4              normal           normal       pale_pink  ...           6.0   \n",
       "\n",
       "  abdomo_appearance abdomo_protein surgical_lesion lesion_1 lesion_2  \\\n",
       "0            cloudy            2.0             yes     3205        0   \n",
       "1     serosanguious            2.0             yes     4205        0   \n",
       "2            cloudy            4.3             yes     2112        0   \n",
       "3     serosanguious            2.0             yes     2209        0   \n",
       "4             clear            5.4             yes     2206        0   \n",
       "\n",
       "   lesion_3 cp_data     outcome  kfold  \n",
       "0         0      no  euthanized      3  \n",
       "1         0      no  euthanized      0  \n",
       "2         0     yes        died      1  \n",
       "3         0      no  euthanized      3  \n",
       "4         0     yes       lived      1  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the training dataframe into kfolds for cross validation. We do this before any processing is done\n",
    "# on the data. We use stratified kfold if the target distribution is unbalanced\n",
    "def strat_kfold_dataframe(df, target_col_name, num_folds=5):\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    # randomize of shuffle the rows of dataframe before splitting is done\n",
    "    df = df.sample(frac=1, random_state=Config.RANDOM_SEED).reset_index(drop=True)\n",
    "    # get the target data\n",
    "    y = df[target_col_name].values\n",
    "    skf = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=Config.RANDOM_SEED)\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=df, y=y)):\n",
    "        df.loc[val_index, \"kfold\"] = fold    \n",
    "    return df     \n",
    "\n",
    "df_train = strat_kfold_dataframe(df_train, target_col_name=Config.TARGET_COL_NAME, num_folds=Config.NUM_FOLDS)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object     17\n",
       "float64     7\n",
       "int64       6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the count of each column type\n",
    "df_train.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_float = df_train.select_dtypes(include=[\"float\"]).columns.to_list()\n",
    "cols_int = df_train.select_dtypes(include=[\"int64\"]).columns.to_list()\n",
    "cols_str = df_train.select_dtypes(include=[\"object\"]).columns.to_list()\n",
    "# remove target \"outcome\" from the list cols_str\n",
    "cols_str.remove(Config.TARGET_COL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rectal_temp', 'pulse', 'respiratory_rate', 'nasogastric_reflux_ph', 'packed_cell_volume', 'total_protein', 'abdomo_protein']\n"
     ]
    }
   ],
   "source": [
    "print(cols_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['surgery', 'age', 'temp_of_extremities', 'peripheral_pulse', 'mucous_membrane']\n",
      "['capillary_refill_time', 'pain', 'peristalsis', 'abdominal_distention', 'nasogastric_tube']\n",
      "['nasogastric_reflux', 'rectal_exam_feces', 'abdomen', 'abdomo_appearance', 'surgical_lesion']\n",
      "['cp_data']\n"
     ]
    }
   ],
   "source": [
    "# print the list cols_str with 5 words per line\n",
    "def print_list_cols(cols_str):\n",
    "    for i in range(0, len(cols_str), 5):\n",
    "        print(cols_str[i:i+5])\n",
    "\n",
    "print_list_cols(cols_str)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each categorical feature, calculate distinct categories and their counts\n",
    "def get_category_summary(df):\n",
    "    # Initialize an empty DataFrame to store the results\n",
    "    category_summary_list = []\n",
    "    # Loop through columns to identify categorical features\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            # For categorical features, calculate distinct categories and their counts        \n",
    "            cat_val_cnt = df[column].value_counts()        \n",
    "            # create a dataframe for this specific categorical feature, distinct categories and their count\n",
    "            cat_feature_df = pd.DataFrame(data={\n",
    "                'Feature': [column] * len(cat_val_cnt),\n",
    "                'Distinct_Categories': cat_val_cnt.index.values.tolist(), \n",
    "                'Category_Count': cat_val_cnt.values.tolist()\n",
    "            })\n",
    "            # Append the results to the categorsummary DataFrame list\n",
    "            category_summary_list.append(cat_feature_df)\n",
    "    \n",
    "    category_summary = pd.concat(category_summary_list)\n",
    "    return category_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categories_train = get_category_summary(df_train)\n",
    "df_categories_test = get_category_summary(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat_notin_test = df_categories_train[[\"Feature\", \"Distinct_Categories\"]].merge(\n",
    "                            df_categories_test[[\"Feature\", \"Distinct_Categories\"]], \n",
    "                            on=['Feature', 'Distinct_Categories'], \n",
    "                            how='left', \n",
    "                            indicator=True\n",
    "                        ).query('_merge == \"left_only\"')\n",
    "\n",
    "train_cat_notin_test = df_categories_train.merge(\n",
    "                            df_categories_test, \n",
    "                            on=['Feature', 'Distinct_Categories'], \n",
    "                            how='left', \n",
    "                            indicator=True\n",
    "                        ).query('_merge == \"left_only\"')\n",
    "\n",
    "# Find rows present in df2 but missing in df1\n",
    "test_cat_notin_train = df_categories_test[[\"Feature\", \"Distinct_Categories\"]].merge(\n",
    "                            df_categories_train[[\"Feature\", \"Distinct_Categories\"]], \n",
    "                            on=['Feature', 'Distinct_Categories'], \n",
    "                            how='left', \n",
    "                            indicator=True\n",
    "                        ).query('_merge == \"left_only\"')\n",
    "\n",
    "# # Drop the '_merge' column and reset the index\n",
    "train_cat_notin_test = train_cat_notin_test.drop(columns=['_merge']).reset_index(drop=True)\n",
    "test_cat_notin_train = test_cat_notin_train.drop(columns=['_merge']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the row from test which has category not present in train\n",
    "# df_test = df_test.drop(df_test[df_test.pain == \"moderate\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(col_names)=27\n",
      "len(cont_col_names)=7, len(cat_col_names)=16, len(noncont_col_names)=20\n"
     ]
    }
   ],
   "source": [
    "cols_to_leave = [\"id\", \"kfold\", Config.TARGET_COL_NAME, Config.TARGET_COL_NAME + \"_encoded\"]\n",
    "col_names = [item for item in df_train.columns.values.tolist() if item not in cols_to_leave]\n",
    "print(f\"len(col_names)={len(col_names)}\")        \n",
    "# get all columns from df_train that are not of type float\n",
    "noncont_col_names = [item for item in col_names if item not in cols_float]\n",
    "cont_col_names = [item for item in cols_float if item not in cols_to_leave]\n",
    "cat_col_names = [item for item in cols_str if item not in cols_to_leave]\n",
    "print(f\"len(cont_col_names)={len(cont_col_names)}, len(cat_col_names)={len(cat_col_names)}, len(noncont_col_names)={len(noncont_col_names)}\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Categorical Columns using different strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding: For lightGBM categorical features must be encoded as non-negative integers (int) less than Int32.MaxValue (2147483647) 1. This means that you cannot use string or float values for categorical features. You can use various encoding methods, such as label encoding, ordinal encoding, or frequency encoding, to convert categorical values into numeric codes. However, you should avoid using one-hot encoding, as it may reduce the performance and accuracy of LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ordinal encoding of categorical features \n",
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# ordinal_encoder = OrdinalEncoder()\n",
    "# ordinal_encoder = ordinal_encoder.fit(df_train[cat_col_names])\n",
    "# encoded_cat_cols = ordinal_encoder.transform(df_train[cat_col_names])\n",
    "# other_col_names = [item for item in df_train.columns.values.tolist() if item not in cat_col_names]\n",
    "# df_train_cat = pd.DataFrame(encoded_cat_cols, columns=cat_col_names)\n",
    "# df_train_other = df_train[other_col_names]\n",
    "# df_train_processed = pd.concat([df_train_other, df_train_cat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_train.shape = (1235, 72)\n",
      "encoded_test.shape = (824, 72)\n"
     ]
    }
   ],
   "source": [
    "one_hot_enc = OneHotEncoder(handle_unknown=\"infrequent_if_exist\", sparse=False, min_frequency=5)\n",
    "one_hot_enc = one_hot_enc.fit(df_train[cat_col_names])\n",
    "encoded_train = one_hot_enc.transform(df_train[cat_col_names])\n",
    "encoded_test = one_hot_enc.transform(df_test[cat_col_names])\n",
    "print(f\"encoded_train.shape = {encoded_train.shape}\")\n",
    "print(f\"encoded_test.shape = {encoded_test.shape}\")\n",
    "df_train.drop(columns=cat_col_names, inplace=True)\n",
    "df_test.drop(columns=cat_col_names, inplace=True)\n",
    "# drop the categorical columns from df_train\n",
    "df_train_oh = pd.DataFrame(encoded_train, columns=one_hot_enc.get_feature_names_out())\n",
    "df_test_oh = pd.DataFrame(encoded_test, columns=one_hot_enc.get_feature_names_out())\n",
    "# append the one hot encoded data to df_train\n",
    "df_train_processed = pd.concat([df_train, df_train_oh], axis=1)\n",
    "df_test_processed = pd.concat([df_test, df_test_oh], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding of target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target\n",
    "label_encoder = LabelEncoder()\n",
    "target_encoded = label_encoder.fit_transform(df_train[Config.TARGET_COL_NAME])\n",
    "# add the target_encoded as a new column to the dataframe\n",
    "df_train_processed[Config.TARGET_COL_NAME + \"_encoded\"] = target_encoded\n",
    "df_train[Config.TARGET_COL_NAME + \"_encoded\"] = target_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outcome_proba_died', 'outcome_proba_euthanized', 'outcome_proba_lived']"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_proba_cols = [Config.TARGET_COL_NAME + \"_proba_\" + tgt_cls for tgt_cls in label_encoder.classes_]\n",
    "tgt_proba_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df, cont_col_names, cols_to_leave):\n",
    "    # normalize continuous features\n",
    "    scaler = StandardScaler()\n",
    "    X_cont = df[cont_col_names]    \n",
    "    X_cont_scaled = scaler.fit_transform(X_cont)     \n",
    "    # get the columns other than continuous features\n",
    "    other_col_names = [item for item in df.columns.values.tolist() if item not in cont_col_names + cols_to_leave]\n",
    "    # combine the normalized continuous features with others\n",
    "    X_processed = np.concatenate([X_cont_scaled, df[other_col_names]], axis=1)    \n",
    "    return X_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_data(fold, df, cols_to_leave, target_col_name):\n",
    "    df_train = df[df.kfold != fold]\n",
    "    df_val = df[df.kfold == fold]\n",
    "    col_names = [item for item in df_train_processed.columns.values.tolist() if item not in cols_to_leave]\n",
    "    X_train = df_train[col_names]\n",
    "    y_train = df_train[target_col_name]\n",
    "    X_val = df_val[col_names]\n",
    "    y_val = df_val[target_col_name]\n",
    "    return X_train, y_train, X_val, y_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_df, train_y, val_df, val_y, params=None, callbacks=None):\n",
    "    col_names = [item for item in train_df.columns.values.tolist() if item not in cols_to_leave]\n",
    "    train_data = lgbm.Dataset(\n",
    "            data=train_df[col_names], label=train_y, feature_name=col_names#, \n",
    "            #categorical_feature=cat_col_names\n",
    "        )\n",
    "    val_data = lgbm.Dataset(\n",
    "            data=val_df[col_names], label=val_y, feature_name=col_names, \n",
    "            #categorical_feature=cat_col_names, \n",
    "            reference=train_data\n",
    "        )    \n",
    "    if callbacks is not None:        \n",
    "        model = lgbm.train(\n",
    "                    params,\n",
    "                    train_set=train_data,                \n",
    "                    valid_sets=val_data,\n",
    "                    verbose_eval=-1,\n",
    "                    callbacks=callbacks\n",
    "                )\n",
    "    else:\n",
    "        model = lgbm.train(\n",
    "                    params,\n",
    "                    train_set=train_data,                \n",
    "                    valid_sets=val_data,\n",
    "                    verbose_eval=-1\n",
    "                )       \n",
    "    val_pred_probs = model.predict(val_df, num_iteration=model.best_iteration)        \n",
    "    val_preds = np.argmax(val_pred_probs, axis=1)\n",
    "    f1 = f1_score(val_y, val_preds, average=\"micro\")\n",
    "    return f1, model, val_pred_probs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import early_stopping\n",
    "from lightgbm import log_evaluation\n",
    "\n",
    "def tune_params(train_df, train_y, params=None):\n",
    "    col_names = [item for item in train_df.columns.values.tolist() if item not in cols_to_leave]\n",
    "    train_data = lgbm.Dataset(\n",
    "            data=train_df[col_names], label=train_y, feature_name=col_names#, \n",
    "            #categorical_feature=Config.CATEGORICAL_COLS\n",
    "        )    \n",
    "    lgbmtuner_cv = LightGBMTunerCV(\n",
    "        params,\n",
    "        train_set=train_data,        \n",
    "        stratified=True,\n",
    "        shuffle=True,\n",
    "        nfold=Config.NUM_FOLDS,\n",
    "        verbose_eval=-1,\n",
    "        callbacks=[early_stopping(100), log_evaluation(100)]\n",
    "    ) \n",
    "    lgbmtuner_cv.run()                \n",
    "    print(\"Best Params: \", lgbmtuner_cv.best_params)    \n",
    "    print(\"Best score: \", lgbmtuner_cv.best_score)    \n",
    "    return lgbmtuner_cv    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#         \"objective\": \"multiclass\",\n",
    "#         \"num_class\": Config.NUM_CLASSES,\n",
    "#         \"metric\": \"softmax\",\n",
    "#         \"verbosity\": -1,\n",
    "#         \"boosting_type\": \"gbdt\",\n",
    "#     }\n",
    "\n",
    "# train_y = df_train_processed[Config.TARGET_COL_NAME+\"_encoded\"]\n",
    "# tuned_model = tune_params(df_train_processed, train_y, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    "    'objective': 'multiclass', \n",
    "    'num_class': 3, \n",
    "    'metric': 'multi_logloss', \n",
    "    'verbosity': -1, \n",
    "    'boosting_type': 'gbdt', \n",
    "    'feature_pre_filter': False, \n",
    "    'lambda_l1': 7.816997746908849, \n",
    "    'lambda_l2': 1.543480389482826e-08, \n",
    "    'num_leaves': 31, \n",
    "    'feature_fraction': 0.4, \n",
    "    'bagging_fraction': 0.4417446162614227, \n",
    "    'bagging_freq': 4, \n",
    "    'min_child_samples': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params2 = {'objective': 'multiclass', 'num_class': 3, 'metric': 'multi_logloss', 'verbosity': -1, \n",
    "'boosting_type': 'gbdt', 'feature_pre_filter': False, 'lambda_l1': 1.090149418097443, 'lambda_l2': 2.9628897819981876, \n",
    "'num_leaves': 4, 'feature_fraction': 0.58, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 f1 score = 0.7408906882591093\n",
      "fold 1 f1 score = 0.757085020242915\n",
      "fold 2 f1 score = 0.7368421052631579\n",
      "fold 3 f1 score = 0.6923076923076923\n",
      "fold 4 f1 score = 0.728744939271255\n"
     ]
    }
   ],
   "source": [
    "fold_metrics_model = []\n",
    "test_preds = {}\n",
    "\n",
    "for fold in range(Config.NUM_FOLDS):\n",
    "    X_train, y_train, X_val, y_val = get_fold_data(\n",
    "        fold=fold, \n",
    "        df=df_train_processed,         \n",
    "        cols_to_leave=cols_to_leave,\n",
    "        target_col_name=Config.TARGET_COL_NAME+\"_encoded\"\n",
    "    )\n",
    "    fold_f1_score, model, fold_val_pred_proba = run_training(X_train, y_train, X_val, y_val, lgbm_params2)\n",
    "    print(f\"fold {fold } f1 score = {fold_f1_score}\")    \n",
    "    # add the validation probability predictions for the fold to a new column in train data\n",
    "    df_train.loc[df_train.kfold == fold, tgt_proba_cols] = fold_val_pred_proba \n",
    "    # for this fold make predictions on the validation set\n",
    "    # in a multiclass classification setting, the prediction from the model is probability for each target class\n",
    "    # so we need to use the argmax to get the predicted class\n",
    "    df_train.loc[df_train.kfold == fold, \"val_preds\"] = np.argmax(model.predict(X_val), axis=1)\n",
    "    fold_metrics_model.append((round(fold_f1_score, 4), model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 scores = [0.7409, 0.7571, 0.7368, 0.6923, 0.7287]\n",
      "mean f1 across folds = 0.73116, f1 stdev across folds = 0.024060922675575004\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "fold_metrics = [item[0] for item in fold_metrics_model]\n",
    "print(f\"f1 scores = {fold_metrics}\")    \n",
    "cv_auc_mean = statistics.mean(fold_metrics)\n",
    "cv_auc_stdev = statistics.stdev(fold_metrics)\n",
    "print(f\"mean f1 across folds = {cv_auc_mean}, f1 stdev across folds = {cv_auc_stdev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation F1 score across 5 folds = 0.7311740890688259\n"
     ]
    }
   ],
   "source": [
    "# calculate the cv score\n",
    "cv_f1 = f1_score(y_pred=df_train.val_preds, y_true=df_train.outcome_encoded, average=\"micro\")\n",
    "print(f\"Cross validation F1 score across {len(fold_metrics)} folds = {cv_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
