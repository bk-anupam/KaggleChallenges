{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF files:  ['df_val_preds_CatBoost1.csv' 'df_val_preds_CatBoost2.csv'\n",
      " 'df_val_preds_LightGBM1.csv' 'df_val_preds_LightGBM2.csv'\n",
      " 'df_val_preds_LightGBM3.csv' 'df_val_preds_LightGBM_fs.csv'\n",
      " 'df_val_preds_LogisticRegression1.csv' 'df_val_preds_RandomForest1.csv'\n",
      " 'df_val_preds_XGBoost1.csv' 'df_val_preds_XGBoost2.csv']\n"
     ]
    }
   ],
   "source": [
    "# This code performs a hill climbing algorithm to create an ensemble of base models by optimizing the ensemble's \n",
    "# accuracy score. Here's a step-by-step explanation of the code:\n",
    "\n",
    "### 1. Initialization and Data Preparation\n",
    "# - `PATH`: Directory containing the out-of-fold (OOF) predictions of the models.\n",
    "PATH = './output/'\n",
    "# - `FILES`: List of all files in the directory.\n",
    "FILES = os.listdir(PATH)\n",
    "# - `OOF`: Sorted list of OOF prediction files.\n",
    "OOF = np.sort([f for f in FILES if 'val_preds' in f])\n",
    "print(\"OOF files: \", OOF)\n",
    "# - `OOF_CSV`: List of DataFrames, each containing OOF predictions.\n",
    "OOF_CSV = [pd.read_csv(PATH + k) for k in OOF]\n",
    "# - `OOF_PREDS_COLS`: List of column names in the OOF predictions.\n",
    "OOF_PREDS_COLS = ['kfold', 'Target', 'oof_preds', 'oof_preds_proba_0', 'oof_preds_proba_1', 'oof_preds_proba_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model df_val_preds_CatBoost1.csv has OOF accuracy = 0.8300\n",
      "Model df_val_preds_CatBoost2.csv has OOF accuracy = 0.8313\n",
      "Model df_val_preds_LightGBM1.csv has OOF accuracy = 0.8336\n",
      "Model df_val_preds_LightGBM2.csv has OOF accuracy = 0.8332\n",
      "Model df_val_preds_LightGBM3.csv has OOF accuracy = 0.8336\n",
      "Model df_val_preds_LightGBM_fs.csv has OOF accuracy = 0.8262\n",
      "Model df_val_preds_LogisticRegression1.csv has OOF accuracy = 0.8273\n",
      "Model df_val_preds_RandomForest1.csv has OOF accuracy = 0.8261\n",
      "Model df_val_preds_XGBoost1.csv has OOF accuracy = 0.8308\n",
      "Model df_val_preds_XGBoost2.csv has OOF accuracy = 0.8318\n"
     ]
    }
   ],
   "source": [
    "### 2. Extract Predictions and True Labels\n",
    "\n",
    "# - `oof_preds_all`: A matrix where each column represents the predictions of a model.\n",
    "oof_preds_all = np.zeros((len(OOF_CSV[0]), len(OOF)))\n",
    "# - `oof_preds_proba_all`: A matrix where each column represents the probability predictions of a model.\n",
    "oof_preds_proba_all = np.zeros((len(OOF_CSV[0]), len(OOF), 3))\n",
    "for k in range(len(OOF)):\n",
    "    oof_preds_all[:, k] = OOF_CSV[k].oof_preds.values\n",
    "    oof_preds_proba_all[:, k, :] = OOF_CSV[k][['oof_preds_proba_0', 'oof_preds_proba_1', 'oof_preds_proba_2']].values\n",
    "\n",
    "# - `target`: True target values (assumed to be the same for all OOF files).\n",
    "target = OOF_CSV[0].Target.values\n",
    "\n",
    "### 3. Evaluate Initial accuracy for Each Model\n",
    "# - `all_model_metrics`: List to store accuracy scores for each model.\n",
    "all_model_metrics = []\n",
    "# - Loop through each model's predictions to compute and print the accuracy score.\n",
    "for model_index in range(oof_preds_all.shape[1]):\n",
    "    accuracy = accuracy_score(target, oof_preds_all[:, model_index])\n",
    "    all_model_metrics.append(accuracy)\n",
    "    print(f'Model {OOF[model_index]} has OOF accuracy = {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble accuracy = 0.8336 by beginning with model df_val_preds_LightGBM1.csv\n"
     ]
    }
   ],
   "source": [
    "### 4. Initialize the Ensemble with the Best Single Model\n",
    "\n",
    "# - `included_model_index`: List to store indices of models included in the ensemble (starting with the best single model).\n",
    "included_model_index = [np.argmax(all_model_metrics)]\n",
    "# - `model_weights`: List to store weights for the models in the ensemble.\n",
    "model_weights = []\n",
    "# - `best_score`: Best accuracy score so far.\n",
    "best_score = np.max(all_model_metrics)\n",
    "\n",
    "### 5. Define Search Parameters\n",
    "# - `RES`: Resolution for weight search.\n",
    "RES = 200\n",
    "# - `PATIENCE`: Patience for early stopping in weight search.\n",
    "PATIENCE = 10\n",
    "# - `TOL`: Minimum improvement required to continue adding models.\n",
    "TOL = 0.000003\n",
    "# - `DUPLICATES`: Whether to allow duplicate models in the ensemble.\n",
    "DUPLICATES = False\n",
    "\n",
    "### 6. Hill Climbing to Optimize Ensemble\n",
    "\n",
    "print(f'Ensemble accuracy = {best_score:.4f} by beginning with model {OOF[included_model_index[0]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - For each iteration (up to the number of models):\n",
    "#     - **Build Current Ensemble**: Start with the best initial model and iteratively combine it with other models in `m` \n",
    "#       using their weights `w`.\n",
    "#     - **Search for Best Model to Add**: For each model not already in the ensemble, evaluate it with different weights \n",
    "#       to find the best one to add.\n",
    "#     - **Evaluate Each Weight**: For each candidate model, iterate over possible weights to find the weight that gives the highest AUC. \n",
    "#       Use early stopping (`PATIENCE`) to terminate weight search early if no improvement is seen.\n",
    "#     - **Check for Improvement**: If the best AUC improvement (`inc`) is less than `TOL`, stop the process.\n",
    "#     - **Update Ensemble**: If improvement is sufficient, update the ensemble with the new model and weight, then print the \n",
    "#       updated AUC and improvement.\n",
    "\n",
    "### Summary\n",
    "\n",
    "# This code performs a hill climbing optimization to build an ensemble of models, starting with the best single model \n",
    "# and iteratively adding models that improve the ensemble's AUC score. The process continues until no significant improvement \n",
    "# is found. This method helps to create a more robust predictive model by combining the strengths of multiple models.\n",
    "for model_index in range(len(OOF)):\n",
    "    # Build current ensemble prediction\n",
    "    # included_model_index[0] is the index of the initial best model.\n",
    "    # oof_preds_all[:, included_model_index[0]] extracts the predictions of this model.\n",
    "    # - `current_ensemble_preds`: Current ensemble predictions.\n",
    "    current_ensemble_preds_proba = oof_preds_proba_all[:, included_model_index[0], :]\n",
    "    for i, k in enumerate(included_model_index[1:]):\n",
    "        # Update the ensemble predictions by combining the current ensemble md with the predictions \n",
    "        # of the model x[:, k] using weight model_weights[i].\n",
    "        current_ensemble_preds_proba = model_weights[i] * oof_preds_proba_all[:, k, :] + (1 - model_weights[i]) * current_ensemble_preds_proba\n",
    "\n",
    "    # Search for the best model to add    \n",
    "    # max_score: Stores the maximum accuracy score found in this iteration.\n",
    "    max_score = 0 \n",
    "    # best_model_index: Stores the index of the model that gives the best accuracy score when added.\n",
    "    best_model_index = 0\n",
    "    # best_model_weight: Stores the weight of the best model when added to the ensemble.\n",
    "    best_model_weight = 0\n",
    "    print('Searching for best model to add... ')\n",
    "\n",
    "    # The for loop iterates over all models (oof_preds_all.shape[1] is the number of models).\n",
    "    for model_index in range(oof_preds_all.shape[1]):\n",
    "        # Print the current model index k.\n",
    "        print(OOF[model_index], ', ', end='')\n",
    "        # Skip models that are already in the ensemble \n",
    "        if not DUPLICATES and (model_index in included_model_index):\n",
    "            continue\n",
    "\n",
    "        # Evaluate adding model at position = model_index with different weights\n",
    "        # Initialize variables for tracking the best weight for the current model at position = model_index\n",
    "        # Best weight found for model OOF[model_index].\n",
    "        best_weight_j = 0\n",
    "        # Best accuracy score achieved with model OOF[model_index].\n",
    "        best_score_j = 0\n",
    "        # Counter to track the number of non-improving iterations for early stopping.\n",
    "        ct = 0\n",
    "        # Inner for loop iterates over possible weights (RES determines the resolution of weights from 0 to 1):\n",
    "        # RES = 200, so we iterate over 0, 0.005, 0.01, ..., 0.995, 1\n",
    "        for j in range(RES):\n",
    "            tmp = j / RES * oof_preds_proba_all[:, model_index, :] + (1 - j / RES) * current_ensemble_preds_proba\n",
    "            tmp_preds = np.argmax(tmp, axis=1)\n",
    "            #  Calculate the AUC score of the ensemble with the current weight.\n",
    "            accuracy = accuracy_score(target, tmp_preds)\n",
    "            print(f\"accuracy = {accuracy}\")\n",
    "            # If the accuracy score is better than the best score best_score_j found so far:\n",
    "            if accuracy > best_score_j:\n",
    "                # Update best_score_j with the new best score auc.\n",
    "                best_score_j = accuracy\n",
    "                # Update best_weight_j with the current weight j / RES.\n",
    "                best_weight_j = j / RES\n",
    "            else:\n",
    "                # If no improvement is found, increment the counter ct.\n",
    "                ct += 1\n",
    "            # If the counter ct is greater than PATIENCE, break out of the inner loop.                \n",
    "            if ct > PATIENCE:\n",
    "                print(f\"Early stopping at j = {j}\")\n",
    "                break\n",
    "        # If the best score best_score_j for the current model OOF[model_index] is better than the maximum score max_score found so far:\n",
    "        # Update max_score to best_score_j.\n",
    "        # Update best_model_index to model_index.\n",
    "        # Update best_model_weight to best_weight_j.                \n",
    "        if best_score_j > max_score:\n",
    "            max_score = best_score_j\n",
    "            best_model_index = model_index\n",
    "            best_model_weight = best_weight_j\n",
    "\n",
    "    # Check if improvement is significant\n",
    "    # Calculate the improvement inc as the difference between the new best score max_score and the previous best score best_score.\n",
    "    inc = max_score - best_score\n",
    "    print(f\"inc = {inc}\")\n",
    "    # If the improvement inc is less than or equal to the tolerance TOL:\n",
    "    # Print a message indicating no significant increase.\n",
    "    # Break the loop, stopping the search for additional models.\n",
    "    if inc <= TOL:\n",
    "        print()\n",
    "        print('No increase in score. Stopping.')\n",
    "        break\n",
    "\n",
    "    # Update ensemble with the new model and weight\n",
    "    print()\n",
    "    print(f'Ensemble accuracy = {max_score:.4f} after adding model {OOF[best_model_index]} with weight {best_model_weight:.3f}. Increase of {inc:.4f}')\n",
    "    print()\n",
    "\n",
    "    best_score = max_score\n",
    "    included_model_index.append(best_model_index)\n",
    "    model_weights.append(best_model_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
