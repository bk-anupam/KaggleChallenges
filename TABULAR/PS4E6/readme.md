# Kaggle Playground Series - Season 4, Episode 6: Predict Academic Risk

This repository contains Jupyter notebooks for the Kaggle Playground Series Season 4, Episode 6 competition. The goal of this competition is to predict student dropout and academic success in higher education based on various academic and socio-demographic factors.

Competition Link: [https://www.kaggle.com/competitions/playground-series-s4e6](https://www.kaggle.com/competitions/playground-series-s4e6)

## Tech Stack

The notebooks utilize the following core Python libraries:

*   **Data Manipulation:** `pandas`, `numpy`
*   **Data Visualization:** `matplotlib`, `seaborn`
*   **Machine Learning:**
    *   `scikit-learn` (Preprocessing, Metrics, Model Selection, Feature Selection, Linear Models)
    *   `xgboost`
    *   `lightgbm`
    *   `catboost`
    *   `optuna` (Hyperparameter Optimization)
    *   `openfe` (Automated Feature Engineering)
    *   `flaml` (Automated Machine Learning - inferred)
*   **Data Fetching:** `ucimlrepo`
*   **Utilities:** `os`, `sys`, `re`, `joblib`, `scipy`, `warnings`, `statistics`, `functools`, `contextlib`
*   **Custom Utilities:** `train_tabular_utils`, `cv_split_utils`, `enums`, `data_utils` (These appear to be custom helper modules)

## Notebook Descriptions

The notebooks cover different stages of a typical data science pipeline:

1.  **`ps4e6_eda.ipynb` - Exploratory Data Analysis (EDA)**
    *   Loads the training and test datasets.
    *   Performs initial data exploration, including checking data types and identifying feature types (categorical, continuous, integer).
    *   Conducts feature analysis, potentially using statistical tests like Chi-squared to understand feature distributions and relationships with the target variable.
    *   Investigates potential feature selection candidates based on EDA findings.

2.  **`ps4e6_generate_features.ipynb` - Feature Engineering**
    *   Loads the competition data and the original dataset from the UCI repository.
    *   Uses the `openfe` library to automatically generate new features based on the training data.
    *   Applies the generated feature transformations to both training and test sets.
    *   Compares the performance of a baseline model (Logistic Regression) before and after adding the automatically generated features.
    *   Saves the augmented datasets (`train_openfe.csv`, `test_openfe.csv`) and the generated feature formulas.

3.  **`ps4e6_exp2.ipynb` - Modeling Experiment**
    *   Loads data, potentially including the original dataset and/or features generated by `openfe`.
    *   Performs preprocessing steps like label encoding the target variable and scaling continuous features using `StandardScaler`.
    *   Implements cross-validation splitting (`StratifiedKFold`).
    *   Includes functions for hyperparameter tuning using `optuna` for various models (LGBM, XGBoost, CatBoost, RandomForest, LogisticRegression).
    *   Trains and validates a chosen model type (e.g., LightGBM in the provided context) using cross-validation.
    *   Includes code (commented out) for feature selection methods (forward/backward selection).
    *   Calculates and displays Out-Of-Fold (OOF) performance metrics (Accuracy, F1-Score, Classification Report, Confusion Matrix).
    *   Generates test set predictions and creates a submission file.
    *   Analyzes and plots feature importance for tree-based models.

4.  **`ps4e6_ensemble.ipynb` - Ensembling (Weighted Average & Voting)**
    *   Loads OOF predictions and test predictions generated by various base models (CatBoost, XGBoost, LightGBM, RandomForest, LogisticRegression) trained previously (likely using notebooks similar to `ps4e6_exp2.ipynb`).
    *   Evaluates the performance of individual base models on the OOF data.
    *   Implements a hard voting ensemble by summing the one-hot encoded predictions of base models.
    *   Implements a weighted averaging ensemble for predicted probabilities. It uses `scipy.optimize.minimize` (Nelder-Mead method) to find the optimal weights for combining base model probabilities based on maximizing OOF accuracy.
    *   Generates submission files based on both the voting and weighted average ensemble predictions.

5.  **`ps4e6_flaml_exp.ipynb` - AutoML Experiment (Inferred)**
    *   Likely uses the `flaml` library to automate the process of model selection and hyperparameter tuning for this tabular prediction task.
    *   Aims to find a high-performing model with minimal manual intervention.

6.  **`ps4e6_hillclimbing_ensemble.ipynb` - Ensembling (Hill Climbing - Inferred)**
    *   Implements an alternative ensembling strategy.
    *   Likely uses a hill climbing optimization algorithm to iteratively adjust the weights of base models or select a subset of models to maximize the ensemble's OOF performance.
    *   Generates a submission file based on the hill climbing ensemble predictions.
