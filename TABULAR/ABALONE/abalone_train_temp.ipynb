{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q xgboost --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q openfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "import statistics\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer\n",
    "from functools import partial\n",
    "from openfe import OpenFE, transform\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"/home/bk_anupam/code/ML/ML_UTILS/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_tabular_utils as tt\n",
    "import cv_split_utils\n",
    "import enums\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    RUN_MODE = \"LOCAL\"\n",
    "    RANDOM_SEED = 42\n",
    "    NUM_FOLDS = 10\n",
    "    TARGET_COL_NAME = \"Rings\"    \n",
    "    SKEW_THRESHOLD = 0.5\n",
    "    EARLY_STOPPING = 500 \n",
    "    SCALER = enums.Scaler.StandardScaler\n",
    "    METRIC = enums.Metrics.RMSLE\n",
    "    # These values are more dynamic   \n",
    "    MODEL_TYPE = enums.ModelName.CatBoost\n",
    "    REMOVE_OUTLIERS = False\n",
    "    POWER_TRANSFORM = False\n",
    "    NORMALIZE_DATA = True    \n",
    "    NUM_TUNING_TRIALS = 2\n",
    "    TUNE_ON_SINGLE_FOLD = True\n",
    "    TRAIN_SINGLE_FOLD = True\n",
    "    GENERATE_AUTO_FEATURES = False\n",
    "    PERSIST_MODEL = False\n",
    "    # perform log transformation on target to train on RMSLE objective\n",
    "    TRANSFORM_TARGET = True\n",
    "\n",
    "COLS_TO_LEAVE = [\"Rings\", \"kfold\", \"outlier_labels\"]\n",
    "CPU_COUNT = os.cpu_count()\n",
    "\n",
    "DATA_READPATH = \"./data/\"\n",
    "DATA_WRITEPATH = \"./output/\"\n",
    "SUBMISSION_FILEPATH = DATA_READPATH\n",
    "if Config.RUN_MODE == \"KAGGLE\":\n",
    "    # If we are not generating features, we are using already generated features\n",
    "    if Config.GENERATE_AUTO_FEATURES:\n",
    "        DATA_READPATH = \"/kaggle/input/playground-series-s4e4/\"\n",
    "        SUBMISSION_FILEPATH = DATA_READPATH\n",
    "    else:\n",
    "        DATA_READPATH = \"/kaggle/input/abalone-openfe/\"\n",
    "        SUBMISSION_FILEPATH = \"/kaggle/input/playground-series-s4e4/\"\n",
    "    DATA_WRITEPATH = \"/kaggle/working/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the already tuned model parameters or set to None if hyperparameter tuning is required\n",
    "# tuned_model_params = {'learning_rate': 0.018786770693979188, 'n_estimators': 800, 'max_depth': 17, 'min_child_weight': 6, 'subsample': 0.8798366073391215, 'colsample_bytree': 0.9095830407561765, 'num_leaves': 108, 'reg_alpha': 0.8627821903308106, 'reg_lambda': 35.942036785421045}\n",
    "tuned_model_params = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params_static = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"seed\": Config.RANDOM_SEED,\n",
    "    \"verbosity\": 0,\n",
    "}\n",
    "lgbm_params_static = {\n",
    "        \"objective\": \"root_mean_squared_error\",\n",
    "        \"metric\": 'rmse',\n",
    "        \"verbosity\": -1,    # <0: fatal, =0: error (warn), =1: info, >1: debug\n",
    "        \"boosting_type\": \"gbdt\"\n",
    "    }\n",
    "cb_params_static = {\n",
    "    \"objective\": \"RMSE\",\n",
    "    \"verbose\": 0,\n",
    "    \"random_seed\": Config.RANDOM_SEED,\n",
    "    \"eval_metric\": \"RMSE\",\n",
    "    'grow_policy':  'Lossguide'\n",
    "}\n",
    "params_static = cb_params_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train dataset locally from data folder\n",
    "df_train = pd.read_csv(DATA_READPATH + \"train_openfe.csv\")\n",
    "# import test dataset locally from data folder\n",
    "df_test = pd.read_csv(DATA_READPATH + \"test_openfe.csv\")\n",
    "# drop id column\n",
    "#df_train = df_train.drop(\"id\", axis=1)\n",
    "#df_test = df_test.drop(\"id\", axis=1)\n",
    "# keep a copy of original train and test data for later use\n",
    "df_train_orig = df_train.copy()\n",
    "df_test_orig = df_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Squared Log Error (RMSLE):\n",
    "\n",
    "- This metric penalizes large errors in predictions more than small errors.\n",
    "- It's commonly used for regression problems where the target variable represents quantities or measurements that are naturally positive (e.g., housing prices, sales figures).\n",
    "\n",
    "LightGBM with \"mean_squared_error\" Objective:\n",
    "\n",
    "- By default, LightGBM's \"mean_squared_error\" objective minimizes the squared difference between predicted and actual target values.\n",
    "\n",
    "Transformation with numpy.log1p(target):\n",
    "\n",
    "- numpy.log1p(target) applies a natural log (ln) function after adding 1 to each target value.\n",
    "- This transformation ensures that the logarithm can be applied to target values that might include zeros.\n",
    "- It also puts more emphasis on relative errors for smaller target values, aligning better with the nature of RMSLE.\n",
    "\n",
    "Impact on Training:\n",
    "\n",
    "- By training on the transformed target (numpy.log1p(target)), LightGBM is implicitly minimizing the squared difference between the log-transformed predictions and log-transformed actual values.\n",
    "- However, during evaluation, you'll need to transform the predicted values back using the inverse function (np.expm1(predicted_values)) to recover the original scale for calculating the actual RMSLE.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "- This approach is more suitable for LightGBM's \"mean_squared_error\" objective because it aligns the training process with the logic of RMSLE.\n",
    "It avoids potential issues with taking the logarithm of zero target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_for_fe = [x for x in df_train.columns if x not in COLS_TO_LEAVE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_features(df_train, df_test, feature_cols, NUM_NEW_FEATURES=10):\n",
    "    train_X = df_train[feature_cols] \n",
    "    test_X = df_test[feature_cols]   \n",
    "    train_y = df_train[Config.TARGET_COL_NAME]\n",
    "    ofe = OpenFE()\n",
    "    features = ofe.fit(data=train_X, label=train_y, n_jobs=CPU_COUNT, verbose=False)  # generate new features\n",
    "    # OpenFE recommends a list of new features. We include the top 10\n",
    "    # generated features to see how they influence the model performance\n",
    "    train_X, test_X = transform(train_X, test_X, ofe.new_features_list[:NUM_NEW_FEATURES], n_jobs=CPU_COUNT)\n",
    "    return train_X, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.GENERATE_AUTO_FEATURES:\n",
    "    df_train, df_test = generate_new_features(df_train, df_test, feature_cols_for_fe)    \n",
    "    df_train_labels = df_train_orig[[Config.TARGET_COL_NAME]]\n",
    "    # Add the label data to the dataframe\n",
    "    df_train = pd.concat([df_train, df_train_labels], axis=1)\n",
    "    # save the new train and test data with openfe features to csv files for later use\n",
    "    df_train.to_csv(DATA_WRITEPATH + \"train_openfe.csv\", index=False)\n",
    "    df_test.to_csv(DATA_WRITEPATH + \"test_openfe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_float = [ x for x in df_train.select_dtypes(include=[\"float\"]).columns.to_list() if x not in COLS_TO_LEAVE]\n",
    "cols_int = df_train.select_dtypes(include=[\"int64\"]).columns.to_list()\n",
    "cols_str = df_train.select_dtypes(include=[\"object\"]).columns.to_list()\n",
    "feature_cols_to_normalize = cols_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Whole weight.1</th>\n",
       "      <th>Whole weight.2</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>autoFE_f_0</th>\n",
       "      <th>autoFE_f_1</th>\n",
       "      <th>autoFE_f_2</th>\n",
       "      <th>autoFE_f_3</th>\n",
       "      <th>autoFE_f_4</th>\n",
       "      <th>autoFE_f_5</th>\n",
       "      <th>autoFE_f_6</th>\n",
       "      <th>autoFE_f_7</th>\n",
       "      <th>autoFE_f_8</th>\n",
       "      <th>autoFE_f_9</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.7715</td>\n",
       "      <td>0.3285</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>2.291667</td>\n",
       "      <td>1.368750</td>\n",
       "      <td>1.791667</td>\n",
       "      <td>2.348554</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>0.3900</td>\n",
       "      <td>2637.0</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.145</td>\n",
       "      <td>1.1300</td>\n",
       "      <td>0.4580</td>\n",
       "      <td>0.2765</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>1.968750</td>\n",
       "      <td>1.431250</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>2.467249</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.6720</td>\n",
       "      <td>0.4650</td>\n",
       "      <td>1173.0</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>3.818182</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>487.0</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>2.380000</td>\n",
       "      <td>1.502000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>2.435419</td>\n",
       "      <td>0.3450</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>2088.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.7820</td>\n",
       "      <td>0.3695</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>0.1975</td>\n",
       "      <td>2.810127</td>\n",
       "      <td>1.870886</td>\n",
       "      <td>2.151899</td>\n",
       "      <td>2.116373</td>\n",
       "      <td>0.3575</td>\n",
       "      <td>0.4125</td>\n",
       "      <td>0.3275</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.1975</td>\n",
       "      <td>0.2275</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sex  Length  Diameter  Height  Whole weight  Whole weight.1  Whole weight.2  \\\n",
       "0   F   0.550     0.430   0.150        0.7715          0.3285          0.1465   \n",
       "1   F   0.630     0.490   0.145        1.1300          0.4580          0.2765   \n",
       "2   I   0.160     0.110   0.025        0.0210          0.0055          0.0030   \n",
       "3   M   0.595     0.475   0.150        0.9145          0.3755          0.2055   \n",
       "4   I   0.555     0.425   0.130        0.7820          0.3695          0.1600   \n",
       "\n",
       "   Shell weight  autoFE_f_0  autoFE_f_1  autoFE_f_2  autoFE_f_3  autoFE_f_4  \\\n",
       "0        0.2400    2.291667    1.368750    1.791667    2.348554      0.3100   \n",
       "1        0.3200    1.968750    1.431250    1.531250    2.467249      0.3100   \n",
       "2        0.0050   32.000000    1.100000   22.000000    3.818182      0.1550   \n",
       "3        0.2500    2.380000    1.502000    1.900000    2.435419      0.3450   \n",
       "4        0.1975    2.810127    1.870886    2.151899    2.116373      0.3575   \n",
       "\n",
       "   autoFE_f_5  autoFE_f_6  autoFE_f_7  autoFE_f_8  autoFE_f_9  Rings  \n",
       "0      0.4430      0.3900      2637.0      0.2400      0.1900     11  \n",
       "1      0.6720      0.4650      1173.0      0.3200      0.1700     11  \n",
       "2      0.0155      0.0300       487.0      0.0050      0.1050      6  \n",
       "3      0.5390      0.4000      2088.0      0.2500      0.2250     10  \n",
       "4      0.4125      0.3275        32.0      0.1975      0.2275      9  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_stratification_target(df, cols, threshold=25):\n",
    "    df[\"outlier_labels\"] = \"\"\n",
    "    for col in cols:\n",
    "        q1 = np.percentile(df[col], threshold)\n",
    "        q3 = np.percentile(df[col], 100 - threshold)\n",
    "        iqr = q3-q1\n",
    "        min_val = q1 - 1.5 * iqr\n",
    "        max_val = q3 + 1.5 * iqr        \n",
    "        outlier_indices = np.logical_or(df[col] < min_val, df[col] > max_val)\n",
    "        df.loc[outlier_indices, \"outlier_labels\"] = df.loc[outlier_indices, \"outlier_labels\"] + col + \"-\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_ouliers = [x for x in cols_float if \"autoFE_f_\" not in x]\n",
    "df_train = define_stratification_target(df_train, cols_with_ouliers, threshold=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Whole weight.1</th>\n",
       "      <th>Whole weight.2</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>autoFE_f_0</th>\n",
       "      <th>autoFE_f_1</th>\n",
       "      <th>...</th>\n",
       "      <th>autoFE_f_3</th>\n",
       "      <th>autoFE_f_4</th>\n",
       "      <th>autoFE_f_5</th>\n",
       "      <th>autoFE_f_6</th>\n",
       "      <th>autoFE_f_7</th>\n",
       "      <th>autoFE_f_8</th>\n",
       "      <th>autoFE_f_9</th>\n",
       "      <th>Rings</th>\n",
       "      <th>outlier_labels</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.8295</td>\n",
       "      <td>0.2405</td>\n",
       "      <td>0.2005</td>\n",
       "      <td>0.265</td>\n",
       "      <td>1.962264</td>\n",
       "      <td>0.907547</td>\n",
       "      <td>...</td>\n",
       "      <td>3.449064</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.5890</td>\n",
       "      <td>0.405</td>\n",
       "      <td>2037.0</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.130</td>\n",
       "      <td>15</td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.8050</td>\n",
       "      <td>0.3155</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.245</td>\n",
       "      <td>2.326531</td>\n",
       "      <td>1.287755</td>\n",
       "      <td>...</td>\n",
       "      <td>2.551506</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.4895</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.195</td>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.175</td>\n",
       "      <td>1.2740</td>\n",
       "      <td>0.5940</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.380</td>\n",
       "      <td>1.763158</td>\n",
       "      <td>1.563158</td>\n",
       "      <td>...</td>\n",
       "      <td>2.144781</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.6800</td>\n",
       "      <td>0.555</td>\n",
       "      <td>435.0</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.130</td>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.3660</td>\n",
       "      <td>0.1705</td>\n",
       "      <td>0.0855</td>\n",
       "      <td>0.120</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>1.420833</td>\n",
       "      <td>...</td>\n",
       "      <td>2.146628</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.1955</td>\n",
       "      <td>0.230</td>\n",
       "      <td>1586.0</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.230</td>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1565</td>\n",
       "      <td>0.0635</td>\n",
       "      <td>0.100</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.565000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.054313</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.210</td>\n",
       "      <td>707.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.235</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sex  Length  Diameter  Height  Whole weight  Whole weight.1  Whole weight.2  \\\n",
       "0   F   0.520     0.395   0.140        0.8295          0.2405          0.2005   \n",
       "1   I   0.570     0.440   0.130        0.8050          0.3155          0.2000   \n",
       "2   F   0.670     0.510   0.175        1.2740          0.5940          0.3000   \n",
       "3   I   0.460     0.350   0.110        0.3660          0.1705          0.0855   \n",
       "4   I   0.435     0.335   0.110        0.3215          0.1565          0.0635   \n",
       "\n",
       "   Shell weight  autoFE_f_0  autoFE_f_1  ...  autoFE_f_3  autoFE_f_4  \\\n",
       "0         0.265    1.962264    0.907547  ...    3.449064       0.255   \n",
       "1         0.245    2.326531    1.287755  ...    2.551506       0.325   \n",
       "2         0.380    1.763158    1.563158  ...    2.144781       0.290   \n",
       "3         0.120    3.833333    1.420833  ...    2.146628       0.340   \n",
       "4         0.100    4.350000    1.565000  ...    2.054313       0.335   \n",
       "\n",
       "   autoFE_f_5  autoFE_f_6  autoFE_f_7  autoFE_f_8  autoFE_f_9  Rings  \\\n",
       "0      0.5890       0.405      2037.0       0.265       0.130     15   \n",
       "1      0.4895       0.375      1277.0       0.245       0.195      9   \n",
       "2      0.6800       0.555       435.0       0.380       0.130      9   \n",
       "3      0.1955       0.230      1586.0       0.120       0.230      8   \n",
       "4      0.1650       0.210       707.0       0.100       0.235      7   \n",
       "\n",
       "   outlier_labels kfold  \n",
       "0                     6  \n",
       "1                     1  \n",
       "2                     4  \n",
       "3                     1  \n",
       "4                     8  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = cv_split_utils.strat_kfold_dataframe(\n",
    "                                    df=df_train,                           \n",
    "                                    target_col_name=Config.TARGET_COL_NAME,\n",
    "                                    num_folds=Config.NUM_FOLDS,\n",
    "                                    random_state=Config.RANDOM_SEED\n",
    "                                )\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(df, features_to_preprocess):\n",
    "    float_outliers = []\n",
    "    for col_name in features_to_preprocess:\n",
    "        df, df_col_outliers = data_utils.process_outliers_iqr(df, col_name, Config.REMOVE_OUTLIERS)\n",
    "        df_float_outliers = float_outliers.append(df_col_outliers)\n",
    "        if Config.POWER_TRANSFORM:\n",
    "            df, transformed = data_utils.power_transform(df, col_name, Config.SKEW_THRESHOLD)\n",
    "    df_float_outliers = pd.concat(float_outliers, axis=0)        \n",
    "    df_float_outliers = df_float_outliers.reset_index(drop=True)\n",
    "    return df, df_float_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess float features for train and test data\n",
    "df_train, df_train_float_outliers = preprocess_features(df_train, cols_float)\n",
    "_, df_test_float_outliers = preprocess_features(df_test, cols_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of categorical variables\n",
    "df_train_onehot = pd.get_dummies(df_train, columns=cols_str)\n",
    "df_test_onehot = pd.get_dummies(df_test, columns=cols_str)\n",
    "\n",
    "if Config.NORMALIZE_DATA:\n",
    "    # normalize\n",
    "    df_train_onehot = tt.normalize_features(df_train_onehot, \n",
    "                                            scaler=Config.SCALER,\n",
    "                                            features_to_normalize=feature_cols_to_normalize)\n",
    "    df_test_onehot = tt.normalize_features(df_test_onehot,\n",
    "                                           scaler=Config.SCALER, \n",
    "                                           features_to_normalize=feature_cols_to_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Length', 'Diameter', 'Height', 'Whole weight', 'Whole weight.1', 'Whole weight.2', 'Shell weight', 'autoFE_f_0', 'autoFE_f_1', 'autoFE_f_2', 'autoFE_f_3', 'autoFE_f_4', 'autoFE_f_5', 'autoFE_f_6', 'autoFE_f_7', 'autoFE_f_8', 'autoFE_f_9', 'Sex_F', 'Sex_I', 'Sex_M']\n"
     ]
    }
   ],
   "source": [
    "feature_cols= [x for x in df_train_onehot.columns.to_list() if x not in COLS_TO_LEAVE]\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lgbm_tuning_params(trial):    \n",
    "    params_dynamic = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1.0, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 2000, step=25),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 20),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 4, 256, step=4),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1, 100),\n",
    "        'early_stopping_rounds': trial.suggest_int('early_stopping_rounds', 50, 500, step=20)\n",
    "    }\n",
    "    return {**lgbm_params_static, **params_dynamic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_catboost_tuning_params(trial):\n",
    "    params_dynamic = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1.0, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 5000, step=50),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 16),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 100),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
    "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 4, 256, step=4),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1, 100),\n",
    "        'random_strength': trial.suggest_loguniform('random_strength', 0.01, 10),\n",
    "        'early_stopping_rounds': trial.suggest_int('early_stopping_rounds', 50, 500, step=20),\n",
    "        'max_bin': trial.suggest_int('max_bin', 32, 255)\n",
    "    }\n",
    "    return {**cb_params_static, **params_dynamic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tuning_params(trial, model_name):\n",
    "    if model_name == enums.ModelName.Ridge:\n",
    "        return {\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-4, 1e4, log=True)\n",
    "        }\n",
    "    if model_name == enums.ModelName.Lasso:\n",
    "        return {\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-4, 1e4, log=True)\n",
    "        }\n",
    "    if model_name == enums.ModelName.RandomForest:\n",
    "        return {        \n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 3000, step=100),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 10, 30),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 2, 16),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 16),\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"log2\", \"sqrt\", None])\n",
    "        }\n",
    "    if model_name == enums.ModelName.XGBoost:\n",
    "        xgb_params_dynamic = {            \n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 5000, step=100),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1.0, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 4, 32),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 1),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1, 100),\n",
    "            'early_stopping_rounds': trial.suggest_int('early_stopping_rounds', 100, 500, step=20)\n",
    "        }\n",
    "        return {**xgb_params_static, **xgb_params_dynamic}\n",
    "    if model_name == enums.ModelName.LGBM:\n",
    "        return get_lgbm_tuning_params(trial)\n",
    "    if model_name == enums.ModelName.CatBoost:\n",
    "        return get_catboost_tuning_params(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparams_tuning_objective(trial, model_name, df_train,  \n",
    "                                 feature_cols, metric, target_col_name, single_fold=False,\n",
    "                                 num_folds=5, val_preds_col=\"val_preds\"):           \n",
    "    model_params = get_model_tuning_params(trial, model_name)    \n",
    "    fold_metrics_model, df_val_preds = tt.run_training(\n",
    "        model_name=model_name,\n",
    "        df_train=df_train,\n",
    "        target_col_name=target_col_name,\n",
    "        feature_col_names=feature_cols,\n",
    "        metric=metric,            \n",
    "        num_folds=num_folds,\n",
    "        model_params=model_params,\n",
    "        val_preds_col=val_preds_col,\n",
    "        single_fold=single_fold,\n",
    "        suppress_print=True,\n",
    "        transform_target=Config.TRANSFORM_TARGET\n",
    "    )       \n",
    "    fold_metrics = [x[0] for x in fold_metrics_model]\n",
    "    mean_metric = statistics.mean(fold_metrics)                \n",
    "    return mean_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model_params(study_name, study_direction, num_trials, model_name, \n",
    "                      df_train,  feature_cols, metric, target_col_name, \n",
    "                      single_fold=False, num_folds=5, val_preds_col=\"val_preds\"):\n",
    "    model_params_tuning_obj_partial = partial(\n",
    "        hyperparams_tuning_objective,\n",
    "        model_name=model_name,        \n",
    "        df_train=df_train,\n",
    "        feature_cols=feature_cols,\n",
    "        metric=metric,\n",
    "        target_col_name=target_col_name,\n",
    "        single_fold=single_fold,\n",
    "        num_folds=num_folds,\n",
    "        val_preds_col=val_preds_col\n",
    "    )\n",
    "    study = optuna.create_study(direction=study_direction, study_name=study_name)\n",
    "    study.optimize(model_params_tuning_obj_partial, n_trials=num_trials)\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"Best trial: number = {best_trial.number}, value = {best_trial.value}, params = {best_trial.params}\")\n",
    "    return best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-17 21:38:41,013] A new study created in memory with name: CatBoost_ModelTuning\n",
      "[I 2024-05-17 21:39:20,561] Trial 0 finished with value: 0.1439316880358377 and parameters: {'learning_rate': 0.10636150813354656, 'n_estimators': 1300, 'max_depth': 5, 'min_data_in_leaf': 8, 'subsample': 0.6785743799431574, 'colsample_bylevel': 0.9315225054353669, 'num_leaves': 144, 'reg_lambda': 13.376124271414547, 'random_strength': 0.05040494338418901, 'early_stopping_rounds': 350, 'max_bin': 40}. Best is trial 0 with value: 0.1439316880358377.\n",
      "[I 2024-05-17 21:39:39,113] Trial 1 finished with value: 0.19780469221368277 and parameters: {'learning_rate': 0.002559219445547501, 'n_estimators': 250, 'max_depth': 10, 'min_data_in_leaf': 70, 'subsample': 0.6071799101949791, 'colsample_bylevel': 0.9812339821500928, 'num_leaves': 160, 'reg_lambda': 75.16614536938954, 'random_strength': 0.21790620646551417, 'early_stopping_rounds': 290, 'max_bin': 50}. Best is trial 0 with value: 0.1439316880358377.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: number = 0, value = 0.1439316880358377, params = {'learning_rate': 0.10636150813354656, 'n_estimators': 1300, 'max_depth': 5, 'min_data_in_leaf': 8, 'subsample': 0.6785743799431574, 'colsample_bylevel': 0.9315225054353669, 'num_leaves': 144, 'reg_lambda': 13.376124271414547, 'random_strength': 0.05040494338418901, 'early_stopping_rounds': 350, 'max_bin': 40}\n"
     ]
    }
   ],
   "source": [
    "if tuned_model_params is None:\n",
    "    tuned_model_params = tune_model_params(\n",
    "                            study_name=Config.MODEL_TYPE + \"_ModelTuning\", \n",
    "                            study_direction=\"minimize\",\n",
    "                            num_trials=Config.NUM_TUNING_TRIALS,\n",
    "                            model_name=Config.MODEL_TYPE,\n",
    "                            df_train=df_train_onehot,\n",
    "                            feature_cols=feature_cols,\n",
    "                            metric=Config.METRIC,\n",
    "                            target_col_name=Config.TARGET_COL_NAME,\n",
    "                            single_fold=Config.TUNE_ON_SINGLE_FOLD,\n",
    "                            num_folds=Config.NUM_FOLDS\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {**params_static, **tuned_model_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training CatBoost\n",
      "Fold 0 - CatBoost - RMSLE : 0.1439316880358377\n",
      "Saved validation data predictions to df_val_preds_CatBoost.csv\n",
      "CatBoost CV score = 0.1439316880358377\n",
      "CatBoost Mean RMSLE = 0.1439316880358377, std = 0.0\n"
     ]
    }
   ],
   "source": [
    "fold_metrics_model = tt.train_model(\n",
    "                            df = df_train_onehot,\n",
    "                            model_name=Config.MODEL_TYPE,\n",
    "                            model_params = model_params,\n",
    "                            feature_col_names = feature_cols,\n",
    "                            target_col_name = Config.TARGET_COL_NAME,\n",
    "                            metric = Config.METRIC,\n",
    "                            num_folds = Config.NUM_FOLDS,\n",
    "                            single_fold = Config.TRAIN_SINGLE_FOLD,\n",
    "                            persist_model = Config.PERSIST_MODEL,\n",
    "                            output_path = DATA_WRITEPATH,\n",
    "                            transform_target = Config.TRANSFORM_TARGET\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.MODEL_TYPE == enums.ModelName.XGBoost:\n",
    "    xgb.plot_importance(booster=fold_metrics_model[0][1])\n",
    "elif Config.MODEL_TYPE == enums.ModelName.LGBM:\n",
    "    lgbm.plot_importance(fold_metrics_model[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each fold, get the test predictions using corresponding fold model\n",
    "df_fold_test_preds = tt.get_fold_test_preds(\n",
    "                        fold_metrics_model,\n",
    "                        df_test = df_test_onehot,\n",
    "                        feature_cols = feature_cols,\n",
    "                        num_folds = Config.NUM_FOLDS,\n",
    "                    )\n",
    "if Config.TRANSFORM_TARGET:\n",
    "    # Since we have trained on np.log1p(y) instead of y, we need to reverse the transformation \n",
    "    # to extract the actual predictions. Thus we apply np.expm1 to each of the fold predictions.\n",
    "    df_fold_test_preds = df_fold_test_preds.apply(np.expm1)\n",
    "fold_metrics = [item[0] for item in fold_metrics_model]\n",
    "# Since for RMSLE metric lower is better, we take the inverse of fold metric value to get its weight    \n",
    "fold_weights = 1 / np.array(fold_metrics)\n",
    "# normalize the fold weights\n",
    "fold_weights = fold_weights / np.sum(fold_weights)\n",
    "# Combine fold predictions using simple averaging    \n",
    "df_fold_test_preds[\"test_preds\"] = tt.combine_fold_preds(df_fold_test_preds, fold_weights=None)\n",
    "print(f\"Completed prediction for {len(df_test)} test rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed prediction for 60411 test rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90615</td>\n",
       "      <td>9.560521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90616</td>\n",
       "      <td>9.761124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90617</td>\n",
       "      <td>9.404902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90618</td>\n",
       "      <td>10.181639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90619</td>\n",
       "      <td>7.744676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      Rings\n",
       "0  90615   9.560521\n",
       "1  90616   9.761124\n",
       "2  90617   9.404902\n",
       "3  90618  10.181639\n",
       "4  90619   7.744676"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission = pd.read_csv(SUBMISSION_FILEPATH + 'sample_submission.csv')\n",
    "df_submission['Rings']= df_fold_test_preds[\"test_preds\"]\n",
    "df_submission.to_csv(DATA_WRITEPATH + f'submission_{Config.MODEL_TYPE}.csv',index=False)\n",
    "df_fold_test_preds.to_csv(DATA_WRITEPATH + 'test_preds.csv',index=False)\n",
    "df_submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
