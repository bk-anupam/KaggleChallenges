{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q xgboost --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna_integration'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22507/3523493908.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog_evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLightGBMTunerCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRidge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.9/site-packages/optuna/integration/lightgbm.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0moptuna_integration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightgbm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna_integration'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "import statistics\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.metrics import root_mean_squared_log_error\n",
    "from functools import partial\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"/home/bk_anupam/code/ML/ML_UTILS/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_tabular as tt\n",
    "import cv_split_utils\n",
    "import enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    RANDOM_SEED = 42\n",
    "    NUM_FOLDS = 5\n",
    "    TARGET_COL_NAME = \"Rings\"    \n",
    "    SKEW_THRESHOLD = 0.5\n",
    "    EARLY_STOPPING = 500\n",
    "    RESULTS_FILE = \"model_execution_results.pkl\"\n",
    "    MODEL_TYPE = enums.ModelName.LGBM\n",
    "    REMOVE_OUTLIERS = True\n",
    "    POWER_TRANSFORM = False\n",
    "    NORMALIZE_DATA = True\n",
    "    SCALER = enums.Scaler.StandardScaler\n",
    "    METRIC = enums.Metrics.RMSLE\n",
    "    NUM_TUNING_TRIALS = 25\n",
    "    TUNE_ON_SINGLE_FOLD = True\n",
    "\n",
    "DATA_PATH = \"./data/\"\n",
    "COLS_TO_LEAVE = [\"Rings\", \"kfold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train dataset locally from data folder\n",
    "df_train = pd.read_csv(DATA_PATH + \"train.csv\")\n",
    "# import test dataset locally from data folder\n",
    "df_test = pd.read_csv(DATA_PATH + \"test.csv\")\n",
    "# drop id column\n",
    "df_train = df_train.drop(\"id\", axis=1)\n",
    "df_test = df_test.drop(\"id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Whole weight.1</th>\n",
       "      <th>Whole weight.2</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Rings</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5290</td>\n",
       "      <td>0.2165</td>\n",
       "      <td>0.1375</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.3705</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.0795</td>\n",
       "      <td>0.1025</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.7390</td>\n",
       "      <td>0.3515</td>\n",
       "      <td>0.1575</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.7090</td>\n",
       "      <td>0.2505</td>\n",
       "      <td>0.1700</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.150</td>\n",
       "      <td>1.0590</td>\n",
       "      <td>0.4275</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sex  Length  Diameter  Height  Whole weight  Whole weight.1  Whole weight.2  \\\n",
       "0   I   0.490     0.380   0.125        0.5290          0.2165          0.1375   \n",
       "1   I   0.420     0.345   0.100        0.3705          0.1625          0.0795   \n",
       "2   M   0.555     0.440   0.135        0.7390          0.3515          0.1575   \n",
       "3   F   0.535     0.410   0.140        0.7090          0.2505          0.1700   \n",
       "4   F   0.605     0.455   0.150        1.0590          0.4275          0.2210   \n",
       "\n",
       "   Shell weight  Rings  kfold  \n",
       "0        0.1550      7      3  \n",
       "1        0.1025      7      3  \n",
       "2        0.2350      9      0  \n",
       "3        0.1900      9      4  \n",
       "4        0.3100     10      2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = cv_split_utils.strat_kfold_dataframe(\n",
    "                                    df=df_train, \n",
    "                                    target_col_name=Config.TARGET_COL_NAME, \n",
    "                                    num_folds=Config.NUM_FOLDS,\n",
    "                                    random_state=Config.RANDOM_SEED\n",
    "                                )\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_float = df_train.select_dtypes(include=[\"float\"]).columns.to_list()\n",
    "cols_int = df_train.select_dtypes(include=[\"int64\"]).columns.to_list()\n",
    "cols_str = df_train.select_dtypes(include=[\"object\"]).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_outliers_iqr(df, col_name, remove_outliers=True):\n",
    "    Q1 = df[col_name].quantile(0.25)\n",
    "    Q3 = df[col_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1    \n",
    "    min_val = Q1 - 1.5 * IQR\n",
    "    max_val = Q3 + 1.5 * IQR    \n",
    "    outlier_count = df[(df[col_name] < min_val) | (df[col_name] > max_val)].shape[0]\n",
    "    if remove_outliers:\n",
    "        df = df[(df[col_name] >= min_val) & (df[col_name] <= max_val)]\n",
    "    # Create a DataFrame for the results\n",
    "    result = pd.DataFrame({\n",
    "        'col_name': [col_name],\n",
    "        'Q1': [Q1],\n",
    "        'Q3': [Q3],\n",
    "        'IQR': [IQR],\n",
    "        'min_val': [min_val],\n",
    "        'max_val': [max_val],\n",
    "        'outlier_count': [outlier_count]\n",
    "    })    \n",
    "    return df, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_transform(df, col_name, skew_threshold=0.5):    \n",
    "    transformed = False\n",
    "    skew = df[col_name].skew()\n",
    "    print(f\"{col_name} has skewness of {skew}\")\n",
    "    power_transformer = PowerTransformer(method='yeo-johnson', standardize=True)    \n",
    "    if abs(skew) > skew_threshold:\n",
    "        transformed = True\n",
    "        print(\"Will apply power transform.\")\n",
    "        col_transformed = power_transformer.fit_transform(df[[col_name]])\n",
    "        df.loc[:, col_name] = col_transformed\n",
    "    return df, transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q3</th>\n",
       "      <th>IQR</th>\n",
       "      <th>min_val</th>\n",
       "      <th>max_val</th>\n",
       "      <th>outlier_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Length</td>\n",
       "      <td>0.4450</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>0.21250</td>\n",
       "      <td>0.83250</td>\n",
       "      <td>1460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Diameter</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.65000</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Height</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.03500</td>\n",
       "      <td>0.23500</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whole weight</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>1.0730</td>\n",
       "      <td>0.6325</td>\n",
       "      <td>-0.50825</td>\n",
       "      <td>2.02175</td>\n",
       "      <td>621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Whole weight.1</td>\n",
       "      <td>0.1865</td>\n",
       "      <td>0.4625</td>\n",
       "      <td>0.2760</td>\n",
       "      <td>-0.22750</td>\n",
       "      <td>0.87650</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Whole weight.2</td>\n",
       "      <td>0.0905</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.1405</td>\n",
       "      <td>-0.12025</td>\n",
       "      <td>0.44175</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shell weight</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.3005</td>\n",
       "      <td>0.1745</td>\n",
       "      <td>-0.13575</td>\n",
       "      <td>0.56225</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         col_name      Q1      Q3     IQR  min_val  max_val  outlier_count\n",
       "0          Length  0.4450  0.6000  0.1550  0.21250  0.83250           1460\n",
       "1        Diameter  0.3500  0.4700  0.1200  0.17000  0.65000            372\n",
       "2          Height  0.1100  0.1600  0.0500  0.03500  0.23500             73\n",
       "3    Whole weight  0.4405  1.0730  0.6325 -0.50825  2.02175            621\n",
       "4  Whole weight.1  0.1865  0.4625  0.2760 -0.22750  0.87650            600\n",
       "5  Whole weight.2  0.0905  0.2310  0.1405 -0.12025  0.44175            130\n",
       "6    Shell weight  0.1260  0.3005  0.1745 -0.13575  0.56225            593"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess float features\n",
    "# Create an empty DataFrame to store the results\n",
    "float_outliers = []\n",
    "for col_name in cols_float:\n",
    "    df_train, df_col_outliers = process_outliers_iqr(df_train, col_name, Config.REMOVE_OUTLIERS)\n",
    "    df_float_outliers = float_outliers.append(df_col_outliers)\n",
    "    if Config.POWER_TRANSFORM:\n",
    "        df_train, transformed = power_transform(df_train, col_name, Config.SKEW_THRESHOLD)\n",
    "df_float_outliers = pd.concat(float_outliers, axis=0)        \n",
    "df_float_outliers = df_float_outliers.reset_index(drop=True)\n",
    "df_float_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding of categorical variables\n",
    "df_train_onehot = pd.get_dummies(df_train, columns=cols_str)\n",
    "feature_cols = df_train_onehot.columns.drop([\"Rings\", \"kfold\"]).to_list()\n",
    "feature_cols_to_normalize = cols_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.NORMALIZE_DATA:\n",
    "    df_train_onehot = tt.normalize_features(df_train_onehot, \n",
    "                                            scaler=Config.SCALER,\n",
    "                                            features_to_normalize=feature_cols_to_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lgbm_tuning_params(trial):\n",
    "    params_static = {\n",
    "        \"objective\": \"mean_squared_error\",\n",
    "        \"metric\": None,\n",
    "        \"verbosity\": -1,    # <0: fatal, =0: error (warn), =1: info, >1: debug\n",
    "        \"boosting_type\": \"gbdt\"\n",
    "    }\n",
    "    params_dynamic = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1.0, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=25),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 20),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 4, 128, step=4),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1, 100)\n",
    "    }\n",
    "    return {**params_static, **params_dynamic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tuning_params(trial, model_name):\n",
    "    if model_name == enums.ModelName.Ridge:\n",
    "        return {\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-4, 1e4, log=True)\n",
    "        }\n",
    "    if model_name == enums.ModelName.Lasso:\n",
    "        return {\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-4, 1e4, log=True)\n",
    "        }\n",
    "    if model_name == enums.ModelName.RandomForest:\n",
    "        return {        \n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 3000, step=100),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 10, 30),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 2, 16),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 16),\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"log2\", \"sqrt\", None])\n",
    "        }\n",
    "    if model_name == enums.ModelName.XGBoost:\n",
    "        return {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"eval_metric\": \"rmsle\",\n",
    "            \"seed\": Config.RANDOM_SEED,\n",
    "            \"verbosity\": 0,\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 2000, step=100),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1.0, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 4, 32),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 1),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1, 100),\n",
    "            'early_stopping_rounds': trial.suggest_int('early_stopping_rounds', 100, 500, step=20)\n",
    "        }\n",
    "    if model_name == enums.ModelName.LGBM:\n",
    "        return get_lgbm_tuning_params(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparams_tuning_objective(trial, model_name, df_train,  \n",
    "                                 feature_cols, metric, target_col_name, single_fold=False,\n",
    "                                 num_folds=5, val_preds_col=\"val_preds\"):           \n",
    "    model_params = get_model_tuning_params(trial, model_name)    \n",
    "    fold_metrics_model, df_val_preds = tt.run_training(\n",
    "        model_name=model_name,\n",
    "        df_train=df_train,\n",
    "        target_col_name=target_col_name,\n",
    "        feature_col_names=feature_cols,\n",
    "        metric=metric,            \n",
    "        num_folds=num_folds,\n",
    "        model_params=model_params,\n",
    "        val_preds_col=val_preds_col,\n",
    "        single_fold=single_fold,\n",
    "        suppress_print=True\n",
    "    )       \n",
    "    fold_metrics = [x[0] for x in fold_metrics_model]\n",
    "    mean_metric = statistics.mean(fold_metrics)                \n",
    "    return mean_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model_params(study_name, study_direction, num_trials, model_name, \n",
    "                      df_train,  feature_cols, metric, target_col_name, \n",
    "                      single_fold=False, num_folds=5, val_preds_col=\"val_preds\"):\n",
    "    model_params_tuning_obj_partial = partial(\n",
    "        hyperparams_tuning_objective,\n",
    "        model_name=model_name,        \n",
    "        df_train=df_train,\n",
    "        feature_cols=feature_cols,\n",
    "        metric=metric,\n",
    "        target_col_name=target_col_name,\n",
    "        single_fold=single_fold,\n",
    "        num_folds=num_folds,\n",
    "        val_preds_col=val_preds_col\n",
    "    )\n",
    "    study = optuna.create_study(direction=study_direction, study_name=study_name)\n",
    "    study.optimize(model_params_tuning_obj_partial, n_trials=num_trials)\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"Best trial: number = {best_trial.number}, value = {best_trial.value}, params = {best_trial.params}\")\n",
    "    return best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2024-05-09 10:59:32,017]\u001b[0m A new study created in memory with name: LightGBM_ModelTuning\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 10:59:35,159]\u001b[0m Trial 0 finished with value: 0.15058018476653942 and parameters: {'learning_rate': 0.24694311142013126, 'n_estimators': 875, 'max_depth': 11, 'min_child_weight': 8, 'subsample': 0.6867207862176299, 'colsample_bytree': 0.6481004767345344, 'num_leaves': 44, 'reg_alpha': 0.6536881907646401, 'reg_lambda': 89.0795454915248}. Best is trial 0 with value: 0.15058018476653942.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 10:59:39,507]\u001b[0m Trial 1 finished with value: 0.14925369645315922 and parameters: {'learning_rate': 0.06324563649683515, 'n_estimators': 100, 'max_depth': 7, 'min_child_weight': 7, 'subsample': 0.7464870072702409, 'colsample_bytree': 0.5650683888692759, 'num_leaves': 120, 'reg_alpha': 0.8786633380791119, 'reg_lambda': 13.402749252736886}. Best is trial 1 with value: 0.14925369645315922.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 10:59:46,289]\u001b[0m Trial 2 finished with value: 0.1830042839306673 and parameters: {'learning_rate': 0.001143553297981841, 'n_estimators': 775, 'max_depth': 18, 'min_child_weight': 2, 'subsample': 0.8949427593112008, 'colsample_bytree': 0.909308035421142, 'num_leaves': 108, 'reg_alpha': 0.1833212698277752, 'reg_lambda': 57.276181351673785}. Best is trial 1 with value: 0.14925369645315922.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 10:59:49,579]\u001b[0m Trial 3 finished with value: 0.14883187251342106 and parameters: {'learning_rate': 0.021975611420437007, 'n_estimators': 900, 'max_depth': 7, 'min_child_weight': 2, 'subsample': 0.6016627719262843, 'colsample_bytree': 0.7538153027864976, 'num_leaves': 32, 'reg_alpha': 0.4918037098984753, 'reg_lambda': 65.81717039518784}. Best is trial 3 with value: 0.14883187251342106.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 10:59:52,771]\u001b[0m Trial 4 finished with value: 0.14835347979052566 and parameters: {'learning_rate': 0.09475561586489388, 'n_estimators': 450, 'max_depth': 20, 'min_child_weight': 5, 'subsample': 0.8868560193188869, 'colsample_bytree': 0.6726105552555475, 'num_leaves': 64, 'reg_alpha': 0.886799035961118, 'reg_lambda': 83.74098566881459}. Best is trial 4 with value: 0.14835347979052566.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 10:59:55,402]\u001b[0m Trial 5 finished with value: 0.16672327535860185 and parameters: {'learning_rate': 0.002426822632688159, 'n_estimators': 725, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.788391494442487, 'colsample_bytree': 0.6258775849274181, 'num_leaves': 52, 'reg_alpha': 0.9307881455769824, 'reg_lambda': 79.18533834603849}. Best is trial 4 with value: 0.14835347979052566.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 10:59:59,975]\u001b[0m Trial 6 finished with value: 0.14903996240142467 and parameters: {'learning_rate': 0.014610835331160005, 'n_estimators': 950, 'max_depth': 7, 'min_child_weight': 3, 'subsample': 0.55466196670935, 'colsample_bytree': 0.9474542552438109, 'num_leaves': 40, 'reg_alpha': 0.5501973959571191, 'reg_lambda': 36.17033112810424}. Best is trial 4 with value: 0.14835347979052566.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 11:00:03,076]\u001b[0m Trial 7 finished with value: 0.1499819280749789 and parameters: {'learning_rate': 0.4572333715362921, 'n_estimators': 900, 'max_depth': 18, 'min_child_weight': 8, 'subsample': 0.6792694694861794, 'colsample_bytree': 0.839387914308505, 'num_leaves': 8, 'reg_alpha': 0.3826902313010301, 'reg_lambda': 52.92063520337131}. Best is trial 4 with value: 0.14835347979052566.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 11:00:07,104]\u001b[0m Trial 8 finished with value: 0.149649368850916 and parameters: {'learning_rate': 0.02106496132497345, 'n_estimators': 675, 'max_depth': 16, 'min_child_weight': 7, 'subsample': 0.5981132409010534, 'colsample_bytree': 0.9903298432360783, 'num_leaves': 20, 'reg_alpha': 0.6330090869877473, 'reg_lambda': 89.06220763573668}. Best is trial 4 with value: 0.14835347979052566.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 11:00:08,951]\u001b[0m Trial 9 finished with value: 0.15233200212298745 and parameters: {'learning_rate': 0.4585294640562151, 'n_estimators': 100, 'max_depth': 18, 'min_child_weight': 9, 'subsample': 0.768975927864475, 'colsample_bytree': 0.9742755083869673, 'num_leaves': 120, 'reg_alpha': 0.05335428411884224, 'reg_lambda': 28.953232606593975}. Best is trial 4 with value: 0.14835347979052566.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 11:00:11,868]\u001b[0m Trial 10 finished with value: 0.1484582397360321 and parameters: {'learning_rate': 0.10096675025582476, 'n_estimators': 375, 'max_depth': 14, 'min_child_weight': 5, 'subsample': 0.921154303838122, 'colsample_bytree': 0.5116510694167072, 'num_leaves': 84, 'reg_alpha': 0.8084995781004884, 'reg_lambda': 97.7551442700078}. Best is trial 4 with value: 0.14835347979052566.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 11:00:17,994]\u001b[0m Trial 11 finished with value: 0.1483769023836427 and parameters: {'learning_rate': 0.0983338451657062, 'n_estimators': 400, 'max_depth': 14, 'min_child_weight': 5, 'subsample': 0.9968530585319841, 'colsample_bytree': 0.5067080792076509, 'num_leaves': 80, 'reg_alpha': 0.7688720006357026, 'reg_lambda': 98.14659069599897}. Best is trial 4 with value: 0.14835347979052566.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 11:00:23,564]\u001b[0m Trial 12 finished with value: 0.14856989639254298 and parameters: {'learning_rate': 0.1157578375834187, 'n_estimators': 425, 'max_depth': 20, 'min_child_weight': 5, 'subsample': 0.9933314304367844, 'colsample_bytree': 0.6970254197999366, 'num_leaves': 76, 'reg_alpha': 0.7447206126322885, 'reg_lambda': 78.74401977274276}. Best is trial 4 with value: 0.14835347979052566.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 11:00:26,962]\u001b[0m Trial 13 finished with value: 0.1559043769760925 and parameters: {'learning_rate': 0.00670078971504237, 'n_estimators': 350, 'max_depth': 12, 'min_child_weight': 4, 'subsample': 0.980951472881284, 'colsample_bytree': 0.5080963873084916, 'num_leaves': 92, 'reg_alpha': 0.9580426187162159, 'reg_lambda': 98.38036983227491}. Best is trial 4 with value: 0.14835347979052566.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 11:00:30,198]\u001b[0m Trial 14 finished with value: 0.14813618680637217 and parameters: {'learning_rate': 0.05140366955156392, 'n_estimators': 500, 'max_depth': 14, 'min_child_weight': 6, 'subsample': 0.879216742575739, 'colsample_bytree': 0.7633991653386956, 'num_leaves': 68, 'reg_alpha': 0.7590024804950147, 'reg_lambda': 73.85120505908986}. Best is trial 14 with value: 0.14813618680637217.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 11:00:33,999]\u001b[0m Trial 15 finished with value: 0.14829450457231513 and parameters: {'learning_rate': 0.047424689399923135, 'n_estimators': 550, 'max_depth': 20, 'min_child_weight': 6, 'subsample': 0.8486172681956452, 'colsample_bytree': 0.7858580968314309, 'num_leaves': 64, 'reg_alpha': 0.9929533523320929, 'reg_lambda': 68.68811790005905}. Best is trial 14 with value: 0.14813618680637217.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 11:00:38,515]\u001b[0m Trial 16 finished with value: 0.14961997278227615 and parameters: {'learning_rate': 0.007987763676398203, 'n_estimators': 575, 'max_depth': 14, 'min_child_weight': 7, 'subsample': 0.833823223151002, 'colsample_bytree': 0.8008286464564198, 'num_leaves': 60, 'reg_alpha': 0.3749243441116294, 'reg_lambda': 67.7342934943235}. Best is trial 14 with value: 0.14813618680637217.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 11:00:40,385]\u001b[0m Trial 17 finished with value: 0.14853937445090812 and parameters: {'learning_rate': 0.040140520670196904, 'n_estimators': 250, 'max_depth': 9, 'min_child_weight': 6, 'subsample': 0.8438005988308626, 'colsample_bytree': 0.8512811199778392, 'num_leaves': 100, 'reg_alpha': 0.9750965636155827, 'reg_lambda': 41.28447424286021}. Best is trial 14 with value: 0.14813618680637217.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 11:00:43,411]\u001b[0m Trial 18 finished with value: 0.16772295993188246 and parameters: {'learning_rate': 0.951216171988012, 'n_estimators': 575, 'max_depth': 16, 'min_child_weight': 1, 'subsample': 0.826747449169709, 'colsample_bytree': 0.73732258373822, 'num_leaves': 72, 'reg_alpha': 0.6718951662039843, 'reg_lambda': 68.89125336345676}. Best is trial 14 with value: 0.14813618680637217.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 11:00:47,554]\u001b[0m Trial 19 finished with value: 0.1499429398479905 and parameters: {'learning_rate': 0.00765834963316133, 'n_estimators': 525, 'max_depth': 10, 'min_child_weight': 6, 'subsample': 0.9284799472670455, 'colsample_bytree': 0.77545820912873, 'num_leaves': 60, 'reg_alpha': 0.8129102696476493, 'reg_lambda': 61.20576454637848}. Best is trial 14 with value: 0.14813618680637217.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 11:00:48,350]\u001b[0m Trial 20 finished with value: 0.149712164886498 and parameters: {'learning_rate': 0.04207543768045919, 'n_estimators': 225, 'max_depth': 16, 'min_child_weight': 4, 'subsample': 0.704722021998593, 'colsample_bytree': 0.8790276545475582, 'num_leaves': 28, 'reg_alpha': 0.9925686046768623, 'reg_lambda': 46.597651216384094}. Best is trial 14 with value: 0.14813618680637217.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 11:00:51,596]\u001b[0m Trial 21 finished with value: 0.1494661282663477 and parameters: {'learning_rate': 0.18004433775128217, 'n_estimators': 500, 'max_depth': 20, 'min_child_weight': 4, 'subsample': 0.8859056926468174, 'colsample_bytree': 0.6909910403237628, 'num_leaves': 64, 'reg_alpha': 0.8651865211117576, 'reg_lambda': 78.71438439841943}. Best is trial 14 with value: 0.14813618680637217.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 11:00:56,709]\u001b[0m Trial 22 finished with value: 0.14822989400505018 and parameters: {'learning_rate': 0.056816887528742296, 'n_estimators': 650, 'max_depth': 20, 'min_child_weight': 6, 'subsample': 0.9306262515445201, 'colsample_bytree': 0.7128980202865314, 'num_leaves': 52, 'reg_alpha': 0.900783878301288, 'reg_lambda': 74.38345219963807}. Best is trial 14 with value: 0.14813618680637217.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 11:01:00,612]\u001b[0m Trial 23 finished with value: 0.14827678896038374 and parameters: {'learning_rate': 0.05359649701249164, 'n_estimators': 650, 'max_depth': 18, 'min_child_weight': 6, 'subsample': 0.9369933978637623, 'colsample_bytree': 0.8166215699132378, 'num_leaves': 48, 'reg_alpha': 0.7337287789273985, 'reg_lambda': 70.40581351117659}. Best is trial 14 with value: 0.14813618680637217.\u001b[0m\n",
      "\u001b[32m[I 2024-05-09 11:01:04,294]\u001b[0m Trial 24 finished with value: 0.14863624149146276 and parameters: {'learning_rate': 0.02533423882544269, 'n_estimators': 650, 'max_depth': 17, 'min_child_weight': 8, 'subsample': 0.9487424162770225, 'colsample_bytree': 0.7271522294549236, 'num_leaves': 48, 'reg_alpha': 0.7274247428770307, 'reg_lambda': 73.19998323129622}. Best is trial 14 with value: 0.14813618680637217.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: number = 14, value = 0.14813618680637217, params = {'learning_rate': 0.05140366955156392, 'n_estimators': 500, 'max_depth': 14, 'min_child_weight': 6, 'subsample': 0.879216742575739, 'colsample_bytree': 0.7633991653386956, 'num_leaves': 68, 'reg_alpha': 0.7590024804950147, 'reg_lambda': 73.85120505908986}\n"
     ]
    }
   ],
   "source": [
    "# tuned_model_params = tune_model_params(\n",
    "#                          study_name=Config.MODEL_TYPE + \"_ModelTuning\", \n",
    "#                          study_direction=\"minimize\",\n",
    "#                          num_trials=Config.NUM_TUNING_TRIALS,\n",
    "#                          model_name=Config.MODEL_TYPE,\n",
    "#                          df_train=df_train_onehot,\n",
    "#                          feature_cols=feature_cols,\n",
    "#                          metric=Config.METRIC,\n",
    "#                          target_col_name=Config.TARGET_COL_NAME,\n",
    "#                          single_fold=Config.TUNE_ON_SINGLE_FOLD,\n",
    "#                          num_folds=Config.NUM_FOLDS                         \n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom evaluation function (root mean squared log error) to be used with lightGBM\n",
    "from sklearn.metrics import mean_squared_log_error  # Import from scikit-learn\n",
    "\n",
    "def custom_rmsle(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Custom objective function for RMSLE in LightGBM.\n",
    "  \"\"\"\n",
    "  # Extract labels from LightGBM dataset object (y_pred)\n",
    "  y_pred = y_pred.get_label()\n",
    "  # Clip predictions to avoid log errors on zero values\n",
    "  y_pred = np.clip(y_pred, np.min(y_true), np.max(y_true))\n",
    "  rmsle = np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "  return 'rmsle', rmsle, True  # Last argument for minimization (lower is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping_eval(y_true, y_pred, trial):\n",
    "    # Extract labels and calculate RMSLE\n",
    "    rmsle = custom_rmsle(y_true, y_pred.get_label())[1]\n",
    "    trial.report(rmsle, minimize=True)  # Report RMSLE to Optuna for optimization\n",
    "    # if rmsle < some_threshold:  # Define your early stopping threshold\n",
    "    #   return True  # Early stopping signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lgbm_params(train_df, train_y, feature_cols, metric, params=None):    \n",
    "    train_data = lgbm.Dataset(data=train_df[feature_cols], label=train_y, feature_name=feature_cols)    \n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    lgbmtuner_cv = LightGBMTunerCV(\n",
    "        params,\n",
    "        train_set=train_data,        \n",
    "        feval=metric,\n",
    "        stratified=True,\n",
    "        shuffle=True,\n",
    "        nfold=Config.NUM_FOLDS,        \n",
    "        study=study,\n",
    "        #callbacks=[early_stopping(100), log_evaluation(100)]\n",
    "    )     \n",
    "    #study.optimize(lgbmtuner_cv, n_trials=Config.NUM_TUNING_TRIALS)\n",
    "    lgbmtuner_cv.run()                \n",
    "    print(\"Best Params: \", lgbmtuner_cv.best_params)    \n",
    "    print(\"Best score: \", lgbmtuner_cv.best_score)    \n",
    "    return lgbmtuner_cv    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM parameter tuning\n",
    "lgbm_params = {\n",
    "    \"objective\": \"mean_squared_error\",\n",
    "    \"metric\": None,\n",
    "    \"verbosity\": -1,\n",
    "    \"boosting_type\": \"gbdt\"\n",
    "}\n",
    "\n",
    "lgbm_tuned_model = tune_lgbm_params(train_df=df_train_onehot, train_y=df_train_onehot[Config.TARGET_COL_NAME], \n",
    "                                    feature_cols=feature_cols, metric=custom_rmsle, params=lgbm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge model (remove outliers, normalize data)\n",
    "# Best trial: number = 18, value = 0.16229338860497053, params = {'alpha': 2120.468857440699}\n",
    "\n",
    "# Lasso (remove outliers, normalize data)\n",
    "# Best trial: number = 10, value = 0.1627903459839038, params = {'alpha': 0.033288635201287185}\n",
    "\n",
    "# Random Forest (remove outliers, normalize data)\n",
    "# Best trial: number = 11, value = 0.14948122240295672, params = {'n_estimators': 1200, 'max_depth': 22, 'min_samples_leaf': 7, 'min_samples_split': 2, 'max_features': 'sqrt'}\n",
    "\n",
    "# XGB\n",
    "# Best trial: number = 22, value = 0.14865409012650604, params = {\n",
    "# 'n_estimators': 600, 'learning_rate': 0.015065276573848749, 'max_depth': 10, 'min_child_weight': 3, \n",
    "# 'gamma': 0.6023139295556132, 'subsample': 0.772199311472915, 'colsample_bytree': 0.7065564004210175, \n",
    "# 'reg_alpha': 0.15112588528335205, 'reg_lambda': 14.38817002024009, 'early_stopping_rounds': 350}\n",
    "\n",
    "# LGBM\n",
    "# Best trial: number = 12, value = 0.14806169103162473, params = {'learning_rate': 0.04248968464174889, 'n_estimators': 650, \n",
    "# 'max_depth': 12, 'min_child_weight': 4, 'subsample': 0.9894123754195663, 'colsample_bytree': 0.687112695399624, \n",
    "# 'num_leaves': 92, 'reg_alpha': 0.9626149380434225, 'reg_lambda': 69.88883953488258}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ridge model\n",
    "# params_ridge = {\"alpha\": 1963.746}\n",
    "# val_preds_col = \"val_preds\"\n",
    "# model = tt.get_model(Config.MODEL_TYPE, params_ridge)        \n",
    "# fold_metrics_model, df_val_preds = tt.run_training(\n",
    "#             model=model,\n",
    "#             df_train=df_train_onehot,\n",
    "#             target_col_name=Config.TARGET_COL_NAME,\n",
    "#             feature_col_names=feature_cols,\n",
    "#             metric=enums.Metrics.RMSLE,            \n",
    "#             num_folds=Config.NUM_FOLDS,\n",
    "#             gb_params=None,\n",
    "#             val_preds_col=val_preds_col,\n",
    "#             single_fold=False\n",
    "#         )       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from optuna.integration import LightGBMTunerCV\n",
    "# import lightgbm as lgb\n",
    "# import numpy as np\n",
    "# from sklearn.datasets import make_regression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Generate a sample dataset\n",
    "# X, y = make_regression(n_samples=500, n_features=10, noise=0.1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the custom evaluation function\n",
    "# def custom_rmsle(y_pred, data):\n",
    "#     y_true = data.get_label()\n",
    "#     y_pred = np.clip(y_pred, 0, None)  # Ensure non-negative predictions\n",
    "#     rmsle = np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "#     return 'rmsle', rmsle, True\n",
    "\n",
    "# # Create an Optuna study\n",
    "# study = optuna.create_study(direction='minimize')  # Minimize the RMSLE metric\n",
    "\n",
    "# # Configure the LightGBMTunerCV\n",
    "# tuner = LightGBMTunerCV(\n",
    "#     {\n",
    "#         'objective': 'regression',  # You can adjust this as needed\n",
    "#         'metric': 'rmse',  # Primary metric, but we're using a custom one\n",
    "#     },\n",
    "#     lgb.Dataset(X_train, label=y_train),\n",
    "#     feval=custom_rmsle,  # Use the custom RMSLE function\n",
    "#     optuna_study=study,\n",
    "#     time_budget=600,  # Tune for 10 minutes\n",
    "#     early_stopping_rounds=10,\n",
    "# )\n",
    "\n",
    "# # Run the tuner to optimize LightGBM's hyperparameters\n",
    "# tuner.run()\n",
    "\n",
    "# # Get the best parameters found by Optuna\n",
    "# best_params = tuner.best_params\n",
    "# print(\"Best parameters:\", best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
