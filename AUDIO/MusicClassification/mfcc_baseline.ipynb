{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipd\n",
    "from glob import glob\n",
    "import librosa\n",
    "import seaborn as sns\n",
    "import librosa.display\n",
    "import skimage.io\n",
    "import os\n",
    "import torch\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    NUM_CLASSES = 5\n",
    "    NUM_MFCC = 40\n",
    "    BATCH_SIZE = 256\n",
    "    NUM_FOLDS = 5\n",
    "    UNFREEZE_EPOCH_NO = 1\n",
    "    NUM_EPOCHS = 50\n",
    "    NUM_WORKERS = 8    \n",
    "    FAST_DEV_RUN = False\n",
    "    PRECISION = 16\n",
    "    DATA_ROOT_FOLDER = \"./data/\"\n",
    "    PATIENCE = 10    \n",
    "    RANDOM_SEED = 42\n",
    "    # model hyperparameters\n",
    "    MODEL_PARAMS = {    \n",
    "        \"drop_out\": 0.25,\n",
    "        \"lr\": 0.00036\n",
    "    }\n",
    "\n",
    "class AudioConfig:\n",
    "    # settings\n",
    "    # number of samples per time-step in spectrogram. Defaults to win_length / 4\n",
    "    hop_length = 512 \n",
    "    # number of bins in spectrogram. Height of image\n",
    "    n_mels = 224 \n",
    "    # number of time-steps. Width of image\n",
    "    time_steps = 223 \n",
    "    # number of samples per second\n",
    "    sampling_rate = 22050\n",
    "    # sec\n",
    "    duration = 10 \n",
    "    fmin = 20\n",
    "    fmax = sampling_rate // 2\n",
    "    # length of the windowed signal after padding with zeros. Default value = 2048 ( for music signals)    \n",
    "    n_fft = hop_length * 4\n",
    "    # Each frame of audio is windowed by window of length win_length and then padded with zeros to match n_fft. Defaults to n_fft\n",
    "    win_length = hop_length * 4    \n",
    "    padmode = 'constant'\n",
    "    samples = sampling_rate * duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc_exists(mfcc):\n",
    "    if mfcc is not None:\n",
    "        if len(mfcc) > 0:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>filepath</th>\n",
       "      <th>genre_id</th>\n",
       "      <th>genre</th>\n",
       "      <th>file_exists</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>mfcc_exists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10150</td>\n",
       "      <td>010150.ogg</td>\n",
       "      <td>train/010150.ogg</td>\n",
       "      <td>7</td>\n",
       "      <td>Instrumental</td>\n",
       "      <td>True</td>\n",
       "      <td>[-256.21112, 125.60442, -0.42895874, 85.35346,...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7358</td>\n",
       "      <td>007358.ogg</td>\n",
       "      <td>train/007358.ogg</td>\n",
       "      <td>2</td>\n",
       "      <td>Punk</td>\n",
       "      <td>True</td>\n",
       "      <td>[-37.284946, 120.587944, -26.513258, 50.83251,...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20573</td>\n",
       "      <td>020573.ogg</td>\n",
       "      <td>train/020573.ogg</td>\n",
       "      <td>5</td>\n",
       "      <td>Folk</td>\n",
       "      <td>True</td>\n",
       "      <td>[-364.31793, 154.59741, -48.284782, 15.837085,...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11170</td>\n",
       "      <td>011170.ogg</td>\n",
       "      <td>train/011170.ogg</td>\n",
       "      <td>12</td>\n",
       "      <td>Old-Time / Historic</td>\n",
       "      <td>True</td>\n",
       "      <td>[-346.21207, 202.10587, -74.181465, -60.517387...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16662</td>\n",
       "      <td>016662.ogg</td>\n",
       "      <td>train/016662.ogg</td>\n",
       "      <td>1</td>\n",
       "      <td>Rock</td>\n",
       "      <td>True</td>\n",
       "      <td>[-48.237347, 141.68365, -49.396336, 69.53004, ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   song_id    filename          filepath  genre_id                genre  \\\n",
       "0    10150  010150.ogg  train/010150.ogg         7         Instrumental   \n",
       "1     7358  007358.ogg  train/007358.ogg         2                 Punk   \n",
       "2    20573  020573.ogg  train/020573.ogg         5                 Folk   \n",
       "3    11170  011170.ogg  train/011170.ogg        12  Old-Time / Historic   \n",
       "4    16662  016662.ogg  train/016662.ogg         1                 Rock   \n",
       "\n",
       "   file_exists                                               mfcc  mfcc_exists  \n",
       "0         True  [-256.21112, 125.60442, -0.42895874, 85.35346,...         True  \n",
       "1         True  [-37.284946, 120.587944, -26.513258, 50.83251,...         True  \n",
       "2         True  [-364.31793, 154.59741, -48.284782, 15.837085,...         True  \n",
       "3         True  [-346.21207, 202.10587, -74.181465, -60.517387...         True  \n",
       "4         True  [-48.237347, 141.68365, -49.396336, 69.53004, ...         True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_pickle(Config.DATA_ROOT_FOLDER + \"df_train_mfcc.pkl\")\n",
    "df_train[\"mfcc_exists\"] = df_train.mfcc.map(lambda mfcc: mfcc_exists(mfcc))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>filepath</th>\n",
       "      <th>genre_id</th>\n",
       "      <th>genre</th>\n",
       "      <th>file_exists</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>mfcc_exists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>3137</td>\n",
       "      <td>003137.ogg</td>\n",
       "      <td>train/003137.ogg</td>\n",
       "      <td>1</td>\n",
       "      <td>Rock</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4668</th>\n",
       "      <td>11088</td>\n",
       "      <td>011088.ogg</td>\n",
       "      <td>train/011088.ogg</td>\n",
       "      <td>2</td>\n",
       "      <td>Punk</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5150</th>\n",
       "      <td>16312</td>\n",
       "      <td>016312.ogg</td>\n",
       "      <td>train/016312.ogg</td>\n",
       "      <td>7</td>\n",
       "      <td>Instrumental</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8764</th>\n",
       "      <td>24899</td>\n",
       "      <td>024899.ogg</td>\n",
       "      <td>train/024899.ogg</td>\n",
       "      <td>0</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10155</th>\n",
       "      <td>4040</td>\n",
       "      <td>004040.ogg</td>\n",
       "      <td>train/004040.ogg</td>\n",
       "      <td>4</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>9963</td>\n",
       "      <td>009963.ogg</td>\n",
       "      <td>train/009963.ogg</td>\n",
       "      <td>3</td>\n",
       "      <td>Experimental</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11386</th>\n",
       "      <td>15980</td>\n",
       "      <td>015980.ogg</td>\n",
       "      <td>train/015980.ogg</td>\n",
       "      <td>4</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11497</th>\n",
       "      <td>22698</td>\n",
       "      <td>022698.ogg</td>\n",
       "      <td>train/022698.ogg</td>\n",
       "      <td>4</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14377</th>\n",
       "      <td>23078</td>\n",
       "      <td>023078.ogg</td>\n",
       "      <td>train/023078.ogg</td>\n",
       "      <td>5</td>\n",
       "      <td>Folk</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14827</th>\n",
       "      <td>17940</td>\n",
       "      <td>017940.ogg</td>\n",
       "      <td>train/017940.ogg</td>\n",
       "      <td>0</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15145</th>\n",
       "      <td>22295</td>\n",
       "      <td>022295.ogg</td>\n",
       "      <td>train/022295.ogg</td>\n",
       "      <td>1</td>\n",
       "      <td>Rock</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18537</th>\n",
       "      <td>3071</td>\n",
       "      <td>003071.ogg</td>\n",
       "      <td>train/003071.ogg</td>\n",
       "      <td>2</td>\n",
       "      <td>Punk</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18796</th>\n",
       "      <td>13954</td>\n",
       "      <td>013954.ogg</td>\n",
       "      <td>train/013954.ogg</td>\n",
       "      <td>4</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       song_id    filename          filepath  genre_id         genre  \\\n",
       "1359      3137  003137.ogg  train/003137.ogg         1          Rock   \n",
       "4668     11088  011088.ogg  train/011088.ogg         2          Punk   \n",
       "5150     16312  016312.ogg  train/016312.ogg         7  Instrumental   \n",
       "8764     24899  024899.ogg  train/024899.ogg         0    Electronic   \n",
       "10155     4040  004040.ogg  train/004040.ogg         4       Hip-Hop   \n",
       "10873     9963  009963.ogg  train/009963.ogg         3  Experimental   \n",
       "11386    15980  015980.ogg  train/015980.ogg         4       Hip-Hop   \n",
       "11497    22698  022698.ogg  train/022698.ogg         4       Hip-Hop   \n",
       "14377    23078  023078.ogg  train/023078.ogg         5          Folk   \n",
       "14827    17940  017940.ogg  train/017940.ogg         0    Electronic   \n",
       "15145    22295  022295.ogg  train/022295.ogg         1          Rock   \n",
       "18537     3071  003071.ogg  train/003071.ogg         2          Punk   \n",
       "18796    13954  013954.ogg  train/013954.ogg         4       Hip-Hop   \n",
       "\n",
       "       file_exists  mfcc  mfcc_exists  \n",
       "1359         False  None        False  \n",
       "4668         False  None        False  \n",
       "5150         False  None        False  \n",
       "8764         False  None        False  \n",
       "10155        False  None        False  \n",
       "10873        False  None        False  \n",
       "11386        False  None        False  \n",
       "11497        False  None        False  \n",
       "14377        False  None        False  \n",
       "14827        False  None        False  \n",
       "15145        False  None        False  \n",
       "18537        False  None        False  \n",
       "18796        False  None        False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[~df_train.mfcc_exists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>filepath</th>\n",
       "      <th>file_exists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7072</td>\n",
       "      <td>007072.ogg</td>\n",
       "      <td>test/007072.ogg</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10207</td>\n",
       "      <td>010207.ogg</td>\n",
       "      <td>test/010207.ogg</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20008</td>\n",
       "      <td>020008.ogg</td>\n",
       "      <td>test/020008.ogg</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10924</td>\n",
       "      <td>010924.ogg</td>\n",
       "      <td>test/010924.ogg</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21896</td>\n",
       "      <td>021896.ogg</td>\n",
       "      <td>test/021896.ogg</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   song_id    filename         filepath  file_exists\n",
       "0     7072  007072.ogg  test/007072.ogg         True\n",
       "1    10207  010207.ogg  test/010207.ogg         True\n",
       "2    20008  020008.ogg  test/020008.ogg         True\n",
       "3    10924  010924.ogg  test/010924.ogg         True\n",
       "4    21896  021896.ogg  test/021896.ogg         True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(Config.DATA_ROOT_FOLDER + \"test.csv\")\n",
    "df_test[\"file_exists\"] = df_test.filepath.map(lambda fp: os.path.exists(Config.DATA_ROOT_FOLDER + fp))\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config.NUM_CLASSES = len(df_train.genre_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>filepath</th>\n",
       "      <th>genre_id</th>\n",
       "      <th>genre</th>\n",
       "      <th>file_exists</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>mfcc_exists</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2296</td>\n",
       "      <td>002296.ogg</td>\n",
       "      <td>train/002296.ogg</td>\n",
       "      <td>1</td>\n",
       "      <td>Rock</td>\n",
       "      <td>True</td>\n",
       "      <td>[-51.503376, 90.0839, -29.26633, 51.95741, -1....</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8198</td>\n",
       "      <td>008198.ogg</td>\n",
       "      <td>train/008198.ogg</td>\n",
       "      <td>17</td>\n",
       "      <td>Blues</td>\n",
       "      <td>True</td>\n",
       "      <td>[-219.78317, 179.04573, -47.457195, 30.100721,...</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17663</td>\n",
       "      <td>017663.ogg</td>\n",
       "      <td>train/017663.ogg</td>\n",
       "      <td>12</td>\n",
       "      <td>Old-Time / Historic</td>\n",
       "      <td>True</td>\n",
       "      <td>[-308.57285, 210.7832, -61.69949, -59.704113, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7167</td>\n",
       "      <td>007167.ogg</td>\n",
       "      <td>train/007167.ogg</td>\n",
       "      <td>9</td>\n",
       "      <td>International</td>\n",
       "      <td>True</td>\n",
       "      <td>[-204.00072, 187.04646, -35.93575, 29.989346, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11242</td>\n",
       "      <td>011242.ogg</td>\n",
       "      <td>train/011242.ogg</td>\n",
       "      <td>6</td>\n",
       "      <td>Chiptune / Glitch</td>\n",
       "      <td>True</td>\n",
       "      <td>[-75.96725, 91.53988, -38.52349, 64.71302, -28...</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   song_id    filename          filepath  genre_id                genre  \\\n",
       "0     2296  002296.ogg  train/002296.ogg         1                 Rock   \n",
       "1     8198  008198.ogg  train/008198.ogg        17                Blues   \n",
       "2    17663  017663.ogg  train/017663.ogg        12  Old-Time / Historic   \n",
       "3     7167  007167.ogg  train/007167.ogg         9        International   \n",
       "4    11242  011242.ogg  train/011242.ogg         6    Chiptune / Glitch   \n",
       "\n",
       "   file_exists                                               mfcc  \\\n",
       "0         True  [-51.503376, 90.0839, -29.26633, 51.95741, -1....   \n",
       "1         True  [-219.78317, 179.04573, -47.457195, 30.100721,...   \n",
       "2         True  [-308.57285, 210.7832, -61.69949, -59.704113, ...   \n",
       "3         True  [-204.00072, 187.04646, -35.93575, 29.989346, ...   \n",
       "4         True  [-75.96725, 91.53988, -38.52349, 64.71302, -28...   \n",
       "\n",
       "   mfcc_exists  kfold  \n",
       "0         True      3  \n",
       "1         True      2  \n",
       "2         True      2  \n",
       "3         True      3  \n",
       "4         True      3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the training dataframe into kfolds for cross validation. We do this before any processing is done\n",
    "# on the data. We use stratified kfold if the target distribution is unbalanced\n",
    "def strat_kfold_dataframe(df, target_col_name, num_folds=Config.NUM_FOLDS):\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    # randomize of shuffle the rows of dataframe before splitting is done\n",
    "    df = df.sample(frac=1, random_state=Config.RANDOM_SEED).reset_index(drop=True)\n",
    "    # get the target data\n",
    "    y = df[target_col_name].values\n",
    "    skf = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=Config.RANDOM_SEED)\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=df, y=y)):\n",
    "        df.loc[val_index, \"kfold\"] = fold    \n",
    "    return df     \n",
    "\n",
    "df_train = df_train[df_train.mfcc_exists]\n",
    "df_train = strat_kfold_dataframe(df_train, target_col_name=\"genre_id\")\n",
    "df_train.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(conf, pathname, trim_long_data):\n",
    "    y, sr = librosa.load(pathname, sr=None, res_type=\"kaiser_fast\")\n",
    "    # trim silence\n",
    "    if 0 < len(y): # workaround: 0 length causes error\n",
    "        y, _ = librosa.effects.trim(y) # trim, top_db=default(60)\n",
    "    # extract a fixed length window\n",
    "    start_sample = 0 # starting at beginning\n",
    "    length_samples = conf.time_steps * conf.hop_length    \n",
    "    # make it unified length to conf.samples\n",
    "    if len(y) > conf.samples: # long enough\n",
    "        if trim_long_data:\n",
    "            y = y[start_sample : start_sample+length_samples]        \n",
    "    else: # pad blank\n",
    "        padding = length_samples - len(y)    # add padding at both ends\n",
    "        offset = padding // 2\n",
    "        y = np.pad(y, (offset, conf.samples - len(y) - offset), conf.padmode)\n",
    "    return y, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_features(filename):\n",
    "    mfccs_processed = None\n",
    "    audio_path = Config.DATA_ROOT_FOLDER + \"train/\" + filename\n",
    "    if os.path.exists(audio_path):\n",
    "        audio, sample_rate = read_audio(AudioConfig, audio_path, True)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=Config.NUM_MFCC)\n",
    "        mfccs_processed = np.mean(mfccs.T,axis=0)     \n",
    "    return mfccs_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train[\"mfcc\"] = df_train.filename.apply(lambda filename: extract_mfcc_features(DATA_PATH + \"train/\" + filename))\n",
    "# df_train.to_pickle(\"df_train_mfcc.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import delayed, Parallel\n",
    "\n",
    "# df_train = df_train.head(100)\n",
    "# delayed_funcs_train = [delayed(extract_mfcc_features)(row[\"filename\"]) for i, row in df_train.iterrows()]\n",
    "# results_train = Parallel(n_jobs=-1, verbose=5)(delayed_funcs_train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataset contains the logic to fetch, load and if required transform data to bring it to a format\n",
    "# that can be used by dataloaders for training. \n",
    "class AudioMfccDataset(Dataset):\n",
    "    def __init__(self, df, mfcc_col, target_col, transform=None, target_transform=None):\n",
    "        self.df = df\n",
    "        self.mfcc_col = mfcc_col\n",
    "        self.target_col = target_col        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):                \n",
    "        mfcc = self.df.loc[index, self.mfcc_col]\n",
    "        target = self.df.loc[index, self.target_col]\n",
    "        if self.transform is not None:\n",
    "            mfcc_tfmd = self.transform(mfcc)            \n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return mfcc_tfmd, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_dls(fold, df_imgs):\n",
    "    df_train = df_imgs[df_imgs[\"kfold\"] != fold].reset_index(drop=True)\n",
    "    df_val = df_imgs[df_imgs[\"kfold\"] == fold].reset_index(drop=True)    \n",
    "    ds_train = AudioMfccDataset(\n",
    "        df_train, \n",
    "        mfcc_col=\"mfcc\",\n",
    "        target_col=\"genre_id\",        \n",
    "        transform=torch.as_tensor,\n",
    "        target_transform=torch.as_tensor\n",
    "    )\n",
    "    ds_val = AudioMfccDataset(\n",
    "        df_val, \n",
    "        mfcc_col=\"mfcc\",\n",
    "        target_col=\"genre_id\",        \n",
    "        transform=torch.as_tensor,\n",
    "        target_transform=torch.as_tensor\n",
    "    )        \n",
    "    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=Config.NUM_WORKERS)    \n",
    "    dl_val = DataLoader(ds_val, batch_size=Config.BATCH_SIZE, num_workers=Config.NUM_WORKERS)\n",
    "    return dl_train, dl_val, ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train, dl_val, ds_train, ds_val = get_fold_dls(0, df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 77.1201,  74.4379, -41.6419,  43.3034, -36.6637,  51.3666, -30.3715,\n",
       "          33.0450, -20.6842,  34.3427, -23.1814,  27.6178, -15.7877,  20.3962,\n",
       "         -10.9349,  12.6420,  -5.7834,  11.1199,  -3.6817,   0.7359,  -0.5042,\n",
       "           1.7668,   9.0716,  -2.0096,   3.4431,  -7.1916,   8.2092,  -7.3172,\n",
       "           9.1665,  -6.9760,   8.0860,  -6.1567,   7.5181,  -6.5443,   6.9241,\n",
       "          -4.6341,   3.5342,  -2.2990,   3.0409,  -1.7411]),\n",
       " tensor(3))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MfccAudioClfNet(nn.Module):\n",
    "    def __init__(self, num_mfcc, num_classes): \n",
    "        super().__init__()       \n",
    "        self.fc1 = nn.Linear(in_features=num_mfcc, out_features=256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout()\n",
    "        self.classifier = nn.Linear(in_features=128, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "import torchmetrics\n",
    "\n",
    "class AudioClfMfccLitModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes, num_mfcc, hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = hparams[\"lr\"]\n",
    "        self.num_classes = num_classes              \n",
    "        self.net = MfccAudioClfNet(num_mfcc=num_mfcc, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        return self.net(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, \"min\")        \n",
    "        return {\n",
    "            \"optimizer\": model_optimizer, \n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        loss = cross_entropy(y_pred, y)\n",
    "        train_f1 = torchmetrics.functional.f1(preds=y_pred, target=y, num_classes=self.num_classes, average=\"micro\")\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"train_f1\", train_f1, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return loss        \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        val_loss = cross_entropy(y_pred, y)\n",
    "        val_f1 = torchmetrics.functional.f1(preds=y_pred, target=y, num_classes=self.num_classes, average=\"micro\")\n",
    "        self.log(\"val_loss\", val_loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"val_f1\", val_f1, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return {\"loss\": val_loss, \"val_f1\": val_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, BackboneFinetuning, EarlyStopping\n",
    "\n",
    "# For results reproducibility \n",
    "# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "pl.seed_everything(Config.RANDOM_SEED, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class MetricsAggCallback(Callback):\n",
    "    def __init__(self, metric_to_monitor, mode):\n",
    "        self.metric_to_monitor = metric_to_monitor\n",
    "        self.metrics = []\n",
    "        self.best_metric = None\n",
    "        self.mode = mode\n",
    "        self.best_metric_epoch = None\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):\n",
    "        metric_value = trainer.callback_metrics[self.metric_to_monitor].cpu().detach().item()\n",
    "        val_loss = trainer.callback_metrics[\"val_loss\"].cpu().detach().item()\n",
    "        print(f\"metric {self.metric_to_monitor} = {metric_value}, val_loss={val_loss}\")        \n",
    "        self.metrics.append(metric_value)\n",
    "        if self.mode == \"max\":\n",
    "            self.best_metric = max(self.metrics)\n",
    "            self.best_metric_epoch = self.metrics.index(self.best_metric)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "def run_training(fold, dl_train, dl_val, fold_loss, fold_f1, find_lr=True):\n",
    "        fold_str = f\"fold{fold}\"\n",
    "        print(f\"Running training for {fold_str}\")\n",
    "        logger = None\n",
    "        chkpt_file_name = \"best_model_{epoch}_{val_loss:.4f}\"                \n",
    "        early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=Config.PATIENCE, mode=\"min\", verbose=True)        \n",
    "        if fold is not None:       \n",
    "            chkpt_file_name = fold_str + \"_\" + chkpt_file_name            \n",
    "        logger = WandbLogger(name=\"mfcc_baseline\", project=\"Pog_Music_Clf\")                    \n",
    "        cassava_model = AudioClfMfccLitModel(\n",
    "            num_classes=Config.NUM_CLASSES, \n",
    "            num_mfcc=Config.NUM_MFCC,\n",
    "            hparams=Config.MODEL_PARAMS\n",
    "        )    \n",
    "        loss_chkpt_callback = ModelCheckpoint(dirpath=\"./model\", verbose=True, monitor=\"val_loss\", mode=\"min\", filename=chkpt_file_name)\n",
    "        acc_chkpt_callback = MetricsAggCallback(metric_to_monitor=\"val_f1\", mode=\"max\")\n",
    "        trainer = pl.Trainer(\n",
    "            gpus=1,\n",
    "            # For results reproducibility \n",
    "            deterministic=True,\n",
    "            auto_select_gpus=True,\n",
    "            progress_bar_refresh_rate=20,\n",
    "            max_epochs=Config.NUM_EPOCHS,\n",
    "            logger=logger,\n",
    "            auto_lr_find=True,    \n",
    "            precision=Config.PRECISION,    \n",
    "            weights_summary=None, \n",
    "            fast_dev_run=Config.FAST_DEV_RUN,                   \n",
    "            callbacks=[loss_chkpt_callback, acc_chkpt_callback, early_stopping_callback]\n",
    "        )\n",
    "        if find_lr:\n",
    "            trainer.tune(model=cassava_model, train_dataloaders=dl_train)\n",
    "            print(cassava_model.lr)\n",
    "        trainer.fit(cassava_model, train_dataloaders=dl_train, val_dataloaders=dl_val)                \n",
    "        if not Config.FAST_DEV_RUN:\n",
    "            fold_loss.append(loss_chkpt_callback.best_model_score.cpu().detach().item())\n",
    "            fold_f1.append(acc_chkpt_callback.best_metric)\n",
    "            print(f\"Loss for {fold_str} = {fold_loss[fold]}, f1 = {fold_f1[fold]}\")\n",
    "        del trainer, cassava_model, early_stopping_callback, acc_chkpt_callback, loss_chkpt_callback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def print_exp_statistics(fold_loss, fold_acc):\n",
    "    print(\"Loss across folds\")\n",
    "    print(fold_loss)\n",
    "    print(\"Accuracy across folds\")\n",
    "    print(fold_acc)\n",
    "    #mean_loss = statistics.mean(fold_loss)\n",
    "    #mean_acc = statistics.mean(fold_acc)\n",
    "    #std_loss = statistics.stdev(fold_loss)\n",
    "    #std_acc = statistics.stdev(fold_acc)\n",
    "    #print(f\"mean loss across folds = {mean_loss}, loss stdev across fold = {std_loss}\")\n",
    "    #print(f\"mean accuracy across folds = {mean_acc}, accuracy stdev across fold = {std_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for fold0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ./model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Using native 16bit precision.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:101: UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop\n",
      "  rank_zero_warn(f\"you defined a {step_name} but have no {loader_name}. Skipping {stage} loop\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9303976049a452ea26c91ae018dddd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Restoring states from the checkpoint file at /home/bk_anupam/code/ML/KaggleChallenges/AUDIO/MusicClassification/lr_find_temp_model.ckpt\n",
      "Restored all states from the checkpoint file at /home/bk_anupam/code/ML/KaggleChallenges/AUDIO/MusicClassification/lr_find_temp_model.ckpt\n",
      "Learning rate set to 0.003981071705534969\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003981071705534969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/bk_anupam/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/bkanupam/Pog_Music_Clf/runs/1a450d1c\" target=\"_blank\">mfcc_baseline</a></strong> to <a href=\"https://wandb.ai/bkanupam/Pog_Music_Clf\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afd2d67153741a9a32fa3f5cd5ad937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.0703125, val_loss=7.652549743652344\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f8b1c4177641d8b1976ce3e153edb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 36it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be32929dcafa45c5be7a93fefca16743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 2.265\n",
      "Epoch 0, global step 62: val_loss reached 2.26545 (best 2.26545), saving model to \"/home/bk_anupam/code/ML/KaggleChallenges/AUDIO/MusicClassification/model/fold0_best_model_epoch=0_val_loss=2.2655.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.2506278157234192, val_loss=2.265454053878784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4384d39033a4a56ae23ec49efdb0190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.143 >= min_delta = 0.0. New best score: 2.123\n",
      "Epoch 1, global step 125: val_loss reached 2.12265 (best 2.12265), saving model to \"/home/bk_anupam/code/ML/KaggleChallenges/AUDIO/MusicClassification/model/fold0_best_model_epoch=1_val_loss=2.1227.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.3001004457473755, val_loss=2.1226511001586914\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67d3a719bc6429a86787183c643f6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.033 >= min_delta = 0.0. New best score: 2.090\n",
      "Epoch 2, global step 188: val_loss reached 2.08991 (best 2.08991), saving model to \"/home/bk_anupam/code/ML/KaggleChallenges/AUDIO/MusicClassification/model/fold0_best_model_epoch=2_val_loss=2.0899.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.30813661217689514, val_loss=2.0899057388305664\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b53803b25c41f3a0afb4db4bb6fa08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.010 >= min_delta = 0.0. New best score: 2.080\n",
      "Epoch 3, global step 251: val_loss reached 2.07959 (best 2.07959), saving model to \"/home/bk_anupam/code/ML/KaggleChallenges/AUDIO/MusicClassification/model/fold0_best_model_epoch=3_val_loss=2.0796.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.2950778603553772, val_loss=2.0795934200286865\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b5e4a130a44420a5f4e32878bf6fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 2.076\n",
      "Epoch 4, global step 314: val_loss reached 2.07638 (best 2.07638), saving model to \"/home/bk_anupam/code/ML/KaggleChallenges/AUDIO/MusicClassification/model/fold0_best_model_epoch=4_val_loss=2.0764.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.29432445764541626, val_loss=2.0763773918151855\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2523b7ffb9a74321892978caf94c4a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.011 >= min_delta = 0.0. New best score: 2.065\n",
      "Epoch 5, global step 377: val_loss reached 2.06539 (best 2.06539), saving model to \"/home/bk_anupam/code/ML/KaggleChallenges/AUDIO/MusicClassification/model/fold0_best_model_epoch=5_val_loss=2.0654.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.31215471029281616, val_loss=2.065389633178711\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfbf9a520e0408c86af26fdad4a150d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.041 >= min_delta = 0.0. New best score: 2.024\n",
      "Epoch 6, global step 440: val_loss reached 2.02431 (best 2.02431), saving model to \"/home/bk_anupam/code/ML/KaggleChallenges/AUDIO/MusicClassification/model/fold0_best_model_epoch=6_val_loss=2.0243.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.321948766708374, val_loss=2.0243079662323\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbcdbf609d341fab85001d1d0f1387e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 503: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.31567052006721497, val_loss=2.040850877761841\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddfb956b56354bc49e37a911be09ee33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 566: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.321948766708374, val_loss=2.0314948558807373\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b5851b5083849339a42c217cb60315e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 2.020\n",
      "Epoch 9, global step 629: val_loss reached 2.02006 (best 2.02006), saving model to \"/home/bk_anupam/code/ML/KaggleChallenges/AUDIO/MusicClassification/model/fold0_best_model_epoch=9_val_loss=2.0201.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.31491711735725403, val_loss=2.020061492919922\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e9961e0bc2409c974edf4a0d87abeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 692: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.31366148591041565, val_loss=2.0286526679992676\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c25da36aa848e78116a9ff75d7275d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 2.018\n",
      "Epoch 11, global step 755: val_loss reached 2.01817 (best 2.01817), saving model to \"/home/bk_anupam/code/ML/KaggleChallenges/AUDIO/MusicClassification/model/fold0_best_model_epoch=11_val_loss=2.0182.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.3264691233634949, val_loss=2.0181663036346436\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6422933d18431ea021ae584b582ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.012 >= min_delta = 0.0. New best score: 2.006\n",
      "Epoch 12, global step 818: val_loss reached 2.00577 (best 2.00577), saving model to \"/home/bk_anupam/code/ML/KaggleChallenges/AUDIO/MusicClassification/model/fold0_best_model_epoch=12_val_loss=2.0058-v1.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.3392767310142517, val_loss=2.0057671070098877\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3643547a18254c9c84bd64df8a8e75fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 881: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.3144148588180542, val_loss=2.024717330932617\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3bff9df5ed4418394a819550991a513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 944: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.3144148588180542, val_loss=2.0209555625915527\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f32d2c6cc3451bb093191b254d1393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 1007: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.3196885883808136, val_loss=2.0071396827697754\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e98e84e67e48a88f7665ee8bb9b945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 1070: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.3322451114654541, val_loss=2.0069897174835205\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e3310249a94ab5837cb11d5e68c786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 1133: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.3211953938007355, val_loss=2.0142407417297363\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d841c9d2a44492f9551d95335aa4b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 1196: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.31039679050445557, val_loss=2.0197834968566895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3801021b85ea42e7bd6b53f42523d9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.014 >= min_delta = 0.0. New best score: 1.992\n",
      "Epoch 19, global step 1259: val_loss reached 1.99198 (best 1.99198), saving model to \"/home/bk_anupam/code/ML/KaggleChallenges/AUDIO/MusicClassification/model/fold0_best_model_epoch=19_val_loss=1.9920-v1.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.33299848437309265, val_loss=1.9919838905334473\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9392fb130874a819c0261d4afeb60fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 1322: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.3267202377319336, val_loss=2.006587505340576\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5f5997c9a343f19bc559975bee28b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 1385: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.3211953938007355, val_loss=2.006376028060913\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d60f92a34b14069a06fb5e8a81b4be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 1448: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.3176795542240143, val_loss=2.017336130142212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0f37692e23404e970902f6d010b287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 1511: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.3412857949733734, val_loss=2.00628399848938\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11381ade0a64c0bba1e75151d93150e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 1574: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.33701658248901367, val_loss=2.001434564590454\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28d04324ed84b46b22ec02ef7ddb76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 1637: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.3317428529262543, val_loss=1.9947372674942017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a336d1fc12aa4acfa600f8524802716c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 1700: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.34856855869293213, val_loss=1.9958279132843018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736af26b662647939e2de397cb17d13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 1763: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.3324962258338928, val_loss=1.995415210723877\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c811b3e19d84dd28951f27fd05bc574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 1826: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.3211953938007355, val_loss=2.00467848777771\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a5fd4fde0f4d1eb821e753173c5ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 10 records. Best score: 1.992. Signaling Trainer to stop.\n",
      "Epoch 29, global step 1889: val_loss was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric val_f1 = 0.3307383358478546, val_loss=2.0053722858428955\n",
      "Loss for fold0 = 1.9919838905334473, f1 = 0.34856855869293213\n",
      "Loss across folds\n",
      "[1.9919838905334473]\n",
      "Accuracy across folds\n",
      "[0.34856855869293213]\n"
     ]
    }
   ],
   "source": [
    "find_lr = True\n",
    "fold_loss = []\n",
    "fold_acc = []\n",
    "for fold in range(Config.NUM_FOLDS):\n",
    "    dl_train, dl_val, ds_train, ds_val = get_fold_dls(fold, df_train)\n",
    "    run_training(fold, dl_train, dl_val, fold_loss, fold_acc, find_lr)\n",
    "    break  \n",
    "print_exp_statistics(fold_loss, fold_acc)       "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0197751694b00855cd01780d565fa2e16f7945f624c4146f8d6aac863c2ba178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
