{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import model_selection\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.io \n",
    "import librosa\n",
    "from PIL import Image\n",
    "import albumentations as alb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformationType:\n",
    "    TORCHVISION = \"torchvision\"\n",
    "    ALB = \"albumentations\"\n",
    "\n",
    "class Models:\n",
    "    RESNET34 = \"resnet34\"\n",
    "    RESNET50 = \"resnet50\"\n",
    "    RESNEXT50 = \"resnext50_32x4d\"    \n",
    "\n",
    "class ImgStats:\n",
    "    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "    IMAGENET_STD = [0.229, 0.224, 0.225]    \n",
    "\n",
    "# CONSTANTS\n",
    "class Config:\n",
    "    NUM_CLASSES = 5\n",
    "    BATCH_SIZE = 96\n",
    "    NUM_FOLDS = 5\n",
    "    UNFREEZE_EPOCH_NO = 1\n",
    "    NUM_EPOCHS = 5\n",
    "    NUM_WORKERS = 8\n",
    "    INPUT_IMAGE_SIZE = (128,128)\n",
    "    IMG_MEAN = ImgStats.IMAGENET_MEAN\n",
    "    IMG_STD = ImgStats.IMAGENET_STD\n",
    "    FAST_DEV_RUN = True\n",
    "    PRECISION = 16\n",
    "    DATA_ROOT_FOLDER = \"./data/\"\n",
    "    PATIENCE = 5\n",
    "    SUBSET_ROWS_FRAC = 0.1\n",
    "    TRAIN_ON_SUBSET = True\n",
    "    RANDOM_SEED = 42\n",
    "    MODEL_TO_USE = Models.RESNET34\n",
    "    # model hyperparameters\n",
    "    MODEL_PARAMS = {    \n",
    "        \"drop_out\": 0.25,\n",
    "        \"lr\": 0.00036\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioConfig:\n",
    "    # settings\n",
    "    hop_length = 512 # number of samples per time-step in spectrogram\n",
    "    n_mels = 128 # number of bins in spectrogram. Height of image\n",
    "    time_steps = 384 # number of time-steps. Width of image\n",
    "    sampling_rate = 22050\n",
    "    duration = 10 # sec\n",
    "    fmin = 20\n",
    "    fmax = sampling_rate // 2\n",
    "    n_mels = 128\n",
    "    n_fft = hop_length * 2\n",
    "    padmode = 'constant'\n",
    "    samples = sampling_rate * duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split the training dataframe into kfolds for cross validation. We do this before any processing is done\n",
    "# on the data. We use stratified kfold if the target distribution is unbalanced\n",
    "def strat_kfold_dataframe(df, target_col_name, num_folds=Config.NUM_FOLDS):\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    # randomize of shuffle the rows of dataframe before splitting is done\n",
    "    df = df.sample(frac=1, random_state=Config.RANDOM_SEED).reset_index(drop=True)\n",
    "    # get the target data\n",
    "    y = df[target_col_name].values\n",
    "    skf = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=Config.RANDOM_SEED)\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=df, y=y)):\n",
    "        df.loc[val_index, \"kfold\"] = fold    \n",
    "    return df     \n",
    "\n",
    "df_train = pd.read_csv(Config.DATA_ROOT_FOLDER + \"train.csv\")\n",
    "# filter out records without any corresponding mel spectrogram image\n",
    "df_train[\"mspec_exists\"] = df_train.filename.map(\n",
    "    lambda fp: os.path.exists(Config.DATA_ROOT_FOLDER + \"processed_train/mel_spec/\" + fp.split(\".\")[0] + \".jpg\")\n",
    ")\n",
    "df_train = df_train[df_train.mspec_exists]\n",
    "df_train = strat_kfold_dataframe(df_train, target_col_name=\"genre_id\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config.NUM_CLASSES = len(df_train.genre_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataset contains the logic to fetch, load and if required transform data to bring it to a format\n",
    "# that can be used by dataloaders for training. Image size is (128, 385, 3)\n",
    "class AudioMelSpecImgDataset(Dataset):\n",
    "    def __init__(self, df, file_name_col, target_col, img_root_folder, transform=None, target_transform=None):\n",
    "        self.df = df\n",
    "        self.file_name_col = file_name_col\n",
    "        self.target_col = target_col\n",
    "        self.img_root_folder = img_root_folder\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):        \n",
    "        file_name_noext = self.df.loc[index, self.file_name_col].split(\".\")[0]\n",
    "        img_path = self.img_root_folder + \"/\" + file_name_noext + \".jpg\"\n",
    "        img = Image.open(img_path)\n",
    "        img_label = self.df.loc[index, self.target_col]\n",
    "        if self.transform is not None:\n",
    "            img_tfmd = self.transform(img)            \n",
    "        if self.target_transform is not None:\n",
    "            img_label = self.target_transform(img_label)\n",
    "        return img_tfmd, img_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor, RandomResizedCrop, ToPILImage\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        #ToPILImage(),\n",
    "        RandomResizedCrop(size=(Config.INPUT_IMAGE_SIZE[0], Config.INPUT_IMAGE_SIZE[1])),                \n",
    "        ToTensor()\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "        #ToPILImage(),\n",
    "        RandomResizedCrop(size=(Config.INPUT_IMAGE_SIZE[0], Config.INPUT_IMAGE_SIZE[1])),        \n",
    "        ToTensor()        \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_dls(fold, df_imgs, img_root_folder):\n",
    "    df_train = df_imgs[df_imgs[\"kfold\"] != fold].reset_index(drop=True)\n",
    "    df_val = df_imgs[df_imgs[\"kfold\"] == fold].reset_index(drop=True)    \n",
    "    ds_train = AudioMelSpecImgDataset(\n",
    "        df_train, \n",
    "        file_name_col=\"filename\",\n",
    "        target_col=\"genre_id\",\n",
    "        img_root_folder=img_root_folder,\n",
    "        transform=train_transform,\n",
    "        target_transform=torch.as_tensor\n",
    "    )\n",
    "    ds_val = AudioMelSpecImgDataset(\n",
    "        df_val, \n",
    "        file_name_col=\"filename\",\n",
    "        target_col=\"genre_id\",\n",
    "        img_root_folder=img_root_folder,\n",
    "        transform=val_transform,\n",
    "        target_transform=torch.as_tensor\n",
    "    )        \n",
    "    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=Config.NUM_WORKERS)    \n",
    "    dl_val = DataLoader(ds_val, batch_size=Config.BATCH_SIZE, num_workers=Config.NUM_WORKERS)\n",
    "    return dl_train, dl_val, ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display images along with their labels from a batch where images are in form of numpy arrays \n",
    "# if predictions are provided along with labels, these are displayed too\n",
    "def show_batch(img_ds, num_items, num_rows, num_cols, predict_arr=None):\n",
    "    fig = plt.figure(figsize=(12, 6))    \n",
    "    img_index = np.random.randint(0, len(img_ds)-1, num_items)\n",
    "    for index, img_index in enumerate(img_index):  # list first 9 images\n",
    "        img, lb = img_ds[img_index]        \n",
    "        ax = fig.add_subplot(num_rows, num_cols, index + 1, xticks=[], yticks=[])\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.detach().numpy()\n",
    "        if isinstance(img, np.ndarray):\n",
    "            # the image data has RGB channels at dim 0, the shape of 3, 64, 64 needs to be 64, 64, 3 for display            \n",
    "            img = img.transpose(1, 2, 0)\n",
    "            ax.imshow(img)        \n",
    "        if isinstance(lb, torch.Tensor):\n",
    "            # extract the label from label tensor\n",
    "            lb = lb.item()            \n",
    "        title = f\"Actual: {lb}\"\n",
    "        if predict_arr: \n",
    "            title += f\", Pred: {predict_arr[img_index]}\"        \n",
    "        ax.set_title(title)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = \"./data/processed_train/mel_spec/000001.jpg\"\n",
    "# img = Image.open(img_path)\n",
    "# print(type(img))\n",
    "# img_arr = np.array(img)\n",
    "# img_arr = np.stack([img_arr]*3, axis=-1)\n",
    "# plt.imshow(img)\n",
    "# print(img_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(img_arr.shape)\n",
    "# img = transforms.ToPILImage()(img_arr)\n",
    "# print(type(img))\n",
    "# img_tfmd = val_transform(img)\n",
    "# img_np = img_tfmd.detach().numpy()\n",
    "# if isinstance(img_np, np.ndarray):\n",
    "#     # the image data has RGB channels at dim 0, the shape of 3, 64, 64 needs to be 64, 64, 3 for display            \n",
    "#     img_np = img_np.transpose(1, 2, 0)\n",
    "#     plt.imshow(img_np)\n",
    "#     print(img_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train, dl_val, ds_train, ds_val = get_fold_dls(0, df_train, Config.DATA_ROOT_FOLDER + \"/processed_train/mel_spec\")\n",
    "show_batch(ds_val, 8, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import cross_entropy\n",
    "import torchmetrics\n",
    "import timm\n",
    "\n",
    "class ImageClassificationLitModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes, hparams, model_to_use):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = hparams[\"lr\"]\n",
    "        self.num_classes = num_classes              \n",
    "        self.backbone, self.classifier = self.get_backbone_classifier(model_to_use, hparams[\"drop_out\"], num_classes) \n",
    "\n",
    "    @staticmethod\n",
    "    def get_backbone_classifier(model_to_use, drop_out, num_classes):\n",
    "        pt_model = timm.create_model(model_to_use, pretrained=True)\n",
    "        backbone = None\n",
    "        classifier = None\n",
    "        if model_to_use in [Models.RESNET34, Models.RESNET50, Models.RESNEXT50]:            \n",
    "            backbone = nn.Sequential(*list(pt_model.children())[:-1])\n",
    "            in_features = pt_model.fc.in_features\n",
    "            classifier = nn.Sequential(\n",
    "                nn.Dropout(drop_out),\n",
    "                nn.Linear(in_features, num_classes)\n",
    "            )    \n",
    "        return backbone, classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = torch.flatten(features, 1)                \n",
    "        x = self.classifier(features)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, \"min\")        \n",
    "        return {\n",
    "            \"optimizer\": model_optimizer, \n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        loss = cross_entropy(y_pred, y)\n",
    "        train_f1 = torchmetrics.functional.f1(preds=y_pred, target=y, num_classes=self.num_classes, average=\"micro\")\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"train_f1\", train_f1, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return loss        \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        val_loss = cross_entropy(y_pred, y)\n",
    "        val_f1 = torchmetrics.functional.f1(preds=y_pred, target=y, num_classes=self.num_classes, average=\"micro\")\n",
    "        self.log(\"val_loss\", val_loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"val_f1\", val_f1, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        return {\"loss\": val_loss, \"val_f1\": val_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, BackboneFinetuning, EarlyStopping\n",
    "\n",
    "# For results reproducibility \n",
    "# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "pl.seed_everything(Config.RANDOM_SEED, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class MetricsAggCallback(Callback):\n",
    "    def __init__(self, metric_to_monitor, mode):\n",
    "        self.metric_to_monitor = metric_to_monitor\n",
    "        self.metrics = []\n",
    "        self.best_metric = None\n",
    "        self.mode = mode\n",
    "        self.best_metric_epoch = None\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):\n",
    "        metric_value = trainer.callback_metrics[self.metric_to_monitor].cpu().detach().item()\n",
    "        val_loss = trainer.callback_metrics[\"val_loss\"].cpu().detach().item()\n",
    "        print(f\"metric {self.metric_to_monitor} = {metric_value}, val_loss={val_loss}\")        \n",
    "        self.metrics.append(metric_value)\n",
    "        if self.mode == \"max\":\n",
    "            self.best_metric = max(self.metrics)\n",
    "            self.best_metric_epoch = self.metrics.index(self.best_metric)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, dl_train, dl_val, fold_loss, fold_acc, find_lr=True):\n",
    "        fold_str = f\"fold{fold}\"\n",
    "        print(f\"Running training for {fold_str}\")\n",
    "        tb_logger = None\n",
    "        chkpt_file_name = \"best_model_{epoch}_{val_loss:.4f}\"        \n",
    "        multiplicative = lambda epoch: 1.5\n",
    "        backbone_finetuning = BackboneFinetuning(Config.UNFREEZE_EPOCH_NO, multiplicative, verbose=True)\n",
    "        early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=Config.PATIENCE, mode=\"min\", verbose=True)        \n",
    "        if fold is not None:       \n",
    "            chkpt_file_name = fold_str + \"_\" + chkpt_file_name\n",
    "            tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\", version=fold_str)\n",
    "        else:\n",
    "            tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\")        \n",
    "        cassava_model = ImageClassificationLitModel(\n",
    "            num_classes=Config.NUM_CLASSES, \n",
    "            hparams=Config.MODEL_PARAMS,        \n",
    "            model_to_use=Config.MODEL_TO_USE\n",
    "        )    \n",
    "        loss_chkpt_callback = ModelCheckpoint(dirpath=\"./model\", verbose=True, monitor=\"val_loss\", mode=\"min\", filename=chkpt_file_name)\n",
    "        acc_chkpt_callback = MetricsAggCallback(metric_to_monitor=\"val_acc\", mode=\"max\")\n",
    "        trainer = pl.Trainer(\n",
    "            gpus=1,\n",
    "            # For results reproducibility \n",
    "            deterministic=True,\n",
    "            auto_select_gpus=True,\n",
    "            progress_bar_refresh_rate=20,\n",
    "            max_epochs=Config.NUM_EPOCHS,\n",
    "            logger=tb_logger,\n",
    "            auto_lr_find=True,    \n",
    "            precision=Config.PRECISION,    \n",
    "            weights_summary=None, \n",
    "            fast_dev_run=Config.FAST_DEV_RUN,                   \n",
    "            callbacks=[loss_chkpt_callback, acc_chkpt_callback, backbone_finetuning, early_stopping_callback]\n",
    "        )\n",
    "        if find_lr:\n",
    "            trainer.tune(model=cassava_model, train_dataloaders=dl_train)\n",
    "            print(cassava_model.lr)\n",
    "        trainer.fit(cassava_model, train_dataloaders=dl_train, val_dataloaders=dl_val)                \n",
    "        if not Config.FAST_DEV_RUN:\n",
    "            fold_loss.append(loss_chkpt_callback.best_model_score.cpu().detach().item())\n",
    "            fold_acc.append(acc_chkpt_callback.best_metric)\n",
    "            print(f\"Loss for {fold_str} = {fold_loss[fold]}, accuracy = {fold_acc[fold]}\")\n",
    "        del trainer, cassava_model, backbone_finetuning, early_stopping_callback, acc_chkpt_callback, loss_chkpt_callback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def print_exp_statistics(fold_loss, fold_acc):\n",
    "    print(\"Loss across folds\")\n",
    "    print(fold_loss)\n",
    "    print(\"Accuracy across folds\")\n",
    "    print(fold_acc)\n",
    "    #mean_loss = statistics.mean(fold_loss)\n",
    "    #mean_acc = statistics.mean(fold_acc)\n",
    "    #std_loss = statistics.stdev(fold_loss)\n",
    "    #std_acc = statistics.stdev(fold_acc)\n",
    "    #print(f\"mean loss across folds = {mean_loss}, loss stdev across fold = {std_loss}\")\n",
    "    #print(f\"mean accuracy across folds = {mean_acc}, accuracy stdev across fold = {std_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_lr = True\n",
    "fold_loss = []\n",
    "fold_acc = []\n",
    "for fold in range(Config.NUM_FOLDS):\n",
    "    dl_train, dl_val, ds_train, ds_val = get_fold_dls(fold, df_train)\n",
    "    run_training(fold, dl_train, dl_val, fold_loss, fold_acc, find_lr)\n",
    "    break  \n",
    "print_exp_statistics(fold_loss, fold_acc)       "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0197751694b00855cd01780d565fa2e16f7945f624c4146f8d6aac863c2ba178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
