{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import model_selection\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.io \n",
    "import librosa\n",
    "from PIL import Image\n",
    "import audiomentations\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "from time import time\n",
    "from functools import wraps\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(\n",
    "    filename=\"./logs/info.log\", \n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s: %(levelname)s: %(message)s', \n",
    "    datefmt='%m/%d/%Y %I:%M:%S %p'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformationType:\n",
    "    TORCHVISION = \"torchvision\"\n",
    "    ALB = \"albumentations\"\n",
    "\n",
    "class Models:\n",
    "    RESNET34 = \"resnet34\"\n",
    "    RESNET50 = \"resnet50\"\n",
    "    RESNEXT50 = \"resnext50_32x4d\"    \n",
    "\n",
    "class ImgStats:\n",
    "    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "    IMAGENET_STD = [0.229, 0.224, 0.225]    \n",
    "\n",
    "# CONSTANTS\n",
    "class Config:\n",
    "    NUM_CLASSES = 5\n",
    "    BATCH_SIZE = 96\n",
    "    NUM_FOLDS = 5\n",
    "    UNFREEZE_EPOCH_NO = 1\n",
    "    NUM_EPOCHS = 5\n",
    "    NUM_WORKERS = 8\n",
    "    INPUT_IMAGE_SIZE = (128,128)\n",
    "    IMG_MEAN = ImgStats.IMAGENET_MEAN\n",
    "    IMG_STD = ImgStats.IMAGENET_STD\n",
    "    FAST_DEV_RUN = True\n",
    "    PRECISION = 16\n",
    "    DATA_ROOT_FOLDER = \"./data/\"\n",
    "    PATIENCE = 5\n",
    "    SUBSET_ROWS_FRAC = 0.1\n",
    "    TRAIN_ON_SUBSET = True\n",
    "    RANDOM_SEED = 42\n",
    "    MODEL_TO_USE = Models.RESNET34\n",
    "    # model hyperparameters\n",
    "    MODEL_PARAMS = {    \n",
    "        \"drop_out\": 0.25,\n",
    "        \"lr\": 0.00036\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioConfig:\n",
    "    # settings\n",
    "    # number of samples per time-step in spectrogram. Defaults to win_length / 4\n",
    "    # Also the step or stride between windows. If the step is smaller than the window length, the windows will overlap\n",
    "    hop_length = 512 \n",
    "    # number of bins in spectrogram. Height of image\n",
    "    n_mels = 128 \n",
    "    # number of time-steps. Width of image\n",
    "    time_steps = 512 \n",
    "    # number of samples per second\n",
    "    sampling_rate = 22050\n",
    "    # sec\n",
    "    duration = 5 \n",
    "    fmin = 20\n",
    "    fmax = sampling_rate // 2\n",
    "    # FFT window size or length of the windowed signal after padding with zeros. Default value = 2048 ( for music signals)    \n",
    "    n_fft = hop_length * 4\n",
    "    # Each frame of audio is windowed by window of length win_length and then padded with zeros to match n_fft. Defaults to n_fft\n",
    "    win_length = hop_length * 4    \n",
    "    padmode = 'constant'\n",
    "    samples = sampling_rate * duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit(func):\n",
    "    @wraps(func)\n",
    "    def wrap(*args, **kwargs):\n",
    "        start = time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time()\n",
    "        exec_time = end - start\n",
    "        logger.info(f\"Executing {func} took {exec_time} seconds\")\n",
    "        return result\n",
    "    return wrap        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>filepath</th>\n",
       "      <th>genre_id</th>\n",
       "      <th>genre</th>\n",
       "      <th>audio_exists</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2296</td>\n",
       "      <td>002296.ogg</td>\n",
       "      <td>train/002296.ogg</td>\n",
       "      <td>1</td>\n",
       "      <td>Rock</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8198</td>\n",
       "      <td>008198.ogg</td>\n",
       "      <td>train/008198.ogg</td>\n",
       "      <td>17</td>\n",
       "      <td>Blues</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17663</td>\n",
       "      <td>017663.ogg</td>\n",
       "      <td>train/017663.ogg</td>\n",
       "      <td>12</td>\n",
       "      <td>Old-Time / Historic</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7167</td>\n",
       "      <td>007167.ogg</td>\n",
       "      <td>train/007167.ogg</td>\n",
       "      <td>9</td>\n",
       "      <td>International</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11242</td>\n",
       "      <td>011242.ogg</td>\n",
       "      <td>train/011242.ogg</td>\n",
       "      <td>6</td>\n",
       "      <td>Chiptune / Glitch</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   song_id    filename          filepath  genre_id                genre  \\\n",
       "0     2296  002296.ogg  train/002296.ogg         1                 Rock   \n",
       "1     8198  008198.ogg  train/008198.ogg        17                Blues   \n",
       "2    17663  017663.ogg  train/017663.ogg        12  Old-Time / Historic   \n",
       "3     7167  007167.ogg  train/007167.ogg         9        International   \n",
       "4    11242  011242.ogg  train/011242.ogg         6    Chiptune / Glitch   \n",
       "\n",
       "   audio_exists  kfold  \n",
       "0          True      3  \n",
       "1          True      2  \n",
       "2          True      2  \n",
       "3          True      3  \n",
       "4          True      3  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# split the training dataframe into kfolds for cross validation. We do this before any processing is done\n",
    "# on the data. We use stratified kfold if the target distribution is unbalanced\n",
    "def strat_kfold_dataframe(df, target_col_name, num_folds=Config.NUM_FOLDS):\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    # randomize of shuffle the rows of dataframe before splitting is done\n",
    "    df = df.sample(frac=1, random_state=Config.RANDOM_SEED).reset_index(drop=True)\n",
    "    # get the target data\n",
    "    y = df[target_col_name].values\n",
    "    skf = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=Config.RANDOM_SEED)\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=df, y=y)):\n",
    "        df.loc[val_index, \"kfold\"] = fold    \n",
    "    return df     \n",
    "\n",
    "df_train = pd.read_csv(Config.DATA_ROOT_FOLDER + \"train.csv\")\n",
    "logger.info(\"Loaded train data frame\")\n",
    "# filter out records without any corresponding mel spectrogram image\n",
    "df_train[\"audio_exists\"] = df_train.filename.map(\n",
    "    lambda fp: os.path.exists(Config.DATA_ROOT_FOLDER + \"train/\" + fp)\n",
    ")\n",
    "df_train_noaudio = df_train[~df_train.audio_exists]\n",
    "df_train = df_train[df_train.audio_exists]\n",
    "df_train = strat_kfold_dataframe(df_train, target_col_name=\"genre_id\")\n",
    "logger.info(\"Peformed stratified k fold split of train data\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>filepath</th>\n",
       "      <th>genre_id</th>\n",
       "      <th>genre</th>\n",
       "      <th>audio_exists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>3137</td>\n",
       "      <td>003137.ogg</td>\n",
       "      <td>train/003137.ogg</td>\n",
       "      <td>1</td>\n",
       "      <td>Rock</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4668</th>\n",
       "      <td>11088</td>\n",
       "      <td>011088.ogg</td>\n",
       "      <td>train/011088.ogg</td>\n",
       "      <td>2</td>\n",
       "      <td>Punk</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5150</th>\n",
       "      <td>16312</td>\n",
       "      <td>016312.ogg</td>\n",
       "      <td>train/016312.ogg</td>\n",
       "      <td>7</td>\n",
       "      <td>Instrumental</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8764</th>\n",
       "      <td>24899</td>\n",
       "      <td>024899.ogg</td>\n",
       "      <td>train/024899.ogg</td>\n",
       "      <td>0</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10155</th>\n",
       "      <td>4040</td>\n",
       "      <td>004040.ogg</td>\n",
       "      <td>train/004040.ogg</td>\n",
       "      <td>4</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>9963</td>\n",
       "      <td>009963.ogg</td>\n",
       "      <td>train/009963.ogg</td>\n",
       "      <td>3</td>\n",
       "      <td>Experimental</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11386</th>\n",
       "      <td>15980</td>\n",
       "      <td>015980.ogg</td>\n",
       "      <td>train/015980.ogg</td>\n",
       "      <td>4</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11497</th>\n",
       "      <td>22698</td>\n",
       "      <td>022698.ogg</td>\n",
       "      <td>train/022698.ogg</td>\n",
       "      <td>4</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14377</th>\n",
       "      <td>23078</td>\n",
       "      <td>023078.ogg</td>\n",
       "      <td>train/023078.ogg</td>\n",
       "      <td>5</td>\n",
       "      <td>Folk</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14827</th>\n",
       "      <td>17940</td>\n",
       "      <td>017940.ogg</td>\n",
       "      <td>train/017940.ogg</td>\n",
       "      <td>0</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15145</th>\n",
       "      <td>22295</td>\n",
       "      <td>022295.ogg</td>\n",
       "      <td>train/022295.ogg</td>\n",
       "      <td>1</td>\n",
       "      <td>Rock</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18537</th>\n",
       "      <td>3071</td>\n",
       "      <td>003071.ogg</td>\n",
       "      <td>train/003071.ogg</td>\n",
       "      <td>2</td>\n",
       "      <td>Punk</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18796</th>\n",
       "      <td>13954</td>\n",
       "      <td>013954.ogg</td>\n",
       "      <td>train/013954.ogg</td>\n",
       "      <td>4</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       song_id    filename          filepath  genre_id         genre  \\\n",
       "1359      3137  003137.ogg  train/003137.ogg         1          Rock   \n",
       "4668     11088  011088.ogg  train/011088.ogg         2          Punk   \n",
       "5150     16312  016312.ogg  train/016312.ogg         7  Instrumental   \n",
       "8764     24899  024899.ogg  train/024899.ogg         0    Electronic   \n",
       "10155     4040  004040.ogg  train/004040.ogg         4       Hip-Hop   \n",
       "10873     9963  009963.ogg  train/009963.ogg         3  Experimental   \n",
       "11386    15980  015980.ogg  train/015980.ogg         4       Hip-Hop   \n",
       "11497    22698  022698.ogg  train/022698.ogg         4       Hip-Hop   \n",
       "14377    23078  023078.ogg  train/023078.ogg         5          Folk   \n",
       "14827    17940  017940.ogg  train/017940.ogg         0    Electronic   \n",
       "15145    22295  022295.ogg  train/022295.ogg         1          Rock   \n",
       "18537     3071  003071.ogg  train/003071.ogg         2          Punk   \n",
       "18796    13954  013954.ogg  train/013954.ogg         4       Hip-Hop   \n",
       "\n",
       "       audio_exists  \n",
       "1359          False  \n",
       "4668          False  \n",
       "5150          False  \n",
       "8764          False  \n",
       "10155         False  \n",
       "10873         False  \n",
       "11386         False  \n",
       "11497         False  \n",
       "14377         False  \n",
       "14827         False  \n",
       "15145         False  \n",
       "18537         False  \n",
       "18796         False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Records with no corresponding audio\n",
    "df_train_noaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_or_pad(y, length, is_train=True):\n",
    "    \"\"\"\n",
    "    Crops an array to a chosen length\n",
    "    Arguments:\n",
    "        y {1D np array} -- Array to crop\n",
    "        length {int} -- Length of the crop\n",
    "        sr {int} -- Sampling rate\n",
    "    Keyword Arguments:\n",
    "        train {bool} -- Whether we are at train time. If so, crop randomly, else return the beginning of y (default: {True})        \n",
    "    Returns:\n",
    "        1D np array -- Cropped array\n",
    "    \"\"\"\n",
    "    # trim silence\n",
    "    if len(y) > 0:\n",
    "        # trim, top_db=default(60) (Below 60dB is considered silence)\n",
    "        y, _ = librosa.effects.trim(y)\n",
    "    if length > len(y):\n",
    "        # if length of array is less than the length to be cropped, we need to pad \n",
    "        padding = length - len(y)    # add padding at both ends\n",
    "        offset = padding // 2\n",
    "        y = np.pad(y, (offset, padding - offset), AudioConfig.padmode)\n",
    "    else:\n",
    "        if not is_train:\n",
    "            start = 0\n",
    "        else:\n",
    "            start = np.random.randint(len(y) - length)            \n",
    "        y = y[start: start + length]\n",
    "    return y.astype(np.float32)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random 5 second clip of audio after trimming the silence\n",
    "@timeit\n",
    "def read_audio(audio_path, is_train=True, is_crop_or_pad=True):\n",
    "    y, sr = librosa.load(audio_path, sr=AudioConfig.sampling_rate, res_type=\"kaiser_fast\")\n",
    "    if is_crop_or_pad:\n",
    "        audio_samples_length = AudioConfig.sampling_rate * AudioConfig.duration\n",
    "        y = crop_or_pad(y, audio_samples_length, is_train=is_train)\n",
    "    return (y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mono_to_color(X, eps=1e-6, mean=None, std=None):\n",
    "    \"\"\"\n",
    "    Converts a one channel array to a 3 channel one in [0, 255]\n",
    "    Arguments:\n",
    "        X {numpy array [H x W]} -- 2D array to convert\n",
    "    Keyword Arguments:\n",
    "        eps {float} -- To avoid dividing by 0 (default: {1e-6})\n",
    "        mean {None or np array} -- Mean for normalization (default: {None})\n",
    "        std {None or np array} -- Std for normalization (default: {None})\n",
    "    Returns:\n",
    "        numpy array [3 x H x W] -- RGB numpy array\n",
    "    \"\"\"\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    X = (X - mean) / (std + eps)\n",
    "\n",
    "    # Normalize to [0, 255]\n",
    "    _min, _max = X.min(), X.max()\n",
    "\n",
    "    if (_max - _min) > eps:\n",
    "        V = np.clip(X, _min, _max)\n",
    "        V = 255 * (V - _min) / (_max - _min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        V = np.zeros_like(X, dtype=np.uint8)\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image, mean=None, std=None):\n",
    "    \"\"\"\n",
    "    Normalizes an array in [0, 255] to the format adapted to neural network\n",
    "    Arguments:\n",
    "        image {np array [3 x H x W]} -- [description]\n",
    "    Keyword Arguments:\n",
    "        mean {None or np array} -- Mean for normalization, expected of size 3 (default: {None})\n",
    "        std {None or np array} -- Std for normalization, expected of size 3 (default: {None})\n",
    "    Returns:\n",
    "        np array [H x W x 3] -- Normalized array\n",
    "    \"\"\"\n",
    "    image = image / 255.0\n",
    "    if mean is not None and std is not None:\n",
    "        image = (image - mean) / std\n",
    "    return np.moveaxis(image, 2, 0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def create_mel_spec(y, sr, conf):\n",
    "    \"\"\"\n",
    "    Computes a mel-spectrogram and puts it at decibel scale\n",
    "    Arguments:\n",
    "        y {np array} -- signal\n",
    "        sr {int} -- audio sample rate\n",
    "        conf {AudioParams} -- Parameters to use for the spectrogram. Expected to have the attributes sr, n_mels, n_fft, hop_length\n",
    "    Returns:\n",
    "        np array -- Mel-spectrogram\n",
    "    \"\"\"\n",
    "    mel_s = librosa.feature.melspectrogram(\n",
    "        y=y, \n",
    "        sr=sr, \n",
    "        n_mels=conf.n_mels,\n",
    "        n_fft=conf.n_fft, \n",
    "        hop_length=conf.hop_length,\n",
    "        fmin=conf.fmin,\n",
    "        fmax=conf.fmax\n",
    "    )\n",
    "    # convert amplitude to decibels\n",
    "    mel_s = librosa.power_to_db(mel_s, ref=np.max)    \n",
    "    mel_s = mono_to_color(mel_s)\n",
    "    mel_s = normalize(mel_s)\n",
    "    return mel_s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import AddGaussianSNR, TimeStretch, PitchShift, Shift\n",
    "from audiomentations import SpecChannelShuffle, SpecCompose, SpecFrequencyMask\n",
    "from torchvision.transforms import Compose, RandomResizedCrop, ToPILImage\n",
    "\n",
    "audio_transforms = audiomentations.Compose([\n",
    "    AddGaussianSNR(min_snr_in_db=5, max_snr_in_db=40.0, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "    Shift(min_fraction=-0.5, max_fraction=0.5, p=0.5),\n",
    "])\n",
    "\n",
    "mel_spec_transforms = audiomentations.SpecCompose([\n",
    "  SpecFrequencyMask(p=0.5),\n",
    "  SpecChannelShuffle(p=0.5)  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataset contains the logic to fetch, load and if required transform data to bring it to a format\n",
    "# that can be used by dataloaders for training. \n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, df, audio_transforms=None, melspec_transforms=None, is_train=True):\n",
    "        self.df = df\n",
    "        self.is_train = is_train        \n",
    "        self.audio_transforms = audio_transforms\n",
    "        self.melspec_transforms = melspec_transforms        \n",
    "\n",
    "    @timeit\n",
    "    def apply_audio_transforms(self, y, sr):        \n",
    "        y = self.audio_transforms(samples=y, sample_rate=sr)        \n",
    "        return y\n",
    "\n",
    "    @timeit\n",
    "    def apply_melspec_transforms(self, mel_s):\n",
    "        mel_s = self.melspec_transforms(mel_s)        \n",
    "        return mel_s\n",
    "\n",
    "    @timeit\n",
    "    def process_audio(self, index):\n",
    "        audio_path = Config.DATA_ROOT_FOLDER + \"train/\" + self.df.loc[index, \"filename\"]\n",
    "        # read in the original audio        \n",
    "        result = read_audio(audio_path, is_train=self.is_train, is_crop_or_pad=True)\n",
    "        y, sr = result[0], result[1]\n",
    "        # apply audio augmentations\n",
    "        if self.audio_transforms is not None:\n",
    "            y = self.apply_audio_transforms(y, sr)\n",
    "        mel_s = create_mel_spec(y, sr, AudioConfig)\n",
    "        logger.info(f\"mel_s.shape = {mel_s.shape}\")                \n",
    "        if self.melspec_transforms is not None:\n",
    "            # apply mel spectrogram augmentations\n",
    "            mel_s = self.apply_melspec_transforms(mel_s)            \n",
    "        # Resize the width of the image (dim=2 in mel spectrogram array)\n",
    "        start = np.random.randint(mel_s.shape[2] - Config.INPUT_IMAGE_SIZE[1])\n",
    "        mel_s = mel_s[:, :, start: start + Config.INPUT_IMAGE_SIZE[1]]\n",
    "        logger.info(f\"After resizing mel_s.shape = {mel_s.shape}\")                                        \n",
    "        target = self.df.loc[index, \"genre_id\"]\n",
    "        return mel_s, target\n",
    "\n",
    "    def __getitem__(self, index):        \n",
    "        result, exec_time = self.process_audio(index)\n",
    "        mel_s, target = result[0], result[1]\n",
    "        return mel_s, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold_dls(fold, df):\n",
    "    df_train = df[df[\"kfold\"] != fold].reset_index(drop=True)\n",
    "    df_val = df[df[\"kfold\"] == fold].reset_index(drop=True)    \n",
    "    ds_train = AudioDataset(\n",
    "        df=df_train,\n",
    "        audio_transforms=audio_transforms,\n",
    "        melspec_transforms=mel_spec_transforms,        \n",
    "        is_train=True\n",
    "    )\n",
    "    ds_val = AudioDataset(\n",
    "        df=df_val,\n",
    "        audio_transforms=audio_transforms,\n",
    "        melspec_transforms=mel_spec_transforms,        \n",
    "        is_train=False\n",
    "    )      \n",
    "    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=Config.NUM_WORKERS)    \n",
    "    dl_val = DataLoader(ds_val, batch_size=Config.BATCH_SIZE, num_workers=Config.NUM_WORKERS)\n",
    "    return dl_train, dl_val, ds_train, ds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train, dl_val, ds_train, ds_val = get_fold_dls(0, df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display images along with their labels from a batch where images are in form of numpy arrays \n",
    "# if predictions are provided along with labels, these are displayed too\n",
    "def show_batch(img_ds, num_items, num_rows, num_cols, predict_arr=None):\n",
    "    fig = plt.figure(figsize=(12, 6))    \n",
    "    img_index = np.random.randint(0, len(img_ds)-1, num_items)\n",
    "    for index, img_index in enumerate(img_index):  # list first 9 images\n",
    "        img, lb = img_ds[img_index]        \n",
    "        ax = fig.add_subplot(num_rows, num_cols, index + 1, xticks=[], yticks=[])\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.detach().numpy()\n",
    "        if isinstance(img, np.ndarray):\n",
    "            # the image data has RGB channels at dim 0, the shape of 3, 64, 64 needs to be 64, 64, 3 for display            \n",
    "            img = img.transpose(1, 2, 0)\n",
    "            ax.imshow(img)        \n",
    "        if isinstance(lb, torch.Tensor):\n",
    "            # extract the label from label tensor\n",
    "            lb = lb.item()            \n",
    "        title = f\"Actual: {lb}\"\n",
    "        if predict_arr: \n",
    "            title += f\", Pred: {predict_arr[img_index]}\"        \n",
    "        ax.set_title(title)  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0197751694b00855cd01780d565fa2e16f7945f624c4146f8d6aac863c2ba178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
