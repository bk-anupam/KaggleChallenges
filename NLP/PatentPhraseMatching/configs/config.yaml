defaults:
  - _self_
  - training_args: deberta-v3-small
  - paths: colab

train_run:
  random_state: 42
  num_labels: 1
  label_col: "score"
  num_folds: 5
  run_all_folds: False
  num_epochs: 2
  num_workers: 2    
  device: "cuda"
  subset_rows_frac: 0.05
  train_on_subset: False
  transformer_checkpoint: "microsoft/deberta-v3-small"
  experiment_name: ""
  save_artifacts: True
  loss_type: "bcewithlogits"
  # whether to output hidden state from each encoder layer. Set to True if we want to use a head
  # that makes use of all intermediate representations like concatlastfour
  output_hidden_states: False

wandb:
  key: None  
  project: "USPPPM"
  enabled: False    
