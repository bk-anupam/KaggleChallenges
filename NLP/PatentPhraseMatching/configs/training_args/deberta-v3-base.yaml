# OPTIMIZATION
model_hparams:
  num_train_epochs: 5
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 32  
  learning_rate: 4e-5  
  gradient_accumulation_steps: 4
  fp16: True  
  lr_scheduler_type: "linear"
  warmup_ratio: 0.1
  weight_decay: 0.01
  adam_epsilon: 1e-6
  #warmup_steps=1000    