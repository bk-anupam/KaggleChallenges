{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerTokenizerFast, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained(\"./data/longformer/\", local_files_only=True, add_prefix_space=True)\n",
    "# DataCollatorWithPadding pads each batch to the longest sequence length\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"This is a iterator driverless sample text for testing. How good is this beautification of grammarly?\", \n",
    "    \"This is a second text installation purposeful. Exclaimation is undress of poetry!\"\n",
    "] \n",
    "text_words = [item.split() for item in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tokenizer(\n",
    "        text_words,\n",
    "        is_split_into_words=True,\n",
    "        max_length=15,\n",
    "        padding=False, \n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True, \n",
    "        return_overflowing_tokens=True,\n",
    "        stride=2\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"overflow_to_sample_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## NEW TEXT ##########\n",
      "########## TOKEN_IDS ############\n",
      "15\n",
      "[0, 152, 16, 10, 49757, 1393, 1672, 7728, 2788, 13, 3044, 4, 1336, 205, 2]\n",
      "########## TOKENS ############\n",
      "15\n",
      "['<s>', 'ĠThis', 'Ġis', 'Ġa', 'Ġiterator', 'Ġdriver', 'less', 'Ġsample', 'Ġtext', 'Ġfor', 'Ġtesting', '.', 'ĠHow', 'Ġgood', '</s>']\n",
      "########## WORD_IDS ############\n",
      "15\n",
      "[None, 0, 1, 2, 3, 4, 4, 5, 6, 7, 8, 8, 9, 10, None]\n",
      "########## WORDS ############\n",
      "13\n",
      "['This', 'is', 'a', 'iterator', 'driverless', 'driverless', 'sample', 'text', 'for', 'testing.', 'testing.', 'How', 'good']\n",
      "########## TOKEN_IDS ############\n",
      "12\n",
      "[0, 1336, 205, 16, 42, 28651, 5000, 9, 33055, 352, 116, 2]\n",
      "########## TOKENS ############\n",
      "12\n",
      "['<s>', 'ĠHow', 'Ġgood', 'Ġis', 'Ġthis', 'Ġbeaut', 'ification', 'Ġof', 'Ġgrammar', 'ly', '?', '</s>']\n",
      "########## WORD_IDS ############\n",
      "12\n",
      "[None, 9, 10, 11, 12, 13, 13, 14, 15, 15, 15, None]\n",
      "########## WORDS ############\n",
      "10\n",
      "['How', 'good', 'is', 'this', 'beautification', 'beautification', 'of', 'grammarly?', 'grammarly?', 'grammarly?']\n",
      "\n",
      "########## NEW TEXT ##########\n",
      "########## TOKEN_IDS ############\n",
      "15\n",
      "[0, 152, 16, 10, 200, 2788, 8809, 3508, 2650, 4, 3015, 31628, 1258, 16, 2]\n",
      "########## TOKENS ############\n",
      "15\n",
      "['<s>', 'ĠThis', 'Ġis', 'Ġa', 'Ġsecond', 'Ġtext', 'Ġinstallation', 'Ġpurpose', 'ful', '.', 'ĠEx', 'claim', 'ation', 'Ġis', '</s>']\n",
      "########## WORD_IDS ############\n",
      "15\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 6, 6, 7, 7, 7, 8, None]\n",
      "########## WORDS ############\n",
      "13\n",
      "['This', 'is', 'a', 'second', 'text', 'installation', 'purposeful.', 'purposeful.', 'purposeful.', 'Exclaimation', 'Exclaimation', 'Exclaimation', 'is']\n",
      "########## TOKEN_IDS ############\n",
      "9\n",
      "[0, 1258, 16, 2432, 5224, 9, 14665, 328, 2]\n",
      "########## TOKENS ############\n",
      "9\n",
      "['<s>', 'ation', 'Ġis', 'Ġund', 'ress', 'Ġof', 'Ġpoetry', '!', '</s>']\n",
      "########## WORD_IDS ############\n",
      "9\n",
      "[None, 7, 8, 9, 9, 10, 11, 11, None]\n",
      "########## WORDS ############\n",
      "7\n",
      "['Exclaimation', 'is', 'undress', 'undress', 'of', 'poetry!', 'poetry!']\n"
     ]
    }
   ],
   "source": [
    "prev_sentence_id = -100\n",
    "for text_index, (token_ids, sentence_id) in enumerate(zip(result[\"input_ids\"], result[\"overflow_to_sample_mapping\"])):\n",
    "    if sentence_id != prev_sentence_id:\n",
    "        print(\"\\n########## NEW TEXT ##########\")\n",
    "    print(\"########## TOKEN_IDS ############\")\n",
    "    print(len(token_ids))\n",
    "    print(token_ids)    \n",
    "    print(\"########## TOKENS ############\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    print(len(tokens))\n",
    "    print(tokens)    \n",
    "    print(\"########## WORD_IDS ############\")   \n",
    "    word_ids = result.word_ids(batch_index=text_index)\n",
    "    print(len(word_ids))\n",
    "    print(word_ids)    \n",
    "    print(\"########## WORDS ############\")\n",
    "    words = text_words[sentence_id]    \n",
    "    sub_text_words = [words[word_id] for word_id in word_ids if word_id is not None]\n",
    "    print(len(sub_text_words))\n",
    "    print(sub_text_words)\n",
    "    prev_sentence_id = sentence_id"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0197751694b00855cd01780d565fa2e16f7945f624c4146f8d6aac863c2ba178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
