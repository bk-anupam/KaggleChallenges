{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import transformer_utils as tu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerTokenizerFast, DataCollatorWithPadding\n",
    "\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained(\"./data/longformer/\", local_files_only=True, add_prefix_space=True)\n",
    "# DataCollatorWithPadding pads each batch to the longest sequence length\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [\n",
    "    \"This is a iterator driverless sample text for testing. How good is this beautification of grammarly?\", \n",
    "    \"This is a second text installation purposeful. Exclaimation is undress of poetry!\"\n",
    "] \n",
    "text_words = [item.split() for item in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tokenizer(\n",
    "        text_words,\n",
    "        is_split_into_words=True,\n",
    "        max_length=15,\n",
    "        padding=False, \n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True, \n",
    "        return_overflowing_tokens=True,\n",
    "        stride=2\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 152, 16, 10, 49757, 1393, 1672, 7728, 2788, 13, 3044, 4, 1336, 205, 2], [0, 1336, 205, 16, 42, 28651, 5000, 9, 33055, 352, 116, 2], [0, 152, 16, 10, 200, 2788, 8809, 3508, 2650, 4, 3015, 31628, 1258, 16, 2], [0, 1258, 16, 2432, 5224, 9, 14665, 328, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]], 'offset_mapping': [[(0, 0), (1, 4), (1, 2), (1, 1), (1, 8), (1, 6), (6, 10), (1, 6), (1, 4), (1, 3), (1, 7), (7, 8), (1, 3), (1, 4), (0, 0)], [(0, 0), (1, 3), (1, 4), (1, 2), (1, 4), (1, 5), (5, 14), (1, 2), (1, 7), (7, 9), (9, 10), (0, 0)], [(0, 0), (1, 4), (1, 2), (1, 1), (1, 6), (1, 4), (1, 12), (1, 7), (7, 10), (10, 11), (1, 2), (2, 7), (7, 12), (1, 2), (0, 0)], [(0, 0), (7, 12), (1, 2), (1, 3), (3, 7), (1, 2), (1, 6), (6, 7), (0, 0)]], 'overflow_to_sample_mapping': [0, 0, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"overflow_to_sample_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 152, 16, 10, 49757, 1393, 1672, 7728, 2788, 13, 3044, 4, 1336, 205, 2],\n",
       " [0, 1336, 205, 16, 42, 28651, 5000, 9, 33055, 352, 116, 2],\n",
       " [0, 152, 16, 10, 200, 2788, 8809, 3508, 2650, 4, 3015, 31628, 1258, 16, 2],\n",
       " [0, 1258, 16, 2432, 5224, 9, 14665, 328, 2]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## NEW TEXT ##########\n",
      "########## TOKEN_IDS ############\n",
      "15\n",
      "[0, 152, 16, 10, 49757, 1393, 1672, 7728, 2788, 13, 3044, 4, 1336, 205, 2]\n",
      "########## TOKENS ############\n",
      "15\n",
      "['<s>', 'ĠThis', 'Ġis', 'Ġa', 'Ġiterator', 'Ġdriver', 'less', 'Ġsample', 'Ġtext', 'Ġfor', 'Ġtesting', '.', 'ĠHow', 'Ġgood', '</s>']\n",
      "########## WORD_IDS ############\n",
      "15\n",
      "[None, 0, 1, 2, 3, 4, 4, 5, 6, 7, 8, 8, 9, 10, None]\n",
      "########## WORDS ############\n",
      "13\n",
      "['This', 'is', 'a', 'iterator', 'driverless', 'driverless', 'sample', 'text', 'for', 'testing.', 'testing.', 'How', 'good']\n",
      "########## TOKEN_IDS ############\n",
      "12\n",
      "[0, 1336, 205, 16, 42, 28651, 5000, 9, 33055, 352, 116, 2]\n",
      "########## TOKENS ############\n",
      "12\n",
      "['<s>', 'ĠHow', 'Ġgood', 'Ġis', 'Ġthis', 'Ġbeaut', 'ification', 'Ġof', 'Ġgrammar', 'ly', '?', '</s>']\n",
      "########## WORD_IDS ############\n",
      "12\n",
      "[None, 9, 10, 11, 12, 13, 13, 14, 15, 15, 15, None]\n",
      "########## WORDS ############\n",
      "10\n",
      "['How', 'good', 'is', 'this', 'beautification', 'beautification', 'of', 'grammarly?', 'grammarly?', 'grammarly?']\n",
      "\n",
      "########## NEW TEXT ##########\n",
      "########## TOKEN_IDS ############\n",
      "15\n",
      "[0, 152, 16, 10, 200, 2788, 8809, 3508, 2650, 4, 3015, 31628, 1258, 16, 2]\n",
      "########## TOKENS ############\n",
      "15\n",
      "['<s>', 'ĠThis', 'Ġis', 'Ġa', 'Ġsecond', 'Ġtext', 'Ġinstallation', 'Ġpurpose', 'ful', '.', 'ĠEx', 'claim', 'ation', 'Ġis', '</s>']\n",
      "########## WORD_IDS ############\n",
      "15\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 6, 6, 7, 7, 7, 8, None]\n",
      "########## WORDS ############\n",
      "13\n",
      "['This', 'is', 'a', 'second', 'text', 'installation', 'purposeful.', 'purposeful.', 'purposeful.', 'Exclaimation', 'Exclaimation', 'Exclaimation', 'is']\n",
      "########## TOKEN_IDS ############\n",
      "9\n",
      "[0, 1258, 16, 2432, 5224, 9, 14665, 328, 2]\n",
      "########## TOKENS ############\n",
      "9\n",
      "['<s>', 'ation', 'Ġis', 'Ġund', 'ress', 'Ġof', 'Ġpoetry', '!', '</s>']\n",
      "########## WORD_IDS ############\n",
      "9\n",
      "[None, 7, 8, 9, 9, 10, 11, 11, None]\n",
      "########## WORDS ############\n",
      "7\n",
      "['Exclaimation', 'is', 'undress', 'undress', 'of', 'poetry!', 'poetry!']\n"
     ]
    }
   ],
   "source": [
    "prev_sentence_id = -100\n",
    "for text_index, (token_ids, sentence_id) in enumerate(zip(result[\"input_ids\"], result[\"overflow_to_sample_mapping\"])):\n",
    "    if sentence_id != prev_sentence_id:\n",
    "        print(\"\\n########## NEW TEXT ##########\")\n",
    "    print(\"########## TOKEN_IDS ############\")\n",
    "    print(len(token_ids))\n",
    "    print(token_ids)    \n",
    "    print(\"########## TOKENS ############\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    print(len(tokens))\n",
    "    print(tokens)    \n",
    "    print(\"########## WORD_IDS ############\")   \n",
    "    word_ids = result.word_ids(batch_index=text_index)\n",
    "    print(len(word_ids))\n",
    "    print(word_ids)    \n",
    "    print(\"########## WORDS ############\")\n",
    "    words = text_words[sentence_id]    \n",
    "    sub_text_words = [words[word_id] for word_id in word_ids if word_id is not None]\n",
    "    print(len(sub_text_words))\n",
    "    print(sub_text_words)\n",
    "    prev_sentence_id = sentence_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_text(tokenizer, data_row):\n",
    "#     # convert the text to word tokens splitting on \" \"        \n",
    "#     print(data_row[\"text\"])\n",
    "#     if isinstance(data_row[\"text\"], list) :\n",
    "#         text_words = [item.split() for item in data_row[\"text\"]]\n",
    "#     else:\n",
    "#         text_words = data_row[\"text\"].split()        \n",
    "#     print(text_words)\n",
    "#     encoding = tokenizer(\n",
    "#         text_words, \n",
    "#         is_split_into_words=True,\n",
    "#         max_length=15,\n",
    "#         padding=False, \n",
    "#         truncation=True,\n",
    "#         return_offsets_mapping=True, \n",
    "#         return_overflowing_tokens=True,\n",
    "#         stride=2\n",
    "#     ) \n",
    "#     word_ids = []\n",
    "#     for idx, token_ids in enumerate(encoding[\"input_ids\"]):\n",
    "#         # The word_id for CLS, SEP special tokens in None, we need to change is the special id of -100 so that\n",
    "#         # encoding can be converted to a tensor during batching        \n",
    "#         word_ids.append([-100 if wordid == None else wordid for wordid in encoding.word_ids(batch_index=idx)])        \n",
    "#     encoding[\"word_ids\"] = word_ids       \n",
    "#     return encoding      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(tokenizer, data_row):    \n",
    "    # convert the text to word tokens splitting on \" \"\n",
    "    # Batch tokenization scenario\n",
    "    if isinstance(data_row[\"text\"], list) :\n",
    "        text_words = [item.split() for item in data_row[\"text\"]]\n",
    "    # tokenize a single data row\n",
    "    else:\n",
    "        text_words = data_row[\"text\"].split()        \n",
    "\n",
    "    encoding = tokenizer(\n",
    "        text_words, \n",
    "        is_split_into_words=True,\n",
    "        max_length=15,\n",
    "        padding=False, \n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True, \n",
    "        return_overflowing_tokens=True,\n",
    "        stride=2\n",
    "    ) \n",
    "    word_ids = []\n",
    "    essay_ids = []    \n",
    "    for idx, (token_ids, text_id) in enumerate(zip(encoding[\"input_ids\"], encoding[\"overflow_to_sample_mapping\"])):        \n",
    "        if isinstance(data_row[\"text\"], list): \n",
    "            eid = data_row[\"essay_id\"][text_id]\n",
    "        else:\n",
    "            eid = data_row[\"essay_id\"]            \n",
    "        essay_ids.append(eid)\n",
    "        # The word_id for CLS, SEP special tokens in None, we need to change it to the special id of -100 so that\n",
    "        # encoding can be converted to a tensor during batching        \n",
    "        word_ids.append([-100 if wordid == None else wordid for wordid in encoding.word_ids(batch_index=idx)])                \n",
    "    encoding[\"word_ids\"] = word_ids\n",
    "    encoding[\"essay_id\"] = essay_ids\n",
    "    return encoding      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\", add_prefix_space=True)\n",
    "# DataCollatorWithPadding pads each batch to the longest sequence length\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({\n",
    "    \"essay_id\": pd.Series([], dtype=\"int64\"), \n",
    "    \"id\": pd.Series([], dtype=\"object\"), \n",
    "    \"text\": pd.Series([], dtype=\"object\"),\n",
    "    \"discourse_type\": pd.Series([], dtype=\"object\"),\n",
    "    \"prediction_string\": pd.Series([], dtype=\"object\")\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id = testid1, essay_id = 0,  text = This is a iterator driverless sample text for testing. How good is this beautification of grammarly?\n",
      "id = testid2, essay_id = 1,  text = This is a second text installation purposeful. Exclaimation is undress of poetry!\n"
     ]
    }
   ],
   "source": [
    "# load and read text from test directory\n",
    "essay_id = 0\n",
    "for id, text in zip([\"testid1\", \"testid2\"], test_text):        \n",
    "    print(f\"id = {id}, essay_id = {essay_id},  text = {text}\")\n",
    "    test_row = pd.Series({\n",
    "        \"essay_id\": essay_id,\n",
    "        \"id\": id,\n",
    "        \"text\": text,\n",
    "        \"discourse_type\": None,\n",
    "        \"prediction_string\": None\n",
    "    })\n",
    "    #encoding = preprocess_test_data(test_row)    \n",
    "    df_test = df_test.append(test_row, ignore_index=True)\n",
    "    essay_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>prediction_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>testid1</td>\n",
       "      <td>This is a iterator driverless sample text for ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>testid2</td>\n",
       "      <td>This is a second text installation purposeful....</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id       id                                               text  \\\n",
       "0         0  testid1  This is a iterator driverless sample text for ...   \n",
       "1         1  testid2  This is a second text installation purposeful....   \n",
       "\n",
       "  discourse_type prediction_string  \n",
       "0           None              None  \n",
       "1           None              None  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import datasets\n",
    "#from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create data loader for test data\n",
    "preprocess_test_data = partial(tokenize_text, tokenizer)\n",
    "ds_test_raw = datasets.Dataset.from_pandas(df_test)\n",
    "ds_test_raw_col_names = ds_test_raw.column_names\n",
    "ds_test = ds_test_raw.map(preprocess_test_data, batched=True, remove_columns=ds_test_raw_col_names)\n",
    "dl_test = DataLoader(ds_test, batch_size=2, collate_fn=data_collator, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 152, 16, 10, 49757, 1393, 1672, 7728, 2788, 13, 3044, 4, 1336, 205, 2],\n",
       " [0, 1336, 205, 16, 42, 28651, 5000, 9, 33055, 352, 116, 2],\n",
       " [0, 152, 16, 10, 200, 2788, 8809, 3508, 2650, 4, 3015, 31628, 1258, 16, 2],\n",
       " [0, 1258, 16, 2432, 5224, 9, 14665, 328, 2]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-100, 0, 1, 2, 3, 4, 4, 5, 6, 7, 8, 8, 9, 10, -100],\n",
       " [-100, 9, 10, 11, 12, 13, 13, 14, 15, 15, 15, -100],\n",
       " [-100, 0, 1, 2, 3, 4, 5, 6, 6, 6, 7, 7, 7, 8, -100],\n",
       " [-100, 7, 8, 9, 9, 10, 11, 11, -100]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test[\"word_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bk_anupam/anaconda3/envs/fastai/lib/python3.9/site-packages/pandas/core/internals/blocks.py:1002: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.array(value)\n"
     ]
    }
   ],
   "source": [
    "df_test.loc[0, [\"discourse_type\", \"prediction_string\"]] = [[\"test_discourse\"], [0,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>prediction_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>testid1</td>\n",
       "      <td>This is a iterator driverless sample text for ...</td>\n",
       "      <td>[test_discourse]</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>testid2</td>\n",
       "      <td>This is a second text installation purposeful....</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  \\\n",
       "0  testid1  This is a iterator driverless sample text for ...   \n",
       "1  testid2  This is a second text installation purposeful....   \n",
       "\n",
       "     discourse_type prediction_string  \n",
       "0  [test_discourse]         [0, 1, 2]  \n",
       "1              None              None  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"record_id\"] = pd.Series([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testid1 This is a iterator driverless sample text for testing. How good is this beautification of grammarly?\n",
      "testid2 This is a second text installation purposeful. Exclaimation is undress of poetry!\n"
     ]
    }
   ],
   "source": [
    "for id, text in zip([\"testid1\", \"testid2\"], test_text):\n",
    "    print(id, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['test_id1'], ['test text'], 0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict = {\"id\": [\"test_id1\"], \"text\": [\"test text\"], \"counter\": 0}\n",
    "\n",
    "list(test_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ['test_id1'], 'text': ['test text'], 'counter': 1}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict[\"counter\"] = test_dict[\"counter\"] + 1\n",
    "test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1] * 5"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0197751694b00855cd01780d565fa2e16f7945f624c4146f8d6aac863c2ba178"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
