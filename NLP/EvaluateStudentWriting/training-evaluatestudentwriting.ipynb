{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T11:20:46.782455Z","iopub.status.busy":"2022-02-19T11:20:46.781826Z","iopub.status.idle":"2022-02-19T11:20:48.539916Z","shell.execute_reply":"2022-02-19T11:20:48.53922Z","shell.execute_reply.started":"2022-02-19T11:20:46.782339Z"},"trusted":true},"outputs":[],"source":["import os\n","import functools\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","from transformers import AutoTokenizer, AutoModelForTokenClassification"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T11:21:10.312478Z","iopub.status.busy":"2022-02-19T11:21:10.312197Z","iopub.status.idle":"2022-02-19T11:21:10.317925Z","shell.execute_reply":"2022-02-19T11:21:10.317091Z","shell.execute_reply.started":"2022-02-19T11:21:10.312446Z"},"trusted":true},"outputs":[],"source":["DATA_PATH = \"./data/\"\n","\n","class Config:\n","    TRANSFORMER_CHECKPOINT = \"allenai/longformer-base-4096\"\n","    BATCH_SIZE = 4\n","    MAX_LENGTH = 4096\n","    STRIDE = 64\n","    NUM_FOLDS = 5\n","    RANDOM_STATE = 42\n","    NUM_WORKERS = 2"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T11:21:10.319669Z","iopub.status.busy":"2022-02-19T11:21:10.319445Z","iopub.status.idle":"2022-02-19T11:21:14.394623Z","shell.execute_reply":"2022-02-19T11:21:14.393798Z","shell.execute_reply.started":"2022-02-19T11:21:10.319641Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>discourse_id</th>\n","      <th>discourse_start</th>\n","      <th>discourse_end</th>\n","      <th>discourse_text</th>\n","      <th>discourse_type</th>\n","      <th>discourse_type_num</th>\n","      <th>predictionstring</th>\n","      <th>discoursetype</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>423A1CA112E2</td>\n","      <td>1.622628e+12</td>\n","      <td>8.0</td>\n","      <td>229.0</td>\n","      <td>Modern humans today are always on their phone....</td>\n","      <td>Lead</td>\n","      <td>Lead 1</td>\n","      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n","      <td>Lead</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>423A1CA112E2</td>\n","      <td>1.622628e+12</td>\n","      <td>230.0</td>\n","      <td>312.0</td>\n","      <td>They are some really bad consequences when stu...</td>\n","      <td>Position</td>\n","      <td>Position 1</td>\n","      <td>[45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 5...</td>\n","      <td>Position</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>423A1CA112E2</td>\n","      <td>1.622628e+12</td>\n","      <td>313.0</td>\n","      <td>401.0</td>\n","      <td>Some certain areas in the United States ban ph...</td>\n","      <td>Evidence</td>\n","      <td>Evidence 1</td>\n","      <td>[60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 7...</td>\n","      <td>Evidence</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>423A1CA112E2</td>\n","      <td>1.622628e+12</td>\n","      <td>402.0</td>\n","      <td>758.0</td>\n","      <td>When people have phones, they know about certa...</td>\n","      <td>Evidence</td>\n","      <td>Evidence 2</td>\n","      <td>[76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 8...</td>\n","      <td>Evidence</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>423A1CA112E2</td>\n","      <td>1.622628e+12</td>\n","      <td>759.0</td>\n","      <td>886.0</td>\n","      <td>Driving is one of the way how to get around. P...</td>\n","      <td>Claim</td>\n","      <td>Claim 1</td>\n","      <td>[139, 140, 141, 142, 143, 144, 145, 146, 147, ...</td>\n","      <td>Claim</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             id  discourse_id  discourse_start  discourse_end  \\\n","0  423A1CA112E2  1.622628e+12              8.0          229.0   \n","1  423A1CA112E2  1.622628e+12            230.0          312.0   \n","2  423A1CA112E2  1.622628e+12            313.0          401.0   \n","3  423A1CA112E2  1.622628e+12            402.0          758.0   \n","4  423A1CA112E2  1.622628e+12            759.0          886.0   \n","\n","                                      discourse_text discourse_type  \\\n","0  Modern humans today are always on their phone....           Lead   \n","1  They are some really bad consequences when stu...       Position   \n","2  Some certain areas in the United States ban ph...       Evidence   \n","3  When people have phones, they know about certa...       Evidence   \n","4  Driving is one of the way how to get around. P...          Claim   \n","\n","  discourse_type_num                                   predictionstring  \\\n","0             Lead 1  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n","1         Position 1  [45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 5...   \n","2         Evidence 1  [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 7...   \n","3         Evidence 2  [76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 8...   \n","4            Claim 1  [139, 140, 141, 142, 143, 144, 145, 146, 147, ...   \n","\n","  discoursetype  \n","0          Lead  \n","1      Position  \n","2      Evidence  \n","3      Evidence  \n","4         Claim  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df_train = pd.read_csv(DATA_PATH + \"train.csv\")\n","df_train[\"predictionstring\"] = df_train.predictionstring.apply(lambda str: [int(item) for item in str.split()])\n","df_train[\"discoursetype\"] = df_train.loc[:, \"discourse_type\"]\n","df_train.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T11:21:14.396638Z","iopub.status.busy":"2022-02-19T11:21:14.396205Z","iopub.status.idle":"2022-02-19T11:21:14.615954Z","shell.execute_reply":"2022-02-19T11:21:14.61536Z","shell.execute_reply.started":"2022-02-19T11:21:14.396603Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>discoursetype_Claim</th>\n","      <th>discoursetype_Concluding Statement</th>\n","      <th>discoursetype_Counterclaim</th>\n","      <th>discoursetype_Evidence</th>\n","      <th>discoursetype_Lead</th>\n","      <th>discoursetype_Position</th>\n","      <th>discoursetype_Rebuttal</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000D23A521A</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>00066EA9880D</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000E6DE9E817</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>001552828BD0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0016926B079C</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             id  discoursetype_Claim  discoursetype_Concluding Statement  \\\n","0  0000D23A521A                    1                                   1   \n","1  00066EA9880D                    3                                   1   \n","2  000E6DE9E817                    5                                   1   \n","3  001552828BD0                    4                                   0   \n","4  0016926B079C                    7                                   0   \n","\n","   discoursetype_Counterclaim  discoursetype_Evidence  discoursetype_Lead  \\\n","0                           1                       3                   0   \n","1                           0                       3                   1   \n","2                           1                       3                   0   \n","3                           0                       4                   1   \n","4                           0                       3                   0   \n","\n","   discoursetype_Position  discoursetype_Rebuttal  \n","0                       1                       1  \n","1                       1                       0  \n","2                       1                       1  \n","3                       1                       0  \n","4                       1                       0  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df_train_onehot = pd.get_dummies(df_train, columns=[\"discoursetype\"])\n","df_train_onehot = df_train_onehot.groupby([\"id\"], as_index=False).sum()\n","label_cols = [c for c in df_train_onehot.columns if c.startswith(\"discoursetype_\") or c == \"id\"]\n","df_train_onehot = df_train_onehot[label_cols]\n","df_train_onehot.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T11:21:14.617213Z","iopub.status.busy":"2022-02-19T11:21:14.616856Z","iopub.status.idle":"2022-02-19T11:21:14.621892Z","shell.execute_reply":"2022-02-19T11:21:14.62108Z","shell.execute_reply.started":"2022-02-19T11:21:14.617179Z"},"trusted":true},"outputs":[],"source":["def create_multilabel_targets(data_row, label_cols):\n","    targets = []\n","    for col in label_cols:\n","        targets.append(data_row[col])\n","    return targets"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T11:21:14.623232Z","iopub.status.busy":"2022-02-19T11:21:14.623002Z","iopub.status.idle":"2022-02-19T11:21:15.366216Z","shell.execute_reply":"2022-02-19T11:21:15.365449Z","shell.execute_reply.started":"2022-02-19T11:21:14.623205Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>discoursetype_Claim</th>\n","      <th>discoursetype_Concluding Statement</th>\n","      <th>discoursetype_Counterclaim</th>\n","      <th>discoursetype_Evidence</th>\n","      <th>discoursetype_Lead</th>\n","      <th>discoursetype_Position</th>\n","      <th>discoursetype_Rebuttal</th>\n","      <th>targets</th>\n","      <th>targets_str</th>\n","      <th>kfold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000D23A521A</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>[1, 1, 1, 3, 0, 1, 1]</td>\n","      <td>1,1,1,3,0,1,1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>00066EA9880D</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>[3, 1, 0, 3, 1, 1, 0]</td>\n","      <td>3,1,0,3,1,1,0</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000E6DE9E817</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>[5, 1, 1, 3, 0, 1, 1]</td>\n","      <td>5,1,1,3,0,1,1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>001552828BD0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>[4, 0, 0, 4, 1, 1, 0]</td>\n","      <td>4,0,0,4,1,1,0</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0016926B079C</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>[7, 0, 0, 3, 0, 1, 0]</td>\n","      <td>7,0,0,3,0,1,0</td>\n","      <td>-1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             id  discoursetype_Claim  discoursetype_Concluding Statement  \\\n","0  0000D23A521A                    1                                   1   \n","1  00066EA9880D                    3                                   1   \n","2  000E6DE9E817                    5                                   1   \n","3  001552828BD0                    4                                   0   \n","4  0016926B079C                    7                                   0   \n","\n","   discoursetype_Counterclaim  discoursetype_Evidence  discoursetype_Lead  \\\n","0                           1                       3                   0   \n","1                           0                       3                   1   \n","2                           1                       3                   0   \n","3                           0                       4                   1   \n","4                           0                       3                   0   \n","\n","   discoursetype_Position  discoursetype_Rebuttal                targets  \\\n","0                       1                       1  [1, 1, 1, 3, 0, 1, 1]   \n","1                       1                       0  [3, 1, 0, 3, 1, 1, 0]   \n","2                       1                       1  [5, 1, 1, 3, 0, 1, 1]   \n","3                       1                       0  [4, 0, 0, 4, 1, 1, 0]   \n","4                       1                       0  [7, 0, 0, 3, 0, 1, 0]   \n","\n","     targets_str  kfold  \n","0  1,1,1,3,0,1,1     -1  \n","1  3,1,0,3,1,1,0     -1  \n","2  5,1,1,3,0,1,1     -1  \n","3  4,0,0,4,1,1,0     -1  \n","4  7,0,0,3,0,1,0     -1  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# For each essay, there can be multiple discourse_types, the target which is discourse type is thus multilabel\n","# For each essay this multilabel target column needs to be created first \n","\n","if \"id\" in label_cols:\n","    label_cols.remove(\"id\")\n","df_train_onehot[\"targets\"] = df_train_onehot.apply(lambda row: create_multilabel_targets(row, label_cols), axis=1)\n","df_train_onehot[\"targets_str\"] = df_train_onehot.targets.apply(lambda x: \",\".join([str(item) for item in x]))\n","df_train_onehot[\"kfold\"] = -1\n","df_train_onehot.head()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T11:21:15.367439Z","iopub.status.busy":"2022-02-19T11:21:15.36724Z","iopub.status.idle":"2022-02-19T11:21:16.316801Z","shell.execute_reply":"2022-02-19T11:21:16.315957Z","shell.execute_reply.started":"2022-02-19T11:21:15.367415Z"},"trusted":true},"outputs":[],"source":["# we need to split the train data into k folds using multilabel stratification\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","\n","# This method uses the iterstrat library for multilabel stratification\n","def iterstrat_multilabel_stratified_kfold_cv_split(df_train_onehot):\n","    mskf = MultilabelStratifiedKFold(n_splits=Config.NUM_FOLDS, shuffle=True, random_state=Config.RANDOM_STATE)    \n","    df_targets = df_train_onehot[label_cols]\n","    for fold, (train_index, val_index) in enumerate(mskf.split(df_train_onehot[\"id\"], df_targets)):        \n","        df_train_onehot.loc[val_index, \"kfold\"] = fold\n","    return df_train_onehot"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T11:21:16.319321Z","iopub.status.busy":"2022-02-19T11:21:16.319101Z","iopub.status.idle":"2022-02-19T11:21:16.333071Z","shell.execute_reply":"2022-02-19T11:21:16.332232Z","shell.execute_reply.started":"2022-02-19T11:21:16.319295Z"},"trusted":true},"outputs":[],"source":["from skmultilearn.model_selection import IterativeStratification\n","\n","# This method uses the skmultilearn library for multilabel stratification\n","def skml_multilabel_stratified_kfold_cv_split(df_train_onehot):\n","    mskf = IterativeStratification(n_splits=Config.NUM_FOLDS, order=1)\n","    X = df_train_onehot[\"id\"]\n","    y = df_train_onehot[label_cols]\n","    for fold, (train_index, val_index) in enumerate(mskf.split(X, y)):        \n","        df_train_onehot.loc[val_index, \"kfold\"] = fold\n","    return df_train_onehot"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T11:21:16.334634Z","iopub.status.busy":"2022-02-19T11:21:16.334266Z","iopub.status.idle":"2022-02-19T11:21:29.027204Z","shell.execute_reply":"2022-02-19T11:21:29.02635Z","shell.execute_reply.started":"2022-02-19T11:21:16.334604Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0    3122\n","3    3120\n","2    3118\n","1    3117\n","4    3117\n","Name: kfold, dtype: int64"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df_train_onehot = skml_multilabel_stratified_kfold_cv_split(df_train_onehot)\n","df_train_onehot.kfold.value_counts()"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T11:21:29.028818Z","iopub.status.busy":"2022-02-19T11:21:29.028593Z","iopub.status.idle":"2022-02-19T11:21:29.039643Z","shell.execute_reply":"2022-02-19T11:21:29.038761Z","shell.execute_reply.started":"2022-02-19T11:21:29.028791Z"},"trusted":true},"outputs":[],"source":["from skmultilearn.model_selection.measures import get_combination_wise_output_matrix\n","from collections import Counter\n","\n","def get_train_val_split_stats(df):\n","    counts = {}\n","    for fold in range(Config.NUM_FOLDS):\n","        y_train = df[df.kfold != fold][label_cols].values\n","        y_val = df[df.kfold == fold][label_cols].values\n","        counts[(fold, \"train_count\")] = Counter(\n","                                        str(combination) for row in get_combination_wise_output_matrix(y_train, order=1) \n","                                        for combination in row\n","                                    )\n","        counts[(fold, \"val_count\")] = Counter(\n","                                        str(combination) for row in get_combination_wise_output_matrix(y_val, order=1) \n","                                        for combination in row\n","                                    )\n","    # View distributions\n","    df_counts = pd.DataFrame(counts).T.fillna(0)\n","    df_counts.index.set_names([\"fold\", \"counts\"], inplace=True)\n","    for fold in range(Config.NUM_FOLDS):\n","        train_counts = df_counts.loc[(fold, \"train_count\"), :]\n","        val_counts = df_counts.loc[(fold, \"val_count\"), :]\n","        val_train_ratio = pd.Series({i: val_counts[i] / train_counts[i] for i in train_counts.index}, name=(fold, \"val_train_ratio\"))\n","        df_counts = df_counts.append(val_train_ratio)\n","    df_counts = df_counts.sort_index() \n","    return df_counts"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T11:21:29.041345Z","iopub.status.busy":"2022-02-19T11:21:29.041132Z","iopub.status.idle":"2022-02-19T11:21:30.350209Z","shell.execute_reply":"2022-02-19T11:21:30.349361Z","shell.execute_reply.started":"2022-02-19T11:21:29.041319Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>(6,)</th>\n","      <th>(2,)</th>\n","      <th>(5,)</th>\n","      <th>(1,)</th>\n","      <th>(0,)</th>\n","      <th>(3,)</th>\n","      <th>(4,)</th>\n","    </tr>\n","    <tr>\n","      <th>fold</th>\n","      <th>counts</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">0</th>\n","      <th>train_count</th>\n","      <td>2895.000000</td>\n","      <td>3674.000000</td>\n","      <td>12284.000000</td>\n","      <td>10735.000000</td>\n","      <td>11941.000000</td>\n","      <td>12440.00</td>\n","      <td>7437.000000</td>\n","    </tr>\n","    <tr>\n","      <th>val_count</th>\n","      <td>703.000000</td>\n","      <td>902.000000</td>\n","      <td>3082.000000</td>\n","      <td>2683.000000</td>\n","      <td>2986.000000</td>\n","      <td>3110.00</td>\n","      <td>1864.000000</td>\n","    </tr>\n","    <tr>\n","      <th>val_train_ratio</th>\n","      <td>0.242832</td>\n","      <td>0.245509</td>\n","      <td>0.250895</td>\n","      <td>0.249930</td>\n","      <td>0.250063</td>\n","      <td>0.25</td>\n","      <td>0.250639</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">1</th>\n","      <th>train_count</th>\n","      <td>2878.000000</td>\n","      <td>3674.000000</td>\n","      <td>12296.000000</td>\n","      <td>10734.000000</td>\n","      <td>11942.000000</td>\n","      <td>12440.00</td>\n","      <td>7437.000000</td>\n","    </tr>\n","    <tr>\n","      <th>val_count</th>\n","      <td>720.000000</td>\n","      <td>902.000000</td>\n","      <td>3070.000000</td>\n","      <td>2684.000000</td>\n","      <td>2985.000000</td>\n","      <td>3110.00</td>\n","      <td>1864.000000</td>\n","    </tr>\n","    <tr>\n","      <th>val_train_ratio</th>\n","      <td>0.250174</td>\n","      <td>0.245509</td>\n","      <td>0.249675</td>\n","      <td>0.250047</td>\n","      <td>0.249958</td>\n","      <td>0.25</td>\n","      <td>0.250639</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">2</th>\n","      <th>train_count</th>\n","      <td>2870.000000</td>\n","      <td>3654.000000</td>\n","      <td>12294.000000</td>\n","      <td>10734.000000</td>\n","      <td>11942.000000</td>\n","      <td>12440.00</td>\n","      <td>7450.000000</td>\n","    </tr>\n","    <tr>\n","      <th>val_count</th>\n","      <td>728.000000</td>\n","      <td>922.000000</td>\n","      <td>3072.000000</td>\n","      <td>2684.000000</td>\n","      <td>2985.000000</td>\n","      <td>3110.00</td>\n","      <td>1851.000000</td>\n","    </tr>\n","    <tr>\n","      <th>val_train_ratio</th>\n","      <td>0.253659</td>\n","      <td>0.252326</td>\n","      <td>0.249878</td>\n","      <td>0.250047</td>\n","      <td>0.249958</td>\n","      <td>0.25</td>\n","      <td>0.248456</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">3</th>\n","      <th>train_count</th>\n","      <td>2898.000000</td>\n","      <td>3672.000000</td>\n","      <td>12281.000000</td>\n","      <td>10735.000000</td>\n","      <td>11941.000000</td>\n","      <td>12440.00</td>\n","      <td>7455.000000</td>\n","    </tr>\n","    <tr>\n","      <th>val_count</th>\n","      <td>700.000000</td>\n","      <td>904.000000</td>\n","      <td>3085.000000</td>\n","      <td>2683.000000</td>\n","      <td>2986.000000</td>\n","      <td>3110.00</td>\n","      <td>1846.000000</td>\n","    </tr>\n","    <tr>\n","      <th>val_train_ratio</th>\n","      <td>0.241546</td>\n","      <td>0.246187</td>\n","      <td>0.251201</td>\n","      <td>0.249930</td>\n","      <td>0.250063</td>\n","      <td>0.25</td>\n","      <td>0.247619</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">4</th>\n","      <th>train_count</th>\n","      <td>2851.000000</td>\n","      <td>3630.000000</td>\n","      <td>12309.000000</td>\n","      <td>10734.000000</td>\n","      <td>11942.000000</td>\n","      <td>12440.00</td>\n","      <td>7425.000000</td>\n","    </tr>\n","    <tr>\n","      <th>val_count</th>\n","      <td>747.000000</td>\n","      <td>946.000000</td>\n","      <td>3057.000000</td>\n","      <td>2684.000000</td>\n","      <td>2985.000000</td>\n","      <td>3110.00</td>\n","      <td>1876.000000</td>\n","    </tr>\n","    <tr>\n","      <th>val_train_ratio</th>\n","      <td>0.262013</td>\n","      <td>0.260606</td>\n","      <td>0.248355</td>\n","      <td>0.250047</td>\n","      <td>0.249958</td>\n","      <td>0.25</td>\n","      <td>0.252660</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                             (6,)         (2,)          (5,)          (1,)  \\\n","fold counts                                                                  \n","0    train_count      2895.000000  3674.000000  12284.000000  10735.000000   \n","     val_count         703.000000   902.000000   3082.000000   2683.000000   \n","     val_train_ratio     0.242832     0.245509      0.250895      0.249930   \n","1    train_count      2878.000000  3674.000000  12296.000000  10734.000000   \n","     val_count         720.000000   902.000000   3070.000000   2684.000000   \n","     val_train_ratio     0.250174     0.245509      0.249675      0.250047   \n","2    train_count      2870.000000  3654.000000  12294.000000  10734.000000   \n","     val_count         728.000000   922.000000   3072.000000   2684.000000   \n","     val_train_ratio     0.253659     0.252326      0.249878      0.250047   \n","3    train_count      2898.000000  3672.000000  12281.000000  10735.000000   \n","     val_count         700.000000   904.000000   3085.000000   2683.000000   \n","     val_train_ratio     0.241546     0.246187      0.251201      0.249930   \n","4    train_count      2851.000000  3630.000000  12309.000000  10734.000000   \n","     val_count         747.000000   946.000000   3057.000000   2684.000000   \n","     val_train_ratio     0.262013     0.260606      0.248355      0.250047   \n","\n","                              (0,)      (3,)         (4,)  \n","fold counts                                                \n","0    train_count      11941.000000  12440.00  7437.000000  \n","     val_count         2986.000000   3110.00  1864.000000  \n","     val_train_ratio      0.250063      0.25     0.250639  \n","1    train_count      11942.000000  12440.00  7437.000000  \n","     val_count         2985.000000   3110.00  1864.000000  \n","     val_train_ratio      0.249958      0.25     0.250639  \n","2    train_count      11942.000000  12440.00  7450.000000  \n","     val_count         2985.000000   3110.00  1851.000000  \n","     val_train_ratio      0.249958      0.25     0.248456  \n","3    train_count      11941.000000  12440.00  7455.000000  \n","     val_count         2986.000000   3110.00  1846.000000  \n","     val_train_ratio      0.250063      0.25     0.247619  \n","4    train_count      11942.000000  12440.00  7425.000000  \n","     val_count         2985.000000   3110.00  1876.000000  \n","     val_train_ratio      0.249958      0.25     0.252660  "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["df_stats = get_train_val_split_stats(df_train_onehot)\n","df_stats"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from collections import defaultdict\n","\n","ner_labels = df_train.discourse_type.unique().tolist()\n","labels = defaultdict()\n","\n","for index, lbl in enumerate(ner_labels):\n","    labels[f\"B-{lbl}\"] = index\n","    labels[f\"I-{lbl}\"] = index + len(ner_labels)\n","\n","labels[f\"O\"] = 2 * len(ner_labels)\n","labels[f\"Special\"] = -100\n","\n","ids_to_labels = {value: key for key, value in enumerate(labels)}"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T11:21:30.351755Z","iopub.status.busy":"2022-02-19T11:21:30.351535Z","iopub.status.idle":"2022-02-19T11:21:30.358885Z","shell.execute_reply":"2022-02-19T11:21:30.357964Z","shell.execute_reply.started":"2022-02-19T11:21:30.351718Z"},"trusted":true},"outputs":[],"source":["def read_text(file_name):\n","    with open(DATA_PATH + \"train/\" + file_name + \".txt\", \"r\") as file:\n","        text = file.read()\n","        return text"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T11:21:30.360888Z","iopub.status.busy":"2022-02-19T11:21:30.36028Z","iopub.status.idle":"2022-02-19T11:22:45.922262Z","shell.execute_reply":"2022-02-19T11:22:45.921372Z","shell.execute_reply.started":"2022-02-19T11:21:30.360853Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>targets</th>\n","      <th>targets_str</th>\n","      <th>kfold</th>\n","      <th>ner_labelslist</th>\n","      <th>discourse_start</th>\n","      <th>discourse_end</th>\n","      <th>predictionstring</th>\n","      <th>text</th>\n","      <th>text_length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000D23A521A</td>\n","      <td>[1, 1, 1, 3, 0, 1, 1]</td>\n","      <td>1,1,1,3,0,1,1</td>\n","      <td>3</td>\n","      <td>[Claim, Concluding Statement, Counterclaim, Ev...</td>\n","      <td>[0.0, 170.0, 358.0, 438.0, 627.0, 722.0, 836.0...</td>\n","      <td>[170.0, 357.0, 438.0, 626.0, 722.0, 836.0, 101...</td>\n","      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n","      <td>Some people belive that the so called \"face\" o...</td>\n","      <td>251</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>00066EA9880D</td>\n","      <td>[3, 1, 0, 3, 1, 1, 0]</td>\n","      <td>3,1,0,3,1,1,0</td>\n","      <td>0</td>\n","      <td>[Claim, Claim, Claim, Concluding Statement, Ev...</td>\n","      <td>[0.0, 456.0, 638.0, 738.0, 1399.0, 1488.0, 231...</td>\n","      <td>[455.0, 592.0, 738.0, 1398.0, 1487.0, 2219.0, ...</td>\n","      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n","      <td>Driverless cars are exaclty what you would exp...</td>\n","      <td>646</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000E6DE9E817</td>\n","      <td>[5, 1, 1, 3, 0, 1, 1]</td>\n","      <td>5,1,1,3,0,1,1</td>\n","      <td>0</td>\n","      <td>[Claim, Claim, Claim, Claim, Claim, Concluding...</td>\n","      <td>[17.0, 64.0, 158.0, 310.0, 438.0, 551.0, 776.0...</td>\n","      <td>[56.0, 157.0, 309.0, 422.0, 551.0, 775.0, 961....</td>\n","      <td>[[2, 3, 4, 5, 6, 7, 8], [10, 11, 12, 13, 14, 1...</td>\n","      <td>Dear: Principal\\n\\nI am arguing against the po...</td>\n","      <td>274</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>001552828BD0</td>\n","      <td>[4, 0, 0, 4, 1, 1, 0]</td>\n","      <td>4,0,0,4,1,1,0</td>\n","      <td>2</td>\n","      <td>[Claim, Claim, Claim, Claim, Evidence, Evidenc...</td>\n","      <td>[0.0, 161.0, 872.0, 958.0, 1191.0, 1542.0, 161...</td>\n","      <td>[160.0, 872.0, 957.0, 1190.0, 1541.0, 1612.0, ...</td>\n","      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n","      <td>Would you be able to give your car up? Having ...</td>\n","      <td>512</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0016926B079C</td>\n","      <td>[7, 0, 0, 3, 0, 1, 0]</td>\n","      <td>7,0,0,3,0,1,0</td>\n","      <td>4</td>\n","      <td>[Claim, Claim, Claim, Claim, Claim, Claim, Cla...</td>\n","      <td>[0.0, 58.0, 94.0, 206.0, 236.0, 272.0, 542.0, ...</td>\n","      <td>[57.0, 91.0, 150.0, 235.0, 271.0, 542.0, 650.0...</td>\n","      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, ...</td>\n","      <td>I think that students would benefit from learn...</td>\n","      <td>261</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             id                targets    targets_str  kfold  \\\n","0  0000D23A521A  [1, 1, 1, 3, 0, 1, 1]  1,1,1,3,0,1,1      3   \n","1  00066EA9880D  [3, 1, 0, 3, 1, 1, 0]  3,1,0,3,1,1,0      0   \n","2  000E6DE9E817  [5, 1, 1, 3, 0, 1, 1]  5,1,1,3,0,1,1      0   \n","3  001552828BD0  [4, 0, 0, 4, 1, 1, 0]  4,0,0,4,1,1,0      2   \n","4  0016926B079C  [7, 0, 0, 3, 0, 1, 0]  7,0,0,3,0,1,0      4   \n","\n","                                      ner_labelslist  \\\n","0  [Claim, Concluding Statement, Counterclaim, Ev...   \n","1  [Claim, Claim, Claim, Concluding Statement, Ev...   \n","2  [Claim, Claim, Claim, Claim, Claim, Concluding...   \n","3  [Claim, Claim, Claim, Claim, Evidence, Evidenc...   \n","4  [Claim, Claim, Claim, Claim, Claim, Claim, Cla...   \n","\n","                                     discourse_start  \\\n","0  [0.0, 170.0, 358.0, 438.0, 627.0, 722.0, 836.0...   \n","1  [0.0, 456.0, 638.0, 738.0, 1399.0, 1488.0, 231...   \n","2  [17.0, 64.0, 158.0, 310.0, 438.0, 551.0, 776.0...   \n","3  [0.0, 161.0, 872.0, 958.0, 1191.0, 1542.0, 161...   \n","4  [0.0, 58.0, 94.0, 206.0, 236.0, 272.0, 542.0, ...   \n","\n","                                       discourse_end  \\\n","0  [170.0, 357.0, 438.0, 626.0, 722.0, 836.0, 101...   \n","1  [455.0, 592.0, 738.0, 1398.0, 1487.0, 2219.0, ...   \n","2  [56.0, 157.0, 309.0, 422.0, 551.0, 775.0, 961....   \n","3  [160.0, 872.0, 957.0, 1190.0, 1541.0, 1612.0, ...   \n","4  [57.0, 91.0, 150.0, 235.0, 271.0, 542.0, 650.0...   \n","\n","                                    predictionstring  \\\n","0  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n","1  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n","2  [[2, 3, 4, 5, 6, 7, 8], [10, 11, 12, 13, 14, 1...   \n","3  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n","4  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, ...   \n","\n","                                                text  text_length  \n","0  Some people belive that the so called \"face\" o...          251  \n","1  Driverless cars are exaclty what you would exp...          646  \n","2  Dear: Principal\\n\\nI am arguing against the po...          274  \n","3  Would you be able to give your car up? Having ...          512  \n","4  I think that students would benefit from learn...          261  "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["df_train_grouped = df_train.groupby([\"id\"])\n","essay_id = pd.Series([*df_train_grouped.groups.keys()])\n","text = essay_id.apply(lambda x: read_text(x))\n","df_text = pd.concat([essay_id, text], axis=1, keys=[\"id\", \"text\"])\n","df_text[\"text_length\"] = df_text.text.apply(lambda text: len(text.split()))\n","df_ner_labelslist = df_train_grouped[\"discourse_type\"].apply(lambda x:list(x.sort_values())).reset_index(name=\"ner_labelslist\")\n","df_discourse_start = df_train_grouped[\"discourse_start\"].apply(list).reset_index(name=\"discourse_start\")\n","df_discourse_end = df_train_grouped[\"discourse_end\"].apply(list).reset_index(name=\"discourse_end\")\n","df_predictionsstring = df_train_grouped[\"predictionstring\"].apply(list).reset_index(name=\"predictionstring\")\n","df_train_onehot = df_train_onehot[[\"id\", \"targets\", \"targets_str\", \"kfold\"]]\n","df_list = [df_train_onehot, df_ner_labelslist, df_discourse_start, df_discourse_end, df_predictionsstring, df_text]\n","df_train_merged = functools.reduce(lambda df1, df2: pd.merge(left=df1, right=df2, on=[\"id\"], how=\"inner\"), df_list)\n","df_train_merged.head()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def label_words(row):    \n","    words = row[\"text\"].split()\n","    word_labels = [\"O\" for word in words]\n","    word_label_ids = [labels[\"O\"] for word in words]\n","    for idx, label in enumerate(row[\"ner_labelslist\"]):\n","        word_idx = row[\"predictionstring\"][idx]        \n","        # As per the NER IOB tagging scheme\n","        # The starting word of the discourse has label B-\n","        word_labels[word_idx[0]] = f\"B-{label}\"\n","        word_label_ids[word_idx[0]] = labels[f\"B-{label}\"]\n","        # All other words of the discourse have label I-\n","        for index in word_idx[1:]:\n","            word_labels[index] = f\"I-{label}\"\n","            word_label_ids[index] = labels[f\"I-{label}\"]        \n","    row[\"word_labels\"] = word_labels\n","    row[\"word_label_ids\"] = word_label_ids\n","    return row"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>targets</th>\n","      <th>targets_str</th>\n","      <th>kfold</th>\n","      <th>ner_labelslist</th>\n","      <th>discourse_start</th>\n","      <th>discourse_end</th>\n","      <th>predictionstring</th>\n","      <th>text</th>\n","      <th>text_length</th>\n","      <th>word_labels</th>\n","      <th>word_label_ids</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000D23A521A</td>\n","      <td>[1, 1, 1, 3, 0, 1, 1]</td>\n","      <td>1,1,1,3,0,1,1</td>\n","      <td>3</td>\n","      <td>[Claim, Concluding Statement, Counterclaim, Ev...</td>\n","      <td>[0.0, 170.0, 358.0, 438.0, 627.0, 722.0, 836.0...</td>\n","      <td>[170.0, 357.0, 438.0, 626.0, 722.0, 836.0, 101...</td>\n","      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n","      <td>Some people belive that the so called \"face\" o...</td>\n","      <td>251</td>\n","      <td>[B-Claim, I-Claim, I-Claim, I-Claim, I-Claim, ...</td>\n","      <td>[3, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>00066EA9880D</td>\n","      <td>[3, 1, 0, 3, 1, 1, 0]</td>\n","      <td>3,1,0,3,1,1,0</td>\n","      <td>0</td>\n","      <td>[Claim, Claim, Claim, Concluding Statement, Ev...</td>\n","      <td>[0.0, 456.0, 638.0, 738.0, 1399.0, 1488.0, 231...</td>\n","      <td>[455.0, 592.0, 738.0, 1398.0, 1487.0, 2219.0, ...</td>\n","      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n","      <td>Driverless cars are exaclty what you would exp...</td>\n","      <td>646</td>\n","      <td>[B-Claim, I-Claim, I-Claim, I-Claim, I-Claim, ...</td>\n","      <td>[3, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000E6DE9E817</td>\n","      <td>[5, 1, 1, 3, 0, 1, 1]</td>\n","      <td>5,1,1,3,0,1,1</td>\n","      <td>0</td>\n","      <td>[Claim, Claim, Claim, Claim, Claim, Concluding...</td>\n","      <td>[17.0, 64.0, 158.0, 310.0, 438.0, 551.0, 776.0...</td>\n","      <td>[56.0, 157.0, 309.0, 422.0, 551.0, 775.0, 961....</td>\n","      <td>[[2, 3, 4, 5, 6, 7, 8], [10, 11, 12, 13, 14, 1...</td>\n","      <td>Dear: Principal\\n\\nI am arguing against the po...</td>\n","      <td>274</td>\n","      <td>[O, O, B-Claim, I-Claim, I-Claim, I-Claim, I-C...</td>\n","      <td>[14, 14, 3, 10, 10, 10, 10, 10, 10, 14, 3, 10,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>001552828BD0</td>\n","      <td>[4, 0, 0, 4, 1, 1, 0]</td>\n","      <td>4,0,0,4,1,1,0</td>\n","      <td>2</td>\n","      <td>[Claim, Claim, Claim, Claim, Evidence, Evidenc...</td>\n","      <td>[0.0, 161.0, 872.0, 958.0, 1191.0, 1542.0, 161...</td>\n","      <td>[160.0, 872.0, 957.0, 1190.0, 1541.0, 1612.0, ...</td>\n","      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n","      <td>Would you be able to give your car up? Having ...</td>\n","      <td>512</td>\n","      <td>[B-Claim, I-Claim, I-Claim, I-Claim, I-Claim, ...</td>\n","      <td>[3, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0016926B079C</td>\n","      <td>[7, 0, 0, 3, 0, 1, 0]</td>\n","      <td>7,0,0,3,0,1,0</td>\n","      <td>4</td>\n","      <td>[Claim, Claim, Claim, Claim, Claim, Claim, Cla...</td>\n","      <td>[0.0, 58.0, 94.0, 206.0, 236.0, 272.0, 542.0, ...</td>\n","      <td>[57.0, 91.0, 150.0, 235.0, 271.0, 542.0, 650.0...</td>\n","      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, ...</td>\n","      <td>I think that students would benefit from learn...</td>\n","      <td>261</td>\n","      <td>[B-Claim, I-Claim, I-Claim, I-Claim, I-Claim, ...</td>\n","      <td>[3, 10, 10, 10, 10, 10, 10, 10, 10, 10, 3, 10,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             id                targets    targets_str  kfold  \\\n","0  0000D23A521A  [1, 1, 1, 3, 0, 1, 1]  1,1,1,3,0,1,1      3   \n","1  00066EA9880D  [3, 1, 0, 3, 1, 1, 0]  3,1,0,3,1,1,0      0   \n","2  000E6DE9E817  [5, 1, 1, 3, 0, 1, 1]  5,1,1,3,0,1,1      0   \n","3  001552828BD0  [4, 0, 0, 4, 1, 1, 0]  4,0,0,4,1,1,0      2   \n","4  0016926B079C  [7, 0, 0, 3, 0, 1, 0]  7,0,0,3,0,1,0      4   \n","\n","                                      ner_labelslist  \\\n","0  [Claim, Concluding Statement, Counterclaim, Ev...   \n","1  [Claim, Claim, Claim, Concluding Statement, Ev...   \n","2  [Claim, Claim, Claim, Claim, Claim, Concluding...   \n","3  [Claim, Claim, Claim, Claim, Evidence, Evidenc...   \n","4  [Claim, Claim, Claim, Claim, Claim, Claim, Cla...   \n","\n","                                     discourse_start  \\\n","0  [0.0, 170.0, 358.0, 438.0, 627.0, 722.0, 836.0...   \n","1  [0.0, 456.0, 638.0, 738.0, 1399.0, 1488.0, 231...   \n","2  [17.0, 64.0, 158.0, 310.0, 438.0, 551.0, 776.0...   \n","3  [0.0, 161.0, 872.0, 958.0, 1191.0, 1542.0, 161...   \n","4  [0.0, 58.0, 94.0, 206.0, 236.0, 272.0, 542.0, ...   \n","\n","                                       discourse_end  \\\n","0  [170.0, 357.0, 438.0, 626.0, 722.0, 836.0, 101...   \n","1  [455.0, 592.0, 738.0, 1398.0, 1487.0, 2219.0, ...   \n","2  [56.0, 157.0, 309.0, 422.0, 551.0, 775.0, 961....   \n","3  [160.0, 872.0, 957.0, 1190.0, 1541.0, 1612.0, ...   \n","4  [57.0, 91.0, 150.0, 235.0, 271.0, 542.0, 650.0...   \n","\n","                                    predictionstring  \\\n","0  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n","1  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n","2  [[2, 3, 4, 5, 6, 7, 8], [10, 11, 12, 13, 14, 1...   \n","3  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...   \n","4  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, ...   \n","\n","                                                text  text_length  \\\n","0  Some people belive that the so called \"face\" o...          251   \n","1  Driverless cars are exaclty what you would exp...          646   \n","2  Dear: Principal\\n\\nI am arguing against the po...          274   \n","3  Would you be able to give your car up? Having ...          512   \n","4  I think that students would benefit from learn...          261   \n","\n","                                         word_labels  \\\n","0  [B-Claim, I-Claim, I-Claim, I-Claim, I-Claim, ...   \n","1  [B-Claim, I-Claim, I-Claim, I-Claim, I-Claim, ...   \n","2  [O, O, B-Claim, I-Claim, I-Claim, I-Claim, I-C...   \n","3  [B-Claim, I-Claim, I-Claim, I-Claim, I-Claim, ...   \n","4  [B-Claim, I-Claim, I-Claim, I-Claim, I-Claim, ...   \n","\n","                                      word_label_ids  \n","0  [3, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10...  \n","1  [3, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10...  \n","2  [14, 14, 3, 10, 10, 10, 10, 10, 10, 14, 3, 10,...  \n","3  [3, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10...  \n","4  [3, 10, 10, 10, 10, 10, 10, 10, 10, 10, 3, 10,...  "]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["df_train_final = df_train_merged.apply(lambda row: label_words(row), axis=1)\n","df_train_final.head()"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T11:22:45.924275Z","iopub.status.busy":"2022-02-19T11:22:45.923785Z","iopub.status.idle":"2022-02-19T11:22:51.016498Z","shell.execute_reply":"2022-02-19T11:22:51.015512Z","shell.execute_reply.started":"2022-02-19T11:22:45.924232Z"},"trusted":true},"outputs":[],"source":["from transformers import LongformerTokenizerFast, DataCollatorWithPadding\n","\n","tokenizer = LongformerTokenizerFast.from_pretrained(DATA_PATH + \"longformer/\", local_files_only=True, add_prefix_space=True)\n","# DataCollatorWithPadding pads each batch to the longest sequence length\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T11:23:18.244398Z","iopub.status.busy":"2022-02-19T11:23:18.243962Z","iopub.status.idle":"2022-02-19T11:23:18.256725Z","shell.execute_reply":"2022-02-19T11:23:18.255722Z","shell.execute_reply.started":"2022-02-19T11:23:18.244368Z"},"trusted":true},"outputs":[],"source":["def tokenize_and_label_with_offsets(tokenizer, row):    \n","    text = row[\"text\"]    \n","    result = tokenizer(\n","        text, \n","        max_length=Config.MAX_LENGTH,\n","        padding=False, \n","        truncation=True,\n","        return_offsets_mapping=True, \n","        return_overflowing_tokens=True,\n","        stride=Config.STRIDE\n","    )    \n","    # If you are doing batch encoding, one sentence with length exceeding > max_length gets split into\n","    # multiple rows, \"overflow_to_sample_mapping\" which is an array with length = number of splits tells you\n","    # which split came from which sentence of the batch. So if you batch has two sentences (0, 1) with sentence 0\n","    # having two splits and sentence 1 having three splits the result[\"overflow_to_sample_mapping\"] = [0, 0, 1, 1, 1]    \n","    token_label_ids = []\n","    token_labels = []\n","    token_words = []\n","    # the word_id in the text that a token belongs to. \n","    token_word_ids = []\n","    for i in result[\"overflow_to_sample_mapping\"]:        \n","        # There are as many labels as there are tokens. For each token set the label to a default value\n","        row_token_label_ids = [labels[\"O\"] for j in range(len(result[\"input_ids\"][i]))]         \n","        row_token_labels = [\"O\" for j in range(len(result[\"input_ids\"][i]))]        \n","        subtext_token_words = [\"\" for j in range(len(result[\"input_ids\"][i]))]\n","        row_ner_labels = row[\"ner_labelslist\"][i]        \n","        discourse_start = row[\"discourse_start\"][i]\n","        discourse_end = row[\"discourse_end\"][i]\n","        token_word_ids.append(result.word_ids(batch_index=i))\n","        # loop thru the tokens\n","        for j in range(len(result[\"input_ids\"][i])):\n","            input_id = result[\"input_ids\"][i][j]\n","            # Set the label of special tokens 'CLS' and 'SEP' to -100\n","            if input_id in [0, 2]:\n","                row_token_label_ids[j] = -100\n","                row_token_labels[j] = \"Special\"\n","                continue\n","            token_start, token_end = result[\"offset_mapping\"][i][j]\n","            subtext_token_words[j] = text[i][token_start:token_end]\n","            for ner_label, label_start, label_end in list(zip(row_ner_labels, discourse_start, discourse_end)):\n","                if token_start == label_start and token_end > token_start:\n","                    row_token_label_ids[j] = labels[f\"B-{ner_label}\"]\n","                    row_token_labels[j] = f\"B-{ner_label}\"\n","                elif token_start > label_start and token_end <= label_end:\n","                    row_token_label_ids[j] = labels[f\"I-{ner_label}\"]\n","                    row_token_labels[j] = f\"I-{ner_label}\"\n","        token_labels.append(row_token_labels)\n","        token_label_ids.append(row_token_label_ids)\n","        token_words.append(subtext_token_words)\n","    result[\"token_label_ids\"] = token_label_ids    \n","    result[\"token_labels\"] = token_labels\n","    result[\"token_words\"] = token_words\n","    result[\"token_word_ids\"] = token_word_ids\n","    return result    "]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["def tokenize_and_label(tokenizer, data_row):    \n","    # convert the text to word tokens splitting on \" \"\n","    text_words = [item.split() for item in data_row[\"text\"]]\n","    encoding = tokenizer(\n","        text_words, \n","        is_split_into_words=True,\n","        max_length=Config.MAX_LENGTH,\n","        padding=False, \n","        truncation=True,\n","        return_offsets_mapping=True, \n","        return_overflowing_tokens=True,\n","        stride=Config.STRIDE\n","    )       \n","    token_label_ids = [] \n","    token_labels = []\n","    tokens = []\n","    token_words = []\n","    # A text(essay) may get split into multiple sub texts if text length > max_length. \n","    # For e.g. text_id=0 => sub_text_id=[0], text_id=1 => sub_text_ids = [1, 2, 3], text_id=2 => sub_text_ids=[4,5] \n","    for sub_text_id, (token_ids, text_id) in enumerate(zip(encoding[\"input_ids\"], encoding[\"overflow_to_sample_mapping\"])):\n","        # There are as many labels as there are tokens. For each token set the label to a default value\n","        sub_token_label_ids = [labels[\"O\"] for j in range(len(token_ids))]         \n","        sub_token_labels = [\"O\" for j in range(len(token_ids))]   \n","        sub_tokens = tokenizer.convert_ids_to_tokens(token_ids)     \n","        words = text_words[text_id]\n","        word_ids = encoding.word_ids(batch_index=sub_text_id)        \n","        sub_text_words = []\n","        for word_id in word_ids:\n","            if word_id is None:\n","                sub_text_words.append(None)\n","            else:\n","                sub_text_words.append(words[word_id])                \n","        for token_idx, word_idx in enumerate(word_ids):\n","            # Set the label of special tokens 'CLS' and 'SEP' to -100\n","            if word_idx is None:\n","                sub_token_label_ids[token_idx] = -100\n","                sub_token_labels[token_idx] = \"Special\"\n","            else:                            \n","                sub_token_label_ids[token_idx] = data_row[\"word_label_ids\"][text_id][word_idx]\n","                sub_token_labels[token_idx] = data_row[\"word_labels\"][text_id][word_idx]            \n","        token_labels.append(sub_token_labels)            \n","        token_label_ids.append(sub_token_label_ids)\n","        tokens.append(sub_tokens)\n","        token_words.append(sub_text_words)\n","    encoding[\"token_labels\"] = token_labels\n","    encoding[\"token_label_ids\"] = token_label_ids\n","    encoding[\"tokens\"] = tokens\n","    encoding[\"token_words\"] = token_words\n","    return encoding\n","        "]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["test_data = df_train_final[df_train_final.id.isin([\"0000D23A521A\", \"00066EA9880D\"])]\n","text_words = [item.split() for item in test_data[\"text\"].values]"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["#test_data[\"word_label_ids\"][0]"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T12:21:32.350135Z","iopub.status.busy":"2022-02-19T12:21:32.349695Z","iopub.status.idle":"2022-02-19T12:21:32.357971Z","shell.execute_reply":"2022-02-19T12:21:32.356884Z","shell.execute_reply.started":"2022-02-19T12:21:32.350094Z"},"trusted":true},"outputs":[],"source":["#test_result = tokenize_and_label(tokenizer, test_data)"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T11:23:21.879604Z","iopub.status.busy":"2022-02-19T11:23:21.878458Z","iopub.status.idle":"2022-02-19T11:23:21.884106Z","shell.execute_reply":"2022-02-19T11:23:21.883372Z","shell.execute_reply.started":"2022-02-19T11:23:21.879552Z"},"trusted":true},"outputs":[],"source":["from functools import partial\n","\n","preprocess_train_data = partial(tokenize_and_label, tokenizer)"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T11:23:23.907883Z","iopub.status.busy":"2022-02-19T11:23:23.907331Z","iopub.status.idle":"2022-02-19T11:23:24.197262Z","shell.execute_reply":"2022-02-19T11:23:24.196241Z","shell.execute_reply.started":"2022-02-19T11:23:23.907826Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from datasets import Dataset\n","\n","def get_fold_dls(fold, df):\n","    train_df = df[df.kfold != fold].reset_index(drop=True)\n","    valid_df = df[df.kfold == fold].reset_index(drop=True)\n","    ds_train_raw = Dataset.from_pandas(train_df)\n","    ds_valid_raw = Dataset.from_pandas(valid_df)\n","    raw_ds_col_names = ds_train_raw.column_names    \n","    ds_train = ds_train_raw.map(preprocess_train_data, batched=True, batch_size=1000, remove_columns=raw_ds_col_names)\n","    ds_valid = ds_valid_raw.map(preprocess_train_data, batched=True, batch_size=1000, remove_columns=raw_ds_col_names)\n","    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, collate_fn=data_collator, num_workers=Config.NUM_WORKERS)\n","    dl_valid = DataLoader(ds_valid, batch_size=Config.BATCH_SIZE, collate_fn=data_collator, num_workers=Config.NUM_WORKERS)\n","    return dl_train, dl_valid, ds_train, ds_valid"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2022-02-19T11:25:08.397137Z","iopub.status.busy":"2022-02-19T11:25:08.396561Z","iopub.status.idle":"2022-02-19T11:25:08.643885Z","shell.execute_reply":"2022-02-19T11:25:08.643081Z","shell.execute_reply.started":"2022-02-19T11:25:08.397081Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b97a59ae815e40a78508d2455f3ee9a6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/13 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee95a510b10f441bb6591489f9772622","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["dl_train, dl_valid, ds_train, ds_valid = get_fold_dls(0, df_train_final)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-02-15T10:57:21.175821Z","iopub.status.busy":"2022-02-15T10:57:21.175328Z","iopub.status.idle":"2022-02-15T10:57:30.083808Z","shell.execute_reply":"2022-02-15T10:57:30.081767Z","shell.execute_reply.started":"2022-02-15T10:57:21.17578Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eb9dbe79fe474d31b4c584fa33658324","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/694 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"662a2ea4fc3e49dfb6e96de9c0b6110f","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/570M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing LongformerForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing LongformerForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of LongformerForTokenClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForTokenClassification, LongformerForTokenClassification\n","model = AutoModelForTokenClassification.from_pretrained(Config.TRANSFORMER_CHECKPOINT)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-02-15T10:57:54.788862Z","iopub.status.busy":"2022-02-15T10:57:54.787578Z","iopub.status.idle":"2022-02-15T10:57:54.796654Z","shell.execute_reply":"2022-02-15T10:57:54.795106Z","shell.execute_reply.started":"2022-02-15T10:57:54.788791Z"},"trusted":true},"outputs":[],"source":["model.config.id2label"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
